---
title: "UCT Masters Minor Disseration"
author: "Gustav Oosthuizen"
date: "22/09/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Importing/Loading packages

```{r, importing/loading packages}

# 1. General workflow
library(tidyverse)
library(stringr)
library(lubridate)
#library(DataExplorer)
library(GGally)
library(gridExtra)
library(gridtext)
library(grid)


# 2. Network Analytics
library(igraph)

# 3. ML packages
library(caret)
library(glmnet)
library(ROCR)

# install.packages("keras")
# install.packages("tensorflow")
# 
# install.packages("devtools")
# 
# devtools::install_github("rstudio/keras", dependencies = T)
# devtools::install_github("rstudio/tensorflow", dependencies = T)

library(keras)
library(tensorflow)
library(tfruns)




```

# 1. Loading data

```{r, loading data}

# Note: There is three separate data files: i) accounts, ii) alerts, iii) transactions

# 1. Training data
training_accounts_raw <- read.csv("training_set_all_ml_typologies/accounts.csv")
training_transactions_raw <- read.csv("training_set_all_ml_typologies/transactions.csv")
training_alert_accounts_raw <- read.csv("training_set_all_ml_typologies/alert_accounts.csv")
training_alert_transactions_raw <- read.csv("training_set_all_ml_typologies/alert_transactions.csv")

# 1. Training data
testing_accounts_raw <- read.csv("testing_set_all_ml_typologies/accounts.csv")
testing_transactions_raw <- read.csv("testing_set_all_ml_typologies/transactions.csv")
testing_alert_accounts_raw <- read.csv("testing_set_all_ml_typologies/alert_accounts.csv")
testing_alert_transactions_raw <- read.csv("testing_set_all_ml_typologies/alert_transactions.csv")


### BASIC STATS - test train ###

# fraud transactions
#table(training_transactions_raw$is_sar)
# 0.1% fraud transactions

#table(testing_transactions_raw$is_sar)
# 0.05% fraud transactions


# fraud alerts
#table(training_alert_transactions_raw$alert_type)/sum(table(training_alert_transactions_raw$alert_type))*100


#table(testing_alert_transactions_raw$alert_type)/sum(table(testing_alert_transactions_raw$alert_type))*100

###  SAMPLE NETORK DATA SET ###

# Constructing sample network used by Bart Baesens et al. (Fraud Analytics book) - Undirected network & Undirected weighted network


# 1. Configuring sample edge and node df's

# nodes (i.e customer's accounts)
# node_df <- data.frame(client_ID = LETTERS[1:20],
#                       is_fraud = c(rep(F, 3), T, T, T, rep(F,2), T, rep(F,11)))

# edges (i.e customers transactions)
# edge_df <- data.frame(from = c("A","A", "A", "A", "A", "A", "D", "D", "D", "D", "Q", "Q", "Q", "N","N", "N", "I", "I", "K", "K", "O", "B", "F", "F", "M"),
#                       to = c("I", "T", "B", "H", "O", "G", "G", "B", "E", "F", "R", "S", "O", "M", "L", "I", "K", "G", "J", "H", "P", "C", "G", "E", "L"),
#                       weight = c(rexp(10,0.001), rep(1000,10), rexp(5,0.001)))


# 2. Building sample network

# 2.1 un- directed network
# sample_network_undirected <- graph_from_data_frame(d = edge_df[,-3], # removing weight col
#                                  vertices = node_df,
#                                  directed = FALSE)


# 2.2 un-directed weighted network
# sample_network_undirected_weighted <- graph_from_data_frame(d = edge_df,
#                                  vertices = node_df,
#                                  directed = FALSE)

# 2.3 directed network
# sample_network_directed_weighted <- graph_from_data_frame(d = edge_df, # removing weight col
#                                  vertices = node_df,
#                                  directed = TRUE)


# visualizing the graph (only directed graph)

# fraud_colour <- if_else(V(sample_network_directed_weighted)$is_fraud == T, "red", "green")
# V(sample_network_directed_weighted)$color <- fraud_colour
# E(sample_network_directed_weighted)$width <- E(sample_network_directed_weighted)$weight/200
# plot(sample_network_directed_weighted)

# interactive plot

# tkid <- tkplot(sample_network_directed_weighted) #tkid is the id of the tkplot that will open
# l <- tkplot.getcoords(sample_network_directed_weighted)


```

# 2. EDA of raw data table(s)

```{r, EDA of raw data table(s)}


# Note: some of the plots and stats can be useful

### KAGGLE DATA SET (OLD) ###

# 1. Accounts data

# 1.1 structure

str(accounts_data_raw)

summary(accounts_data_raw)

# 1.2 Checking uniqueness of variables

accounts_data_raw$ACCOUNT_ID %>% 
  unique() %>% 
  length()

# Note: there are 1000 accounts

accounts_data_raw$CUSTOMER_ID %>% 
  unique() %>% 
  length()

# Note: It seems like the assumption was made that the a customer only has one account.

accounts_data_raw$COUNTRY %>% 
  unique() 

# Seems like the only country is US

accounts_data_raw$ACCOUNT_TYPE %>% 
  unique()

# plotting fraudulent and not fraudent accounts

accounts_data_raw %>% 
  group_by(IS_FRAUD) %>% 
  summarise(total_count = n()) %>% 
  ggplot(aes(y = total_count, x = IS_FRAUD, fill = IS_FRAUD)) + 
  geom_col() + 
  geom_text(aes(label = total_count), vjust = 1)


# 17% is fraudulent accounts and the 83% are honest users

# plot of accounts distribution

# box and whiskers plot - initial balance

ggplot(accounts_data_raw, aes(y = IS_FRAUD, x = INIT_BALANCE)) +
  geom_boxplot()

# distribution plots

# histogram
ggplot(accounts_data_raw, aes(x = INIT_BALANCE, fill = IS_FRAUD)) +
  geom_histogram() +
  facet_wrap(~IS_FRAUD)

# density plot
ggplot(accounts_data_raw, aes(x = INIT_BALANCE, fill = IS_FRAUD)) +
  geom_density() +
  facet_wrap(~IS_FRAUD)

# 2. alerts 

str(alerts_data_raw)

summary(alerts_data_raw)

# Checking the uniqueness of each column

sapply(alerts_data_raw[sapply(alerts_data_raw, class) == "character"], unique)


# plot of alerts distribution

# box and whiskers plot - transaction amount

ggplot(alerts_data_raw, aes(x = TX_AMOUNT, y = IS_FRAUD)) +
  geom_boxplot()

# distribution - transaction amount

# histogram
ggplot(alerts_data_raw, aes(x = TX_AMOUNT)) +
  geom_histogram()

# density plot
ggplot(alerts_data_raw, aes(x = TX_AMOUNT)) +
  geom_density()

# time series plot

alerts_data_raw %>% 
ggplot(aes(x = TIMESTAMP, y = TX_AMOUNT)) + 
  geom_jitter()

# Note: No transactions were flagged that had a amount between 5 and 10 interesting to note that 

#  Histogram of received accounts

alerts_data_raw %>% 
  group_by(RECEIVER_ACCOUNT_ID) %>% 
  summarise(total_count = n()) %>% 
  arrange(desc(total_count)) %>% 
  ggplot(aes(x = total_count)) +
  geom_histogram()

#  Histogram of sender accounts

alerts_data_raw %>% 
  group_by(SENDER_ACCOUNT_ID) %>% 
  summarise(total_count = n()) %>% 
  arrange(desc(total_count)) %>% 
  ggplot(aes(x = total_count)) +
  geom_histogram()


# Count plot of the number of alert types

alerts_data_raw %>% 
  group_by(ALERT_TYPE) %>% 
  summarise(total_count = n()) %>% 
  ggplot(aes(y = total_count, x = ALERT_TYPE, fill = ALERT_TYPE)) + 
  geom_col() + 
  geom_text(aes(label = total_count), vjust = 1)


# 2. Transactions

# structure

str(transactions_data_raw)


# classes of variables

sapply(transactions_data_raw[sapply(transactions_data_raw, class) == "character"], unique)

# distributions - transactions

# box and whiskers plot
ggplot(transactions_data_raw, aes(x = TX_AMOUNT, y = IS_FRAUD)) +
  geom_boxplot() +
  scale_x_continuous(trans='log10')

# histogram
ggplot(transactions_data_raw, aes(x = TX_AMOUNT, fill = IS_FRAUD)) +
  geom_histogram() +
  facet_wrap(~IS_FRAUD) +
  scale_x_continuous(trans='log10') + 
  scale_y_continuous(trans='log10')

# density plots - fraud transactions

transactions_data_fraud <- transactions_data_raw %>% 
  filter(IS_FRAUD == "True")

# maximum density occurs at tx value of:
mode_tx_fraud <- density(transactions_data_fraud$TX_AMOUNT)$x[which.max(density(transactions_data_fraud$TX_AMOUNT)$y)]


ggplot(transactions_data_fraud, aes(TX_AMOUNT)) + 
  geom_density() + 
  geom_vline(xintercept = mode_tx_fraud)

# density plots - Non-fraud transactions (NB! something weird is heaping here!)

transactions_data_honest <- transactions_data_raw %>% 
  filter(IS_FRAUD == "False",
         TX_AMOUNT != 0)

# maximum density occurs at tx value of:
# which.max(density(transactions_data_honest$TX_AMOUNT)$y)
# 
# ggplot(transactions_data_honest, aes(TX_AMOUNT)) + 
#   geom_density() + 
#   geom_vline(xintercept = mode_tx_honest) + 
#   scale_x_log10()


         
```

# 3. Feature engieering 

# 3.1 Data preperation

```{r, data preperation - feature engineering}

# 1. Data preparations

# 1.1 Defining needed data-pre processing functions
# Note: The following function removes and renames the the accounts raw data such that it is ready for downstream processes

# accounts data table

accounts_data_prep_func <- function(accounts_data_raw) {
  
  # 1. selecting variables and renaming them
  
  accounts_data_up <- accounts_data_raw %>% 
    select(acct_id, prior_sar_count, initial_deposit ) %>% 
     rename(client_ID = acct_id,
           is_fraud = prior_sar_count,
           init_balance = initial_deposit) %>% 
    mutate(is_fraud = if_else(is_fraud == "false", F, T))
  
  return(accounts_data_up)
}

# transaction data table

transactions_data_prep_func <- function(transactions_data_raw) {
  
  # 1. selecting variables and renaming them
  
  transactions_data_up <- transactions_data_raw %>% 
    select(orig_acct, bene_acct,base_amt, tran_timestamp) %>% 
     rename(from = orig_acct,
           to = bene_acct,
           weight = base_amt) %>% 
    mutate(tran_timestamp = ymd(str_sub(end =10, tran_timestamp)))
  
  return(transactions_data_up)
  
}

# 2. Applying data pre-processing functions

# training
training_transactions_final <- transactions_data_prep_func(training_transactions_raw)
training_accounts_final <- accounts_data_prep_func(training_accounts_raw)

# testing
testing_transactions_final <- transactions_data_prep_func(testing_transactions_raw)
testing_accounts_final <- accounts_data_prep_func(testing_accounts_raw)


### KAGGLE DATA SET (OLD) ###

# 1. Data preparation

# 1.1 transactions data & alerts data 

# joining alert type col from alerts df to transactions df 
# alerts_types <- alerts_data_raw %>% 
#   select(ALERT_ID, ALERT_TYPE)
# 
# transactions_data_up <- transactions_data_raw %>% 
#   left_join(alerts_types, by = "ALERT_ID") %>% 
#   unique()

# replacing NA values formed by transactions that are not fraud
# transactions_data_up[is.na(transactions_data_up)] <- "No Alert"

# rearranging columns such that sender node and receiver node is mentioned first in the df
# transactions_data_up <- transactions_data_up[,c(2,3,1,4:ncol(transactions_data_up))]

# 1.2 Selecting/renaming only needed cols for networks that will be generated

# nodes

# accounts_data_final <- accounts_data_raw %>% 
#   select(ACCOUNT_ID, INIT_BALANCE, IS_FRAUD) %>% 
#   rename(client_ID = ACCOUNT_ID,
#          is_fraud = IS_FRAUD,
#          init_balance = INIT_BALANCE) %>% 
#   mutate(is_fraud = if_else(is_fraud == "true", T, F))

# edges
# transactions_data_final <- transactions_data_raw %>% 
#   select(SENDER_ACCOUNT_ID, RECEIVER_ACCOUNT_ID, TX_AMOUNT, TIMESTAMP, IS_FRAUD) %>% 
#   rename(from = SENDER_ACCOUNT_ID,
#          to = RECEIVER_ACCOUNT_ID,
#          weight = TX_AMOUNT)


# 2. Data checks 

# 2.1 Checking when account is deemed as fraud

## sender accounts

# sender_transactions_fraud <- transactions_data_up %>% 
#   filter(IS_FRAUD == "True") %>% 
#   select(SENDER_ACCOUNT_ID) %>% 
#   unique()
# 
# receiver_transactions_fraud <- transactions_data_up %>% 
#   filter(IS_FRAUD == "False") %>% 
#   select(RECEIVER_ACCOUNT_ID) %>% 
#   unique()
# 
# clients <- accounts_data_raw %>%
#   select(IS_FRAUD, ACCOUNT_ID) %>% 
#   mutate(sender_account = ACCOUNT_ID %in% sender_transactions_fraud$SENDER_ACCOUNT_ID) %>% 
#   mutate(receiver_account = ACCOUNT_ID %in% receiver_transactions_fraud$RECEIVER_ACCOUNT_ID) %>% 
#   mutate(logic_tester = if_else(((sender_account = T) | (receiver_account = T)) & IS_FRAUD != "true", "Not Valid", "Valid"))
# 
# 
# not_valid_clients <- clients %>% 
#   filter(logic_tester == "Not Valid")

# Conclusion: NB! An account is deemed fraudulent if it is a recipient or sender of a flagged fraudulent transaction. 

# 2.2 Checking if there are any repetitions in node and edge df's

# nodes - accounts
# nrow(accounts_data_raw)
# 
# accounts_data_raw$ACCOUNT_ID %>%
#   unique() %>% 
#   length()
  
# Note: there are 1000 different accounts

# 2.2 links - transactions

# nrow(transactions_data_up)
# 
# nrow(unique(transactions_data_up[,c("SENDER_ACCOUNT_ID", "RECEIVER_ACCOUNT_ID")]))

# Note: There are more links than sender-receiver combination, therefore we have cases where there is multiple links between the same nodes. This will be adresed in the next code section.


```

# 3.2 Network construction - Undirected weighted network (simplified) function(s)

*Note:* The simplified graph will only consider one edge between incident nodes.

```{r, network construction and feature extraction - undirected weighted network (simplified) - feature engineering}

#### Un-directed weighted Network feature generation function ####

# Description: The function will take as input a sub-graph with more than 2 nodes and calculate the relevant network metrics.

# input: a weighted undirected sub-graph with more than two nodes
# output: a data frame containing all the network metrics extracted from the sub-graph. 

# NB! The function will only  be able to take as input an un-directed-weighted network.

# 1. Defining additional functions that will be used within the network feature generation function

# 1.1 Degree functions

# 1.1.1 Fraud degree calculations
fraud_degree_counter_func <- function(graph){
  
  # names of all the nodes in the network
  node_names <- V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  fraud_degree_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_fraud_degree <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      filter(name != node_names[i] & is_fraud == T) %>% 
      nrow()
    
    # saving fraud degree
    fraud_degree_vec[i] <- temp_fraud_degree
    
  }
  
  return(fraud_degree_vec)
  
}

# 1.1.2  Non-fraud degree calculations
non_fraud_degree_counter_func <- function(graph){
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  non_fraud_degree_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_non_fraud_degree <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      filter(name != node_names[i] & is_fraud == F) %>% 
      nrow()
    
    # saving fraud degree
    non_fraud_degree_vec[i] <- temp_non_fraud_degree
    
  }
  
  return(non_fraud_degree_vec)
  
}


# 1.2 Node density function

node_density_func <- function(graph) {
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  node_density_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    # calculating density and saving value
    node_density_vec[i] <- edge_density(temp_sub_graph)
    
  }
  
  return(node_density_vec)
  
}

# 1.3 Relational neighbor (classifier)

relational_neighbour_classifier_func <- function(graph) {
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store the probability of node being fraudulent or non-fraudulent according to its neighborhood
  prob_fraud_vec <- rep(NA, length(node_names))
  prob_non_fraud_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_df <- as_data_frame(temp_sub_graph, what = "vertices")
    
    # calculating number of # fraud neighbors
    temp_count_fraud <- temp_df %>% 
      filter(name != node_names[i] & is_fraud == T) %>% 
      nrow()
    
    # calculating number of # non-fraud neighbors
    temp_count_non_fraud <- temp_df %>% 
      filter(name != node_names[i] & is_fraud == F) %>% 
      nrow()
    
    # calculating nrmalisation factor (Z)
    Z =  temp_count_fraud + temp_count_non_fraud
    
    prob_fraud_vec[i] <- temp_count_fraud/Z
    
    prob_non_fraud_vec[i] <- temp_count_non_fraud/Z
    
    }
  
  probability_res <- list(prob_fraud = prob_fraud_vec,
                          prob_non_fraud = prob_non_fraud_vec) 
  
  
  return(probability_res)
  
}

# 1.4 Probabilistic relational neighbor (classifier)

prob_relational_neighbor_classifier_func <- function(graph, relational_neig_prob_fraud, relational_neig_prob_non_fraud){
  
  # adding relational neighborhood classifier results to graph
  V(graph)$relational_neig_prob_fraud <- relational_neig_prob_fraud
  V(graph)$relational_neig_prob_non_fraud <- relational_neig_prob_non_fraud
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store the probability of node being fraudulent or non-fraudulent according to its neighborhood
  prob_fraud_vec <- rep(NA, length(node_names))
  prob_non_fraud_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_df <- as_data_frame(temp_sub_graph, what = "vertices")
    
    # calculating number of # fraud neighbors
    temp_count_fraud <- temp_df %>% 
      filter(name != node_names[i]) %>% 
      select(relational_neig_prob_fraud) %>% 
      sum()
    
    # calculating number of # non-fraud neighbors
    temp_count_non_fraud <- temp_df %>% 
      filter(name != node_names[i]) %>% 
      select(relational_neig_prob_non_fraud) %>% 
      sum()
    
    # calculating normalization factor (Z)
    Z =  temp_count_fraud + temp_count_non_fraud
    
    prob_fraud_vec[i] <- temp_count_fraud/Z
    
    prob_non_fraud_vec[i] <- temp_count_non_fraud/Z
    
  }
    

  
  probability_res <- list(prob_fraud = prob_fraud_vec,
                          prob_non_fraud = prob_non_fraud_vec) 
  
}

# 1.5 Calculating, Fraud, Semi-Fraud, and Legit triangles

# The function below classifies all the triangles found in a network as either being legit, fraud or semi-fraud
triangle_classifier_func <- function(graph){
  
  
  if(sum(count_triangles(graph)) > 0){
  
  # calculates the complete sub-graphs with 3 vertices (i.e triangles)
  cl.tri = cliques(graph,
                   min=3,
                   max=3)
  
  # constructing a data frame where each row corresponds to a triangle and each column to a node in that triangle
  df <- lapply(cl.tri, function(x){V(graph)$name[x]})
  triangles_df = data.frame(matrix(unlist(df),ncol=3,byrow=T))
  
  # creating class label for triangles (will be important later)
  triangles_df$triangle_label <- rep(NA,nrow(triangles_df))
  
  # creating a sub graph for each triangle
  for (i in 1:nrow(triangles_df)) {
    
    # counting fraud triangles
    temp_triangle <- c(triangles_df$X1[i], triangles_df$X2[i], triangles_df$X3[i])
    
    # creating sub_graph
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = temp_triangle, # triangle nodes
                                       impl = "auto")
    
    fraud_sum <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      select(is_fraud) %>% 
      sum()
    
    
    if(fraud_sum == 0){
      
      triangles_df$triangle_label[i] <- "legit_triangles"
      
    }else if(fraud_sum == 1 | fraud_sum == 2){
      
      triangles_df$triangle_label[i] <- "semi_fraud_triangles"
    }else{
      
      triangles_df$triangle_label[i] <- "fraud_triangles"
      
      } 
  }
  
  # changing df in long format
  triangles_df_long <- pivot_longer(triangles_df,
                                        cols = 1:3,
                                        names_to = "col_names",
                                        values_to = "client_ID"
                                        ) %>% 
  select(-col_names)
  
  # tabulating triangles result
  triangles_df_table_long <-  as.data.frame(t(table(triangles_df_long)))
  
  # converting to wide format
  triangles_df <- pivot_wider(triangles_df_table_long,
                              names_from = triangle_label,
                              values_from = Freq)
  
  }else{
    
   number_vertices <-  vcount(graph)  
   null_traingle_df <- as.data.frame(matrix(0, number_vertices, 3)) %>% 
     rename(legit_triangles = V1,
            semi_fraud_triangles = V2,
            fraud_triangles = V3)
   
   client_ID <- V(graph)$name
   
   triangles_df <- cbind(client_ID, null_traingle_df)
   
   }
  
  
  return(triangles_df)
  
}

# 2. Defining weighted network feature generation function 


undirected_weighted_network_feature_func <- function(sub_graph, verbose = c(TRUE,FALSE)){
  
  sample_network <- sub_graph
  
  # Note: Sample network refers to the current sub-graph that is used.
  
  # 1. Extracting network metrics
  
  # 1.1 Neighborhood metrics
  
  if(verbose){print("1. Calculating neighbourhood metrics")}
  
  # 1.1.1 Transitivity - (local) ratio of triangles to connected triples each vertex is part of. Can be interpreted as a probability for the network to have adjacent nodes interconnected, thus revealing the existence of tightly connected communities (or clusters, subgroups, cliques).
  
  if(verbose){print("1.1 Calculating transitivity...")}
  
  transitivity_loc_uw <- transitivity(sample_network, 
                                      type = "local",
                                      vids = V(sample_network),
                                      isolates = "zero",
                                      weights = E(sample_network)$weight) 

  # 1.1.2 Total Degree (local) - Here the degree is defined as the number of edges between connected nodes.
  
  if(verbose){print("1.2 Calculating total degree...")}
  
  total_degree_loc_uw <- degree(sample_network, 
                                       loops = F, 
                                       mode = "all")
  # Note: Loops are not counted.
  
  # 1.1.3  Calculating fraud degree
  
  # NB! Check the name and is_fraud column in the filtering operation when bigger network is used.
  
  if(verbose){print("1.3 Calculating fraud degree...")}
  
  is_fraud_degree_loc_uw <- fraud_degree_counter_func(graph = sample_network)
  
  # 1.1.4 Calculating  Non-fraud degree
  
  if(verbose){print("1.4 Calculating non-fraud degree...")}
  
  # calculating non-fraud degree
  non_fraud_degree_loc_uw <- non_fraud_degree_counter_func(graph = sample_network)
  
  # 1.1.5 Strength (weighted degree) - Summing up the edge weights of the adjacent edges for each vertex
  
  if(verbose){print("1.5 Calculating strength...")}
  
  degree_strength_loc_uw <- strength(sample_network,
                                     loops = F,
                                     mode = "all",
                                     weights = E(sample_network)$weight # NB! will change when  weights are incorporated
  )
  
  # NB!! Try to implement degree strength for fraud and non-fraud nodes (when implementing weighted network)

  # 1.1.6  Node density: considering an each node and its neighborhood of 1, then the node density is defined as the ratio of the number of edges and the number of possible edges (which you will find in a connected graph). 
  
  if(verbose){print("1.6 Calculating node density...")}
  
  node_density_loc_uw <- node_density_func(graph = sample_network)
  
  
  # 1.1.7 Relational neighbor: Assigns a probability to node i is part of class fraud or non-fraud given the class labels of node i's neighborhood. 
  
  if(verbose){print("1.7 Calculating relational neighbour...")}
  
  # results of the relational neighborhood classifier
  relational_neig_results <- relational_neighbour_classifier_func(graph = sample_network)
  
  # probability of non-fraud
  relational_neig_prob_non_fraud_loc_uw <- relational_neig_results$prob_non_fraud
  
  # probability of fraud
  relational_neig_prob_fraud_loc_uw <- relational_neig_results$prob_fraud
  
# 1.1.8 Probabilistic relational neighbor classifier
  
  if(verbose){print("1.8 Calculating probabilistic relational neighbour...")}
  
  # results of the prob relational neighborhood classifier
  prob_relational_neig_results <- prob_relational_neighbor_classifier_func(graph = sample_network,
                                                                           relational_neig_prob_fraud = relational_neig_prob_fraud_loc_uw,
                                                                           relational_neig_prob_non_fraud = relational_neig_prob_non_fraud_loc_uw)
  
  
  # probability of non-fraud
  prob_relational_neig_prob_non_fraud_loc_uw <- prob_relational_neig_results$prob_non_fraud
  
  # probability of fraud
  prob_relational_neig_prob_fraud_loc_uw <- prob_relational_neig_results$prob_fraud
  
  
  if(verbose){print("1.9 Calculating triangles...")}
  
  # 1.1.9 Triangles - Count how many triangles a vertex is part of, in a graph, or just list the triangles of a graph.
  
  # Total triangles
  
  triangles_loc_uw <- count_triangles(sample_network)
  
  # Note: This will be the total triangles Baesens et al. (2015) broke this up into: total fraud triangles, total legit triangles, and total semi-fraud triangles. 
  
  # Calculating, Fraud, Semi-Fraud, and Legit triangles
  
  sample_triangle_df <- triangle_classifier_func(sample_network)
  
  # creating dummy data frame
  sample_triangle_df_full <- data.frame(client_ID = V(sample_network)$name)
  
  # join operation
  sample_triangle_df_full <- left_join(sample_triangle_df_full,sample_triangle_df, by = "client_ID")
  
  # replacing NA values
  sample_triangle_df_full[is.na(sample_triangle_df_full)] <- 0
  

  # 1.2 Centrality metrics
  
  if(verbose){print("2. Calculating centrality metrics")}
  
  # 1.2.1 Closeness centrality and farness - Measures the average farness (inverse distance) to all other nodes. Nodes with a high closeness score have the shortest distances to all other nodes.
  
  if(verbose){print("2.1 Calculating closeness centrality and Farness...")}
  
  centrality_closeness_loc_uw <- closeness(sample_network,
                                           normalized = T, # can think of using TRUE on full network
                                           weights = E(sample_network)$weight) # Can use weights in full network  
  
  
  farness_loc_uw <- (centrality_closeness_loc_uw)^-1
  
  
  # 1.2.2 Eigenvector centrality - Eigenvector centrality scores correspond to the values of the first eigenvector of the graph adjacency matrix; these scores may, in turn, be interpreted as arising from a reciprocal process in which the centrality of each actor is proportional to the sum of the centralities of those actors to whom he or she is connected.
  
  if(verbose){print("2.2 Calculating eigenvector centrality...")}
  
  centrality_eigen_loc_uw_res <- eigen_centrality(sample_network,
                                              scale = T,
                                              directed = F,
                                              weights = E(sample_network)$weight) #weights can be included 
  
  centrality_eigen_loc_uw <- centrality_eigen_loc_uw_res$vector
  
  
  # 1.2.3 betweeness - the number of geodesics (shortest paths) going through a vertex or an edge
  
  if(verbose){print("2.3 Calculating betweeness...")}
  
  centrality_betweenness_loc_uw <-  betweenness(sample_network,
                                                directed = F,
                                                weights = E(sample_network)$weight,
                                                normalized = F) #weights can be included
  
  # 1.2.4  Average Geodesic - average length of all shortest paths (hops to nodes)
  
  if(verbose){print("2.4 Calculating average geodesic...")}
  
  geodesic_loc_uw <- distances(sample_network,
                                      mode = "all",
                                      weights = E(sample_network)$weight, # weights can be added 
                                      algorithm = "automatic")
  
  
  # excluding nodes that are unconnected
  avg_geodesic_loc_uw <- apply(geodesic_loc_uw, 1, mean)
  
  
  # 1.3 Inference algorithms 
  
  if(verbose){print("3. Calculating collective inference algorithms...")}
  
  # 1.3.1 PageRank
  
  # base PageRank algorithm
  pr_base_loc_uw_res <- page_rank(sample_network,
                            algo = "prpack",
                            directed = F,
                            damping = 0.85,
                            weights = E(sample_network)$weight) # can add weights 
  
  pr_base_loc_uw <- pr_base_loc_uw_res$vector
  
  # PageRank algorithm with emphasis on fraud nodes
  
  # defining start vector for algorithm (0 for legitimate clients and non-zero for fraudsters)
  total_fraud_nodes <- sum(V(sample_network)$is_fraud)

  
  # adding condition if there are any fraud nodes in the network then starting vector for page rank algorithm should be personalised
  if(total_fraud_nodes > 0){
  
  fraud_nodes_start_value <- 1/(total_fraud_nodes)
  start_vec_fraud <- if_else(V(sample_network)$is_fraud == F, 0, fraud_nodes_start_value)
  
  # calculating PageRank
  pr_fraud_loc_uw_res <- page_rank(sample_network,
                            algo = "prpack",
                            directed = F,
                            damping = 0.85,
                            personalized = start_vec_fraud,
                            weights = E(sample_network)$weight) # can add weights
  
  pr_fraud_loc_uw <- pr_fraud_loc_uw_res$vector
  
  }else{
    
  pr_fraud_loc_uw <-   pr_base_loc_uw
    
  }
  
  # 1.4 Compiling results in data frame
  
  if(verbose){print("Compiling calculated metrics in data frame...")}
  
  sample_network_feature_df <- data_frame(client_ID = V(sub_graph)$name,
                                          transitivity = transitivity_loc_uw,
                                          total_degree = total_degree_loc_uw,
                                          fraud_degree = is_fraud_degree_loc_uw,
                                          non_fraud_degree = non_fraud_degree_loc_uw,
                                          degree_strenght = degree_strength_loc_uw,
                                          node_density = node_density_loc_uw,
                                          relational_neighbour_not_fraud = relational_neig_prob_non_fraud_loc_uw,
                                          relational_neighbour_fraud = relational_neig_prob_fraud_loc_uw,
                                          probabilistic_relational_neighbour_not_fraud = prob_relational_neig_prob_non_fraud_loc_uw,
                                          probabilistic_relational_neighbour_fraud = prob_relational_neig_prob_fraud_loc_uw,
                                          total_triangles = triangles_loc_uw,
                                          legit_triangles = sample_triangle_df_full$legit_triangles,
                                          semi_fraud_triangles = sample_triangle_df_full$semi_fraud_triangles,
                                          fraud_triangles = sample_triangle_df_full$fraud_triangles,
                                          closeness_centrality = centrality_closeness_loc_uw,
                                          farness = farness_loc_uw,
                                          eigen_vector_centrality = centrality_eigen_loc_uw,
                                          betweeness = centrality_betweenness_loc_uw,
                                          avg_geodesic = avg_geodesic_loc_uw,
                                          page_rank_base = pr_base_loc_uw,
                                          page_rank_fraud = pr_fraud_loc_uw)
  

return(sample_network_feature_df)  

}

# undirected_weighted_network_feature_df <-  undirected_weighted_network_feature_func(sub_graph = sample_network_undirected_weighted, verbose = T)


```

# 3.3 Network construction - Directed network (non-simplified) function(s)

```{r, network construction - directed network (non-simplified) - feature engineering}

#### Directed Network feature generation function ####

# Description: The function will take as input a sub-graph with more than 2 nodes and calculate the relevant network metrics.

# input: a directed sub-graph with more than two nodes
# output: a data frame containing all the network metrics extracted from the sub-graph. 

# NB! The function will only  be able to take as input an un-directed-weighted network.

# 1. Defining additional functions that will be used within the network feature generation function

# 1.1 Degree functions

# 1.1.1 Fraud degree calculations
directed_degree_counter_func <- function(graph, fraud_selection = c(TRUE, FALSE), degree_direction = c("in", "out")){
  
  # names of all the nodes in the network
  node_names <- V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  fraud_degree_vec <- rep(NA, length(node_names))
  
  if(degree_direction == "in"){
    
    # looping to calculate for each fraud degree
    for (i in 1:length(node_names)) {
      
      # extracting temp sub-graph for a single node
      temp_sub_graph <- induced_subgraph(graph, 
                                         vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                         impl = "auto")
      
      # converting sub-graph to data_frame and filtering
      temp_df <- as_data_frame(temp_sub_graph, what = "both")
      
      temp_edge_df <- temp_df$edges %>%
        rename(name = from)
      
      temp_fraud_count <- left_join(temp_edge_df,temp_df$vertices, by = "name") %>% 
        filter(name != node_names[i] & to == node_names[i] & is_fraud == fraud_selection) %>% 
        nrow() 
      
      # saving fraud degree
      fraud_degree_vec[i] <- temp_fraud_count
    }
  }
  
  if(degree_direction == "out"){
    
    # looping to calculate for each fraud degree
    for (i in 1:length(node_names)) {
      
      # extracting temp sub-graph for a single node
      temp_sub_graph <- induced_subgraph(graph, 
                                         vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                         impl = "auto")
      
      # converting sub-graph to data_frame and filtering
      temp_df <- as_data_frame(temp_sub_graph, what = "both")
      
      temp_edge_df <- temp_df$edges %>%
        rename(name = to)
      
      temp_fraud_count <- left_join(temp_edge_df,temp_df$vertices, by = "name") %>% 
        filter(name != node_names[i] & from == node_names[i] & is_fraud == fraud_selection) %>% 
        nrow() 
      
      # saving fraud degree
      fraud_degree_vec[i] <- temp_fraud_count
    }
  }
  
  return(fraud_degree_vec)
  
}

# 2. Defining directed network feature generation function 

directed_network_feature_function <- function(sub_graph, verbose = c(TRUE, FALSE)){
  
  # 1. Extracting network metrics
  
  if(verbose == T){print("1. Calculating neighbourhood metrics (directed)...")}
  
  # 1.1 Neighborhood metrics
  
  if(verbose == T){print("1.1 Calculating in-degree...")}
  # 1.1.1 in-degree
  in_degree_loc_d <- degree(sub_graph, 
                            mode = "in",
                            loops = F,
                            normalized = F)
  
  if(verbose == T){print("1.2 Calculating out-degree...")}
  
  # 1.1.2 out-degree
  out_degree_loc_d <- degree(sub_graph, 
                            mode = "out",
                            loops = F,
                            normalized = F)
  
  # 1.1.3 in-degree (fraudulent/non-fraudulent clients)
  
  if(verbose == T){print("1.3 Calculating fraud in-degree...")}
  # number of transactions made by fraudulent node to the specific reference node
  in_degree_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = T, degree_direction = "in")
  
  if(verbose == T){print("1.4 Calculating non-fraud in-degree...")}
  # number of transactions made by non-fraudulent node to the specific reference node
  in_degree_non_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = F, degree_direction = "in")
  
  # 1.1.4 out-degree (fraudulent/non-fraudulent clients)
  
  if(verbose == T){print("1.5 Calculating fraud out-degree...")}
  # number of transactions made by fraudulent node to the specific reference node
  out_degree_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = T, degree_direction = "out")
  
  if(verbose == T){print("1.6 Calculating non-fraud out-degree...")}
  # number of transactions made by non-fraudulent node to the specific reference node
  out_degree_non_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = F, degree_direction = "out")
  
  
  # 1.2 Compiling results in data frame
  
  if(verbose){print("Compiling calculated metrics in data frame...")}
  
  sample_network_feature_df <- data_frame(client_ID = V(sub_graph)$name,
                                          in_degree = in_degree_loc_d,
                                          in_degree_fraud = in_degree_fraud_loc_d,
                                          in_degree_non_fraud = in_degree_non_fraud_loc_d,
                                          out_degree = out_degree_loc_d,
                                          out_degree_fraud = out_degree_fraud_loc_d,
                                          out_degree_non_fraud = out_degree_non_fraud_loc_d
                                          )
  
return(sample_network_feature_df) 
  
  
  
  }

#directed_network_feature_df <-  directed_network_feature_function(sub_graph = sample_network_directed_weighted, verbose = T)


```

# 3.4 Feature extraction - Transactional data function(s)

```{r, feature extraction - transactional data function(s) - feature engineering}

#### Transaction feature generation function ####

# Description: The function will take as input the accounts_df and the transactions df and output the metrics  

# input: accounts_df and the transactions df
# output: a data frame containing transaction metrics 

transaction_feature_function <- function(transaction_df, accounts_df){ 
  
  # Can drill each of down into fraud and non-fraud
  
  # 1. Generating incoming transaction stats
  
  # basic incoming stats
  incoming_transaction_stats <- transaction_df %>% 
    mutate(is_round = if_else(plyr::round_any(weight, 10) == weight, T, F)) %>% 
    group_by(to) %>% 
    summarise(incoming_total = sum(weight),
              incoming_avg = mean(weight),
              incoming_sd = sd(weight),
              incoming_median = median(weight),
              incoming_max = max(weight),
              incoming_min = min(weight),
              incoming_round_numbers_count = sum(is_round))
  
  # fraud/non-fraud incoming transaction totals - receiving from fraudsters/non-fraudsters
  incoming_transactions_fraud_non_fraud <- transaction_df %>% 
    rename(client_ID = from) %>% 
    left_join(accounts_df, by = "client_ID")%>% 
    group_by(to, is_fraud) %>% 
    summarise(total = sum(weight)) %>% 
    pivot_wider(names_from = is_fraud,
                values_from = total) %>% 
    rename(fraud_total_income = `TRUE`,
           non_fraud_total_income = `FALSE`)
  
  # removing NA's
  incoming_transactions_fraud_non_fraud[is.na(incoming_transactions_fraud_non_fraud)] <- 0
  
  # joining and creating final df 
  incoming_transaction_stats_final <- left_join(incoming_transaction_stats,incoming_transactions_fraud_non_fraud, by = "to" ) %>% 
    rename(client_ID = to)
  
  
  # 2. Generating outgoing transaction stats
  
  # basic outgoing stats
  outgoing_transaction_stats <- transaction_df %>% 
    mutate(is_round = if_else(plyr::round_any(weight, 10) == weight, T, F)) %>%
    group_by(from) %>% 
    summarise(outgoing_total = sum(weight),
              outgoing_avg = mean(weight),
              outgoing_sd = sd(weight),
              outgoing_median = median(weight),
              outgoing_max = max(weight),
              outgoing_min = min(weight),
              outgoing_round_numbers_count = sum(is_round))
  
  # fraud/non-fraud incoming transaction totals - paying fraudsters/non-fraudsters
  outgoing_transactions_fraud_non_fraud <- transaction_df %>% 
    rename(client_ID = to) %>% 
    left_join(accounts_df, by = "client_ID")%>% 
    group_by(from, is_fraud) %>% 
    summarise(total = sum(weight)) %>% 
    pivot_wider(names_from = is_fraud,
                values_from = total) %>% 
    rename(total_fraud_payments = `TRUE`,
           total_non_fraud_payments = `FALSE`)
  
  # removing NA's
  outgoing_transactions_fraud_non_fraud[is.na(outgoing_transactions_fraud_non_fraud)] <- 0
  
  # joining and creating final df 
  outgoing_transaction_stats_final <- left_join(outgoing_transaction_stats,outgoing_transactions_fraud_non_fraud, by = "from" ) %>% 
    rename(client_ID = from)
  
  #### Period stats (in funcrion) ####
  
  # 3. Generating time period stats for accounts
  
  # 3.1 payment made (from)
  
  # all accounts that made a payment
  payment_IDs <- unique(transaction_df$from)
  
  # data frame that stores the results
  payment_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        payment_period_max = rep(NA, length(payment_IDs)),
                                        payment_period_min = rep(NA, length(payment_IDs)),
                                        payment_period_avg = rep(NA, length(payment_IDs)),
                                        payment_period_std = rep(NA, length(payment_IDs)))
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(from == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA,nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- max(current_days_difference, na.rm =T)
      payment_period_stats_df$payment_period_min[i] <- min(current_days_difference, na.rm =T) + 1
      payment_period_stats_df$payment_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      payment_period_stats_df$payment_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- 1
      payment_period_stats_df$payment_period_min[i] <- 1 
      payment_period_stats_df$payment_period_avg[i] <- 1 
      payment_period_stats_df$payment_period_std[i] <- 0 
      
    }
    
  }
  
  # 3.2 payment received (to)
  
  # all accounts that received a payment
  payment_IDs <- unique(transaction_df$to)
  
  # data frame that stores the results
  receive_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        receive_period_max = rep(NA, length(payment_IDs)),
                                        receive_period_min = rep(NA, length(payment_IDs)),
                                        receive_period_avg = rep(NA, length(payment_IDs)),
                                        receive_period_std = rep(NA, length(payment_IDs)))
  
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(to == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA, nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- max(current_days_difference, na.rm =T)
      receive_period_stats_df$receive_period_min[i] <- min(current_days_difference, na.rm =T) + 1 # Assumption that payment is made in the morning
      receive_period_stats_df$receive_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      receive_period_stats_df$receive_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- 1
      receive_period_stats_df$receive_period_min[i] <- 1 
      receive_period_stats_df$receive_period_avg[i] <- 1 
      receive_period_stats_df$receive_period_std[i] <- 0 
      
    }
    
  }
  
  
  # 4. Creating transaction feature df
  
  transaction_feature_df <- data.frame(client_ID = accounts_df$client_ID,
                                       init_balance = accounts_df$init_balance)
  
  transaction_feature_df <- left_join(transaction_feature_df, incoming_transaction_stats_final, by = "client_ID") %>% 
    left_join(outgoing_transaction_stats_final, by = "client_ID") %>% 
    left_join(receive_period_stats_df, by = "client_ID") %>% 
    left_join(payment_period_stats_df, by = "client_ID")
  
  # replacing NA values
  
  cols_to_replace <- c("incoming_total", "incoming_avg", "incoming_max", "incoming_min", "incoming_round_numbers_count", "non_fraud_total_income", "fraud_total_income" , "outgoing_total", "outgoing_avg", "outgoing_max", "outgoing_min", "outgoing_round_numbers_count", "total_non_fraud_payments", "total_fraud_payments", "receive_period_min", "payment_period_min", "receive_period_max" , "payment_period_max", "payment_period_avg", "receive_period_avg")
  
  transaction_feature_df[cols_to_replace][is.na(transaction_feature_df[cols_to_replace])] <- 0
  
  # Note: SD and Medians have NA values
  
  return(transaction_feature_df)
  
}


```

# 3.5  Constructing structured data tables

*Note:*
1. For the network feature data set a function will:
 i) Extract the transaction features from data.
 ii) Construct the needed type of networks (weighted-undirected network & Directed network).
 iii) For each graph component (V > 2) extract the weighted-undirected features & directed features.

2. The transactional and combined feature data sets will be generated at the end of the chunk (does not form part of above function)  

```{r, constructing structured data tables - feature engineering}

# 1. Defining function that outputs final pre-processed data

feature_generation_func <- function(final_transactional_df, final_accounts_df) {
  
  # 1. Extracting transaction features
  
  print("*** Generating transactional data features ***")
  
  transaction_feature_df <- transaction_feature_function(transaction_df = final_transactional_df , accounts_df = final_accounts_df)
  transaction_feature_df$client_ID <- as.character(transaction_feature_df$client_ID)
  
  # 2. Constructing needed networks
  
  print("*** Constructing networks ***")
  
  # 2.1 Un-directed-weighted network
  
  # constructing raw network
  undirected_weighted_graph_raw <- graph_from_data_frame(d = final_transactional_df,
                                                               directed = F,
                                                               vertices = final_accounts_df) 
  
  # Simplifying the graph (removing multiple edges)
  undirected_weighted_graph_simp <- simplify(undirected_weighted_graph_raw,
                                                   remove.multiple = T,
                                                   remove.loops = T,
                                                   edge.attr.comb = c(weight = "sum"))
  
  # 2.2 Directed graph
  
  weight_ind <- which(names(final_transactional_df) == 'weight')
  
  directed_graph <- graph_from_data_frame(d = final_transactional_df[,-weight_ind],
                                                directed = T,
                                                vertices = final_accounts_df)
  
  # 3. Extract the network features for each component in the generated  graphs
  
  print("*** Extracting network features from each component in the graph(s) ***")
  
  # 3.1 Un-directed weighted graph
  
  print("* Un-direceted weighted graph  *")
  
  # decomposing graph into its graph components
  undirected_weighted_graph_components <- decompose.graph(graph = undirected_weighted_graph_simp,
                                                                min.vertices = 3) 
  
  # creating loop to extract the network features from each graph component
  
  undirected_weighted_graph_features <- data.frame()
  
  for (i in 1:length(undirected_weighted_graph_components)) {
    
    print(paste0("Busy with graph component ", i) )
    
    # assigning the ith component
    current_undirected_weighted_component <- undirected_weighted_graph_components[[i]]
    
    # extracting network features
    current_undirected_weighted_graph_features <- undirected_weighted_network_feature_func(sub_graph = current_undirected_weighted_component, verbose = T)
    
    # constructing final feature df
    undirected_weighted_graph_features <- rbind(undirected_weighted_graph_features, current_undirected_weighted_graph_features)
  
  }
  
  # Note: The number of accounts we are investigating decreased due to graph components that have less than 3 vertices are ommitted
  
  # 3.2 Directed graph
  
  print("* Direceted graph  *")

  # decomposing graph into its graph components
  directed_graph_components <- decompose.graph(graph = directed_graph,
                                                     min.vertices = 3) 
  
  # Note: Multiple graph components identified
  
  # creating loop to extract the network features from each graph component
  
  directed_graph_features <- data.frame()
  
  for (i in 1:length(directed_graph_components)) {
    
    print(paste0("Busy with graph component ", i) )
    
    # assigning the ith component
    current_directed_component <- directed_graph_components[[i]]
    
    # extracting network features
    current_directed_graph_features <- directed_network_feature_function(sub_graph = current_directed_component, verbose = T)
    
    # constructing final feature df
    directed_graph_features <- rbind(directed_graph_features, current_directed_graph_features)
    
  }

  
  # 4. Combing generated data tables into one feature df
  
  print("*** Creating feature data tables  ***")
  
  client_df <- data.frame(client_ID = as.character(final_accounts_df$client_ID),
                               is_fraud = final_accounts_df$is_fraud)
  
  network_feature_df <- left_join(undirected_weighted_graph_features, directed_graph_features, by = "client_ID") %>%  
    left_join(client_df, by = "client_ID")
  
  network_feature_clients <- data.frame(client_ID = network_feature_df$client_ID)
  
  transaction_feature_df <- left_join(network_feature_clients, transaction_feature_df, by = "client_ID") %>% 
    left_join(client_df, by = "client_ID")

  feature_tables <- list(network_features = network_feature_df,
                         transactional_features = transaction_feature_df) 
  
  return(feature_tables)
  
}



# 2. Configuring training data features

training_features <- feature_generation_func(final_transactional_df = training_transactions_final,
                                             final_accounts_df = training_accounts_final)

train_network_features <- training_features$network_features
train_transactional_features <- training_features$transactional_features

train_fraud_ind <- which(names(train_network_features) == "is_fraud")
train_all_features <- left_join(train_network_features[,-train_fraud_ind], train_transactional_features, by = "client_ID")

# 3. Configuring testing data features

testing_features <- feature_generation_func(final_transactional_df = testing_transactions_final,
                                             final_accounts_df = testing_accounts_final)

test_network_features <- testing_features$network_features
test_transactional_features <- testing_features$transactional_features

test_fraud_ind <- which(names(train_network_features) == "is_fraud")
test_all_features <- left_join(test_network_features[,-test_fraud_ind], test_transactional_features, by = "client_ID")


```

# 4. EDA of structured data table(s)


```{r, EDA of structured data}

#(can include how network evolve over time - if there is time)

```

# 5. Modelling

In the following code chunks we will examine the performance of different models:
i) Only using network features as inputs (network feature data set).
ii) Only using transactional features as inputs (transactional feature data set).
iii) Using network and transactional features (combined feature data set).

# 5.1 Logistic regression models 

# 5.1.1 Data pre-processing

```{r, data pre-processing - logistic regression models - modelling}

# 1. Data-pre processing for LR model

# 1.1 transforming response to factor variable
train_network_features$is_fraud <- as.factor(train_network_features$is_fraud)
train_transactional_features$is_fraud <- as.factor(train_transactional_features$is_fraud)
train_all_features$is_fraud <- as.factor(train_all_features$is_fraud)

test_network_features$is_fraud <- as.factor(test_network_features$is_fraud)
test_transactional_features$is_fraud <- as.factor(test_transactional_features$is_fraud)
test_all_features$is_fraud <- as.factor(test_all_features$is_fraud)

# 1.2 omitting all columns that have NA values
train_transactional_features <- train_transactional_features[, colSums(is.na(train_transactional_features)) == 0]
train_all_features <- train_all_features[, colSums(is.na(train_all_features)) == 0]

test_transactional_features <- test_transactional_features[, colSums(is.na(test_transactional_features)) == 0]
test_all_features <- test_all_features[, colSums(is.na(test_all_features)) == 0]

# 1.3 scaling features
train_network_features[,2:28] <- scale(train_network_features[,2:28])
train_transactional_features[,2:22] <- scale(train_transactional_features[,2:22])
train_all_features[,2:49] <- scale(train_all_features[,2:49])

test_network_features[,2:28] <- scale(test_network_features[,2:28])
test_transactional_features[,2:22] <- scale(test_transactional_features[,2:22])
test_all_features[,2:49] <- scale(test_all_features[,2:49])

# 1.4 dropping client_ID column
train_network_features <- train_network_features[,-1]
train_transactional_features <- train_transactional_features[,-1]
train_all_features <- train_all_features[,-1]

test_network_features <- test_network_features[,-1]
test_transactional_features <- test_transactional_features[,-1]
test_all_features <- test_all_features[,-1]


```


# 5.1.2 Hyper-parameter tuning 

```{r, hyper-parameter tuning - logistic regression models - modelling}

# 1. Constructing standard logistic regression models

# network features LR model
log_std_model_network <- glm(is_fraud ~ . , data = train_network_features, family = "binomial")
summary(log_std_model_network)
round(exp(coef(log_std_model_network)),3)

# transactional features LR model
log_std_model_trans <- glm(is_fraud ~ . , data = train_transactional_features, family = "binomial")
summary(log_std_model_trans)
round(exp(coef(log_std_model_trans)),3)

# all features LR model
log_std_model_all <- glm(is_fraud ~ . , data = train_all_features, family = "binomial")
summary(log_std_model_all)
round(exp(coef(log_std_model_all)),3)

# 2. Constructing logistic regression with LASSO penalty

# network features
log_LASSO_model_network <- glmnet(x = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                                                y = train_network_features$is_fraud , 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_network, xvar = 'lambda', label=TRUE)1


# transaction features

log_LASSO_model_trans <- glmnet(x = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                                                y = train_transactional_features$is_fraud, 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_trans, xvar = 'lambda', label=TRUE)


# all features

log_LASSO_model_all <- glmnet(x = as.matrix(train_all_features[,-c(1,ncol(train_all_features))]),
                                                y = train_all_features$is_fraud, 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_all, xvar = 'lambda', label=TRUE)



# 3.1 Applying 10-fold CV to determine optimal lambda value

# network features
log_cv_model_network <- cv.glmnet(x = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                               y = train_network_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_network)
log_cv_model_network$lambda.1se

test <- log_cv_model_network$lambda

# transactional features
log_cv_model_trans <- cv.glmnet(x = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                               y = train_transactional_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_trans)
log_cv_model_trans$lambda.1se

# all features
log_cv_model_all <- cv.glmnet(x = as.matrix(train_all_features[,-c(ncol(train_all_features))]),
                               y = train_all_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_all)
log_cv_model_all$lambda.1se


# 4. Probability Predictions (Training)

# 4.1 Network features

# Regular LR model
pi_hat_log_std_network <- predict(log_std_model_network, type = "response")

# LASSO LR model
pi_hat_log_lasso_network <- predict(log_cv_model_network, 
                                    newx = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                                    s = log_cv_model_network$lambda.1se,
                                    type = "response")


# 4.2 Transactions features

# Regular LR model
pi_hat_log_std_trans <- predict(log_std_model_trans, type = "response")

# LASSO LR model
pi_hat_log_lasso_trans <- predict(log_cv_model_trans, 
                                    newx = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                                    s = log_cv_model_trans$lambda.1se,
                                    type = "response")


# 4.3 all features
pi_hat_log_std_all <- predict(log_std_model_all, type = "response")

# LASSO LR model
pi_hat_log_lasso_all <- predict(log_cv_model_all, 
                                    newx = as.matrix(train_all_features[,-c(ncol(train_all_features))]),
                                    s = log_cv_model_all$lambda.1se,
                                    type = "response")


# 5. Choosing threshold value

# 5.1 Defining function that extract the optimal threshold value according to ROC and precision recall curves

# 5.1.1 ROC
optimal_threshold_ROC <- function(model_performance){
  
  # 1. Calculating G-mean (Geometric mean)
  gmeans <- sqrt(model_performance@y.values[[1]]*(1-model_performance@x.values[[1]]) )
  
  # 2. Extracting threshold values
  thresholds <- model_performance@alpha.values[[1]]
  
  # 3. Getting index where geometric mean is the highes
  max_gmean_ind <- which(gmeans == max(gmeans, na.rm = T))
  
  # Extracting Index
  optimal_ROC_threshold <- thresholds[max_gmean_ind]
  
  if(length(optimal_ROC_threshold) > 1){
    
    optimal_ROC_threshold <- optimal_ROC_threshold[1]
    
  }

  
  # 4. Plotting results
  plot(model_performance, colorize = FALSE, col = 'black')
  lines(c(0,1), c(0,1), col = 'gray', lty = 4)
  points(model_performance@x.values[[1]][max_gmean_ind] , model_performance@y.values[[1]][max_gmean_ind] , col = "red", pch = 19)
  text(model_performance@x.values[[1]][max_gmean_ind] , model_performance@y.values[[1]][max_gmean_ind], labels = round(optimal_ROC_threshold,3), col = "red", pos = 3)
  legend("bottomright", legend=c("Optimal threshold", "Standard logistic regression model", "No skill classifier"), col=c("red", "black", "gray"), pch = c(19, NA, NA), lty = c(0,1,4))
  
   ROC_plot <- recordPlot()
   plot.new()
  
  
  ROC_threshold_res <- list(optimal_threshold = optimal_ROC_threshold,
                            tpr = model_performance@y.values[[1]][max_gmean_ind],
                            fpr = model_performance@x.values[[1]][max_gmean_ind],
                            max_G_mean = max(gmeans, na.rm = T),
                            ROC_plot = ROC_plot )
  
  return(ROC_threshold_res)
  
}


# 5.1.2 Precision-recall  
optimal_threshold_PR <- function(model_performance){
  
  
  # 1. Calculating PR F-measure
  f_measure <- (2 * model_performance@y.values[[1]] * model_performance@x.values[[1]])/(model_performance@x.values[[1]] + model_performance@y.values[[1]])
  
  # 2. Find index of largest F-measure
  max_f_ind <- which(f_measure == max(f_measure, na.rm = T))
  
  # 3. Optimal threshold value
  
  optimal_threshold <- model_performance@alpha.values[[1]][max_f_ind]
  
  # 4. Plotting results
  plot(model_performance, colorize = FALSE, col = 'black', ylim = c(-0.05,1))
  abline(h=0, col="gray", lty = 4)
  points(model_performance@x.values[[1]][max_f_ind] , model_performance@y.values[[1]][max_f_ind] , col = "red", pch = 19)
  text(model_performance@x.values[[1]][max_f_ind] , model_performance@y.values[[1]][max_f_ind], labels = round(optimal_threshold,3), col = "red", pos = 3)
  legend("left",box.lty=0, inset = 0.02, legend=c("Optimal threshold", "Standard logistic regression model", "No skill classifier"), col=c("red", "black", "gray"), pch = c(19, NA, NA), lty = c(0,1,4))
  
  PR_plot <- recordPlot()
  plot.new()
  
  PR_threshold_res <- list(optimal_threshold = optimal_threshold,
                           recall = model_performance@y.values[[1]][max_f_ind],
                           precision = model_performance@x.values[[1]][max_f_ind],
                           max_f_measure = max(f_measure, na.rm = T),
                           PR_plot = PR_plot)
  
  return(PR_threshold_res)
  
}


# 5.2 Calculating performance of each model based on ROC curve and optimal threshold value

# 5.2.1  Network features

# standard LR
pred_std_network <- prediction(pi_hat_log_std_network, train_network_features$is_fraud)
perf_std_network <- performance(pred_std_network, "tpr", "fpr")


# LASSO LR
pred_lasso_network <- prediction(pi_hat_log_lasso_network, train_network_features$is_fraud)
perf_lasso_network <- performance(pred_lasso_network, "tpr", "fpr")


# 5.2.2 Transactional features

# standard LR
pred_std_trans <- prediction(pi_hat_log_std_trans, train_transactional_features$is_fraud)
perf_std_trans <- performance(pred_std_trans, "tpr", "fpr")

# LASSO LR
pred_lasso_trans <- prediction(pi_hat_log_lasso_trans, train_transactional_features$is_fraud)
perf_lasso_trans <- performance(pred_lasso_trans, "tpr", "fpr")


# 5.2.3 all features

# standard LR
pred_std_all <- prediction(pi_hat_log_std_all, train_all_features$is_fraud)
perf_std_all <- performance(pred_std_all, "tpr", "fpr")


# LASSO LR
pred_lasso_all <- prediction(pi_hat_log_lasso_all, train_all_features$is_fraud)
perf_lasso_all <- performance(pred_lasso_all, "tpr", "fpr")


# Calculating optimal threshold values according to ROC curves 

#  Network features
threshold_std_network <- optimal_threshold_ROC(perf_std_network)
threshold_lasso_network <- optimal_threshold_ROC(perf_lasso_network)

#  transaction features
threshold_std_trans <- optimal_threshold_ROC(perf_std_trans)
threshold_lasso_trans <- optimal_threshold_ROC(perf_lasso_trans)

# transaction features
threshold_std_all <- optimal_threshold_ROC(perf_std_all)
threshold_lasso_all <- optimal_threshold_ROC(perf_lasso_all)


# 5.3 Calculating performance of each model based on precision-recall curves and determining optimal threshold value

# Note: Unlike the ROC Curve, a precision-recall curve focuses on the performance of a classifier on the positive (minority class) only.

# 5.3.1  Network features

# standard LR
perf_std_network_pr <- performance(pred_std_network, "prec", "rec")

# LASSO LR

perf_lasso_network_pr <- performance(pred_lasso_network, "prec", "rec")

# 5.3.2 Transactional features

# standard LR
perf_std_trans_pr <- performance(pred_std_trans, "prec", "rec")

# LASSO LR
perf_lasso_trans_pr <- performance(pred_lasso_trans, "prec", "rec")

# 5.3.3 all features

# standard LR
perf_std_all_pr <- performance(pred_std_all, "prec", "rec")

# LASSO LR
perf_lasso_all_pr <- performance(pred_lasso_all, "prec", "rec")


# Calculating optimal threshold values according to PR curves 

#  Network features
threshold_std_network_pr <- optimal_threshold_PR(perf_std_network_pr)
threshold_lasso_network_pr <- optimal_threshold_PR(perf_lasso_network_pr)

#  transaction features
threshold_std_trans_pr <- optimal_threshold_PR(perf_std_trans_pr)
threshold_lasso_trans_pr <- optimal_threshold_PR(perf_lasso_trans_pr)

# transaction features
threshold_std_all_pr <- optimal_threshold_PR(perf_std_all_pr)
threshold_lasso_all_pr <- optimal_threshold_PR(perf_lasso_all_pr)

# 6. Calculating the models performance on training set

Y_train <- train_network_features$is_fraud

# 6.1 Network features

# standard LR
y_hat_log_std_network_ROC <- if_else(pi_hat_log_std_network >= threshold_std_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_network_PR <- if_else(pi_hat_log_std_network >= threshold_std_network_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_network_ROC <- if_else(pi_hat_log_lasso_network >= threshold_lasso_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_network_PR <- if_else(pi_hat_log_lasso_network >= threshold_lasso_network_pr$optimal_threshold, "TRUE", "FALSE" )

# 6.2 Transactional features

# standard LR
y_hat_log_std_trans_ROC <- if_else(pi_hat_log_std_trans >= threshold_std_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_trans_PR <- if_else(pi_hat_log_std_trans >= threshold_std_trans_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_trans_ROC <- if_else(pi_hat_log_lasso_trans >= threshold_lasso_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_trans_PR <- if_else(pi_hat_log_lasso_trans >= threshold_lasso_trans_pr$optimal_threshold, "TRUE", "FALSE" )


# 6.3 All features

# standard LR
y_hat_log_std_all_ROC <- if_else(pi_hat_log_std_all >= threshold_std_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_all_PR <- if_else(pi_hat_log_std_all >= threshold_std_all_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_all_ROC <- if_else(pi_hat_log_lasso_all >= threshold_lasso_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_all_PR <- if_else(pi_hat_log_lasso_all >= threshold_lasso_all_pr$optimal_threshold, "TRUE", "FALSE" )


# saving all confusion matrices of all the models in a list

log_train_results <- list(std_network_ROC = table(y_hat_log_std_network_ROC, Y_train),
                          std_network_PR = table(y_hat_log_std_network_PR, Y_train),
                          lasso_network_ROC = table(y_hat_log_lasso_network_ROC, Y_train),
                          lasso_network_PR = table(y_hat_log_lasso_network_PR, Y_train),
                          std_trans_ROC = table(y_hat_log_std_trans_ROC, Y_train),
                          std_trans_PR = table(y_hat_log_std_trans_PR, Y_train),
                          lasso_trans_ROC = table(y_hat_log_lasso_trans_ROC, Y_train),
                          lasso_trans_PR = table(y_hat_log_lasso_trans_PR, Y_train),
                          std_all_ROC = table(y_hat_log_std_all_ROC, Y_train),
                          std_all_PR = table(y_hat_log_std_all_PR, Y_train),
                          lasso_all_ROC = table(y_hat_log_lasso_all_ROC, Y_train),
                          lasso_all_PR = table(y_hat_log_lasso_all_PR, Y_train))

# defining function that calculates classification accuracy

clas_accuracy_func <- function(conf_matrix_list){
  
  # 1. Creating vector that will contain the classification accuracy 
  class_accur_res <- rep(NA, length(conf_matrix_list))
  
  # 2. Loop through to calculate classification accuracy
  for (i in 1:length(conf_matrix_list)) {
    
    current_confusion_mat <- conf_matrix_list[[i]]
    
    current_class_accuracy <- round((current_confusion_mat[1,1] + current_confusion_mat[2,2])/sum(current_confusion_mat),3)
    
    class_accur_res[i] <- current_class_accuracy
    
  }
  
  result_df <- data.frame(log_model = names(conf_matrix_list),
                          class_accur = class_accur_res,
                          lasso_aaplied = rep(c(F,F,T,T),3),
                          threshold_tuning = rep(c("ROC threshold-tuning","Precision-recall threshold-tuning"),6))
  
  return(result_df)
  
}

# calculating training result

log_train_res <- clas_accuracy_func(log_train_results)


# visualizing results

log_train_res %>% 
ggplot(aes(x = reorder(log_model, -class_accur), y = class_accur, group = 1, colour = lasso_aaplied)) +
  geom_point() +
  geom_line(colour = "gray") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(colour = "Lasso applied", x = "Logistic regression models", y = "Classification accuracy" ) +
  facet_wrap(threshold_tuning~.)
  

# 7. Testing models performance

# 7.1 Probability predictions with test data set

# 7.1.1 Network features

# Regular LR model
pi_hat_log_std_network_test <- predict(log_std_model_network, newdata = test_network_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_network_test <- predict(log_cv_model_network, 
                                    newx = as.matrix(test_network_features[,-c(ncol(test_network_features))]),
                                    s = log_cv_model_network$lambda.1se,
                                    type = "response")

# 7.1.2 Transactions features

# Regular LR model
pi_hat_log_std_trans_test <- predict(log_std_model_trans, newdata = test_transactional_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_trans_test <- predict(log_cv_model_trans, 
                                    newx = as.matrix(test_transactional_features[,-c(ncol(test_transactional_features))]),
                                    s = log_cv_model_trans$lambda.1se,
                                    type = "response")


# 7.1.3 all features
pi_hat_log_std_all_test <- predict(log_std_model_all, newdata = test_all_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_all_test <- predict(log_cv_model_all, 
                                    newx = as.matrix(test_all_features[,-c(ncol(test_all_features))]),
                                    s = log_cv_model_all$lambda.1se,
                                    type = "response")

# 8. Actual test prediction given probabilities

Y_test <- test_network_features$is_fraud

# standard LR
y_hat_log_std_network_ROC_test <- if_else(pi_hat_log_std_network_test >= threshold_std_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_network_PR_test <- if_else(pi_hat_log_std_network_test >= threshold_std_network_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_network_ROC_test <- if_else(pi_hat_log_lasso_network_test >= threshold_lasso_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_network_PR_test <- if_else(pi_hat_log_lasso_network_test >= threshold_lasso_network_pr$optimal_threshold, "TRUE", "FALSE" )

# 6.2 Transactional features

# standard LR
y_hat_log_std_trans_ROC_test <- if_else(pi_hat_log_std_trans_test >= threshold_std_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_trans_PR_test <- if_else(pi_hat_log_std_trans_test >= threshold_std_trans_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_trans_ROC_test <- if_else(pi_hat_log_lasso_trans_test >= threshold_lasso_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_trans_PR_test <- if_else(pi_hat_log_lasso_trans_test >= threshold_lasso_trans_pr$optimal_threshold, "TRUE", "FALSE" )


# 6.3 All features

# standard LR
y_hat_log_std_all_ROC_test <- if_else(pi_hat_log_std_all_test >= threshold_std_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_all_PR_test <- if_else(pi_hat_log_std_all_test >= threshold_std_all_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_all_ROC_test <- if_else(pi_hat_log_lasso_all_test >= threshold_lasso_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_all_PR_test <- if_else(pi_hat_log_lasso_all_test >= threshold_lasso_all_pr$optimal_threshold, "TRUE", "FALSE" )


# saving all confusion matrices of all the models in a list

log_test_results <- list(std_network_ROC = table(y_hat_log_std_network_ROC_test, Y_test),
                          std_network_PR = table(y_hat_log_std_network_PR_test, Y_test),
                          lasso_network_ROC = table(y_hat_log_lasso_network_ROC_test, Y_test),
                          lasso_network_PR = table(y_hat_log_lasso_network_PR_test, Y_test),
                          std_trans_ROC = table(y_hat_log_std_trans_ROC_test, Y_test),
                          std_trans_PR = table(y_hat_log_std_trans_PR_test, Y_test),
                          lasso_trans_ROC = table(y_hat_log_lasso_trans_ROC_test, Y_test),
                          lasso_trans_PR = table(y_hat_log_lasso_trans_PR_test, Y_test),
                          std_all_ROC = table(y_hat_log_std_all_ROC_test, Y_test),
                          std_all_PR = table(y_hat_log_std_all_PR_test, Y_test),
                          lasso_all_ROC = table(y_hat_log_lasso_all_ROC_test, Y_test),
                          lasso_all_PR = table(y_hat_log_lasso_all_PR_test, Y_test))

# calculating training result

log_test_res <- clas_accuracy_func(log_test_results)


# visualizing results

log_test_res %>% 
ggplot(aes(x = reorder(log_model, -class_accur), y = class_accur, group = 1, colour = lasso_aaplied)) +
  geom_point() +
  geom_line(colour = "gray") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(colour = "Lasso applied", x = "Logistic regression models", y = "Classification accuracy" ) +
  facet_wrap(threshold_tuning~.)



```

# 5.2 Neural network models

# 5.2.1 Data-pre-processing

```{r, data pre-processing -  neural network models - modelling}

# Note: The approach is to start simple and build up in complexity. For each NN model the batch size and number of epochs needs to be determined. 

# 1. pre-processing of testing and training data

# 1.1 Configuring training data 

train_target_nn <- train_network_features %>% 
  select(is_fraud) %>% 
  mutate(is_fraud = if_else(is_fraud == T, 1,0)) %>% 
  to_categorical()

# Note: The training target will be the same for all data sets (above we used the network feature data set but we could have either one of the other two).

# 1.1.1 removing client_ID, assigning a numeric value to response, and scaling features

# network features 
train_network_features_nn <- train_network_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# transaction features 

## omitting all columns that have NA values
train_transactional_features <- train_transactional_features[, colSums(is.na(train_transactional_features)) == 0]

train_transactional_features_nn <- train_transactional_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# All features

## omitting all columns that have NA values
train_all_features <- train_all_features[, colSums(is.na(train_all_features)) == 0]

train_all_features_nn <- train_all_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()


# 1.2 Configuring testing data

test_target_nn <- test_network_features %>% 
  select(is_fraud) %>% 
  mutate(is_fraud = if_else(is_fraud == T, 1,0)) %>% 
  to_categorical()


# 1.2.1 removing client_ID, assigning a numeric value to response, and scaling features

# network features
test_network_features_nn <- test_network_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# transaction features 

## omitting all columns that have NA values
test_transactional_features <- test_transactional_features[, colSums(is.na(test_transactional_features)) == 0]

test_transactional_features_nn <- test_transactional_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# All features

## omitting all columns that have NA values
test_all_features <- test_all_features[, colSums(is.na(test_all_features)) == 0]

test_all_features_nn <- test_all_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

```


5.2.2 Hyper-parameter tuning

[old approach] 

```{r, hyper-parameter tuning -  neural network models - modelling}

# Note: This code chunk will be reused for all three data sets that need to be evaluated. Only the inputs to the "experiment.R" will change slighly.


# 1. Defining first experiment:

# Establishing broad range of choices for hyper-parameters:
# i) Epochs
# ii) Batch size
# iii) 1st layer hidden units

epochs_vec <- round(seq(from = 5, to = 100, length.out = 10), 0)
batch_size_vec <- seq(from = 10, to = 100, length.out = 10 )
dense_units_vec <- c(8,16,32,64,128)

start <- Sys.time()
runs_1 <- tuning_run("experiment_1.R",
                   flags = list(batch_size = batch_size_vec,
                                epochs = epochs_vec,
                                dense_units = dense_units_vec)) 

end <- Sys.time()

# Note: Took 12 hours to run

### TEMP ####
runs_1 <- ls_runs(runs_dir = "exp_1_runs")
### TEMP ####

# 1.1 Exploring results for experiment via visualisations

# Note: The following plots should give an indication as to what combination of neurons, batch size and epochs seem to perform well.

# minor data type changes for plots

runs_1_plot <- runs_1

runs_1_plot$flag_dense_units <- as.factor(runs_1_plot$flag_dense_units)
runs_1_plot$flag_batch_size <- as.factor(runs_1_plot$flag_batch_size)
runs_1_plot$flag_epochs <- as.factor(runs_1_plot$flag_epochs)


# 1.1.1 box plots - facet wrap on number of hidden units

# defining functions that calculate the upper and lower quantiles

q1_func <- function(x){
  
  q1 <- quantile(x, probs = c(0.25))
  
  return(q1)
  
}

q3_func <- function(x){
  
  q3 <- quantile(x, probs = c(0.75))
  
  return(q3)
  
}

# batch size

runs_1_plot  %>% 
  ggplot(aes(x = flag_batch_size, y = metric_val_acc, fill = flag_batch_size)) +
  geom_boxplot() +
  labs(title = "Grid of different number of hidden neurons", x = "Batch size", y = "Validation accuracy") +
  scale_fill_brewer(palette = "Set3") +
  facet_wrap(flag_dense_units~.) +
  stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) + 
  stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: 
# Conclusion: consider using batch size of 10 with 64 or 128 hidden neurons.


# epochs
runs_1_plot  %>% 
  ggplot(aes(x = flag_epochs, y = metric_val_acc, fill = flag_epochs)) +
  geom_boxplot() +
  labs(title = "Grid of different number of hidden neurons", x = "Epochs", y = "Validation accuracy") +
  facet_wrap(flag_dense_units~.) +
  scale_fill_brewer(palette = "Set3") +
  stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) +
  stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: 
# Conclusion:  

runs_1_plot %>% 
  ggplot(aes(x = metric_acc, y = metric_val_acc, colour = flag_dense_units)) +
  geom_point(alpha = 0.5) +
  labs(title = "Grid of batch sizes", x = "Training accuracy", y = "Validation accuracy") + 
  geom_abline(intercept = 0) + 
  #geom_vline(xintercept = 0.985, colour = "red", linetype = "dashed") + 
  #geom_hline(yintercept = 0.955, colour = "red", linetype = "dashed") + 
  facet_wrap(flag_batch_size~.) +
  scale_colour_discrete(name = "Hidden layer units") +
  theme_bw()

# No conclusion really
  
runs_1_plot %>% 
  ggplot(aes(x = metric_acc, y = metric_val_acc, colour = flag_dense_units)) +
  labs(title = "Grid of epochs", x = "Training accuracy", y = "Validation accuracy") +
  geom_point(alpha = 0.5) +
  #geom_vline(xintercept = 0.985, colour = "red", linetype = "dashed") + 
  #geom_hline(yintercept = 0.955, colour = "red", linetype = "dashed") + 
  scale_colour_discrete(name = "Hidden layer units") +
  facet_wrap(flag_epochs~.) +
  theme_bw()

# Findings: i) Points (models) become more clustered at the top right hand side indicating that the higher epochs are porducing more generalised models. ii) Seem like the purple (128) and blue (64 units) points are prominent in the upper right hand corners of the plots. 
# Conclusion: Use +/- 75 epochs with hidden layer units 64 or 128

# 1.1.2 Bar charts

# extracting 90th percentile for validation accuracy

perc_90th <- quantile(runs_1$metric_val_acc, probs = 0.9)

runs_1_plot <- runs_1 %>% 
  filter(metric_val_acc > perc_90th)

# hidden layer units
exp_1_bar_dense_units <- runs_1_plot %>% 
  group_by(flag_dense_units) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_dense_units,-count), y = count, fill = as.factor(flag_dense_units))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = 0, size = 3.5) + 
  labs(x = "Hidden layer units", y = " ") + 
  scale_fill_brewer(palette = "Paired") + 
  theme_bw() +
  theme(legend.position = "none")

# batch size
exp_1_bar_batch_size <- runs_1_plot %>% 
  group_by(flag_batch_size) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_batch_size,-count), y = count, fill = as.factor(flag_batch_size))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = 0, size = 3.5) + 
  labs(x = "Batch size", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# epochs
exp_1_bar_epochs <- runs_1_plot %>% 
  group_by(flag_epochs) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_epochs,-count), y = count, fill = as.factor(flag_epochs))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = 0, size = 3.5) + 
  labs(x = "Epochs", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# defining grid containing all bar plots
grid.arrange(arrangeGrob(exp_1_bar_dense_units, 
                         exp_1_bar_batch_size, 
                         exp_1_bar_epochs, 
                         nrow = 2,
                         left = textGrob(paste0("Count of validation accuracy above ", perc_90th, " (90th percentile)"), rot = 90))
)



# 2. Defining second experiment:

# Note: Using the findings of the first experiment the following experiment 

epochs_vec <- round(seq(67, 100, length.out = 3),0)
batch_size_vec <- c(5, 10, 15)
dense_units_vec <- c(64, 128)
lambda_vec <- c(0.001, 0.01, 0.1, 1, 2)

start <- Sys.time()
runs_2 <- tuning_run("experiment_2.R",
                   flags = list(batch_size = batch_size_vec,
                                epochs = epochs_vec,
                                dense_units = dense_units_vec,
                                lambda = lambda_vec)) 

end <- Sys.time()

# Note: Took 3.6 hours to run

# 2.1 Exploring results for experiment via visualizations

# 2..1.1 box plots

runs_2_plot <- runs_2

runs_2_plot$flag_dense_units <- as.factor(runs_2_plot$flag_dense_units)
runs_2_plot$flag_batch_size <- as.factor(runs_2_plot$flag_batch_size)
runs_2_plot$flag_epochs <- as.factor(runs_2_plot$flag_epochs)
runs_2_plot$flag_lambda <- as.factor(runs_2_plot$flag_lambda)

# batch size

runs_2_plot  %>% 
  ggplot(aes(x = flag_batch_size, y = metric_val_acc, fill = flag_batch_size)) +
  geom_boxplot() +
  labs(title = "Grid of different L2 regularization factors", x = "Batch size", y = "Validation accuracy") +
  scale_fill_brewer(palette = "Paired") +
  facet_wrap(flag_lambda~.) +
  #stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) + 
  #stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  #stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Lambda values of 0.01 and 0.1 yield very good results for all batch sizes considered (small std and high medians). Specifically a batch size of 100 and lambda value of 0.01.
# Conclusion: Consider using lambda value of 0.01 and batch size of 100.


# epochs
runs_2_plot  %>% 
  ggplot(aes(x = flag_epochs, y = metric_val_acc, fill = flag_epochs)) +
  geom_boxplot() +
  labs(title = "Grid of different L2 regularization factors", x = "Epochs", y = "Validation accuracy") +
  facet_wrap(flag_lambda~.) +
  scale_fill_brewer(palette = "Set3") +
  #stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) +
  #stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  #stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Again, Lambda values of 0.01 and 0.1 yield very good results for all epoch sizes considered. Specifically a lambda value at 0.01 or 0.1 and 30 epochs.
# Conclusion: Consider using lambda value range between 0.01 or 0.1 at 30 epochs.

# hidden units
runs_2_plot  %>% 
  ggplot(aes(x = flag_dense_units, y = metric_val_acc, fill = flag_dense_units)) +
  geom_boxplot() +
  labs(title = "Grid of different L2 regularization factors", x = "Hidden layer units", y = "Validation accuracy") +
  facet_wrap(flag_lambda~.) +
  scale_fill_brewer(palette = "Set3") +
  #stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) +
  #stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  #stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Again, Lambda values of 0.01 and 0.1 yield very good results for all hidden layer units.Specifically, 32 hidden layer units seem to perform very well at lamda 0.01 and 64 hidden layer units seem to perform very well with lambda value of 0.1.
# Conclusion: Consider using lambda value range between 0.1 with 64 hidden layer units or lambda value of 0.01 with 32 hidden units.

# 2.1.2 Bar charts

# extracting 90th percentile for validation accuracy

perc_90th <- quantile(runs_2$metric_val_acc, probs = 0.9)

runs_2_plot <- runs_2 %>% 
  filter(metric_val_acc > perc_90th)

# lambda values 
exp_2_bar_lambda <-  runs_2_plot %>% 
  group_by(flag_lambda) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_lambda,-count), y = count, fill = as.factor(flag_lambda))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Lambda", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")


# hidden layer units
exp_2_bar_dense_units <-  runs_2_plot %>% 
  group_by(flag_dense_units) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_dense_units,-count), y = count, fill = as.factor(flag_dense_units))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Hidden layer units", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# batch size
exp_2_bar_batch_size <- runs_2_plot %>% 
  group_by(flag_batch_size) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_batch_size,-count), y = count, fill = as.factor(flag_batch_size))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Batch size", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# epochs
exp_2_bar_epochs <- runs_2_plot %>% 
  group_by(flag_epochs) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_epochs,-count), y = count, fill = as.factor(flag_epochs))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Epochs", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# defining grid containing all bar plots
grid.arrange(arrangeGrob(exp_2_bar_lambda, 
                         exp_2_bar_dense_units, 
                         exp_2_bar_batch_size,
                         exp_2_bar_epochs,
                         nrow = 2,
                         left = textGrob(paste0("Count of validation accuracy above ", perc_90th, " (90th percentile)"), rot = 90))
)

# 3. Defining third experiment:

# Note: Very similar to the second experiment only dropout will be used now

epochs_vec <- round(seq(67, 100, length.out = 3),0)
batch_size_vec <- c(5, 10, 15)
dense_units_vec <- c(64, 128)
dropout_vec <- c(0.5, 0.6, 0.7, 0.8, 0.9)

start <- Sys.time()
runs_3 <- tuning_run("experiment_3.R",
                   flags = list(batch_size = batch_size_vec,
                                epochs = epochs_vec,
                                dense_units = dense_units_vec,
                                dropout = dropout_vec)) 

end <- Sys.time()

# Note: Took 4.4 hours to run

# 3.1 Exploring results for experiment via visualizations

# 3.1.1 Comparing regularization mechanisms

runs_2 <- ls_runs(runs_dir = "exp_2_runs")
runs_3 <-  ls_runs() 

plot_3_df <- data.frame(val_acc = c(runs_2$metric_val_acc, runs_3$metric_val_acc),
                        experiment_name = c(rep("L2 regularization", nrow(runs_2)), rep("Dropout regularization",nrow(runs_3))))

# Density plot
exp_3_density <- plot_3_df %>% 
ggplot(aes(x = val_acc, fill = experiment_name)) + 
  geom_density(alpha=.3) +
  labs(y = "Density", x = "Validation accuracy") +
  theme_bw() + 
  scale_fill_brewer(palette = "Dark2") + 
  theme(legend.title = element_blank())

# box-plot

exp_3_box <- plot_3_df %>% 
ggplot(aes(x = val_acc, fill = experiment_name)) + 
  geom_boxplot() +
  labs(y = "Regularization mechanism", x = "Validation accuracy") +
  scale_fill_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.title = element_blank())

grid.arrange(arrangeGrob(exp_3_density, exp_3_box, nrow = 2, ncol = 1))


#### If dropout yields better results than L2 then also plot these curves ####

# 3.1.2 box plots

runs_3_plot <- runs_3

runs_3_plot$flag_dense_units <- as.factor(runs_3_plot$flag_dense_units)
runs_3_plot$flag_batch_size <- as.factor(runs_3_plot$flag_batch_size)
runs_3_plot$flag_epochs <- as.factor(runs_3_plot$flag_epochs)
runs_3_plot$flag_dropout <- as.factor(runs_3_plot$flag_dropout)

# batch size

runs_3_plot  %>% 
  ggplot(aes(x = flag_batch_size, y = metric_val_acc, fill = flag_batch_size)) +
  geom_boxplot() +
  labs(title = "Grid of different dropout rates", x = "Batch size", y = "Validation accuracy") +
  scale_fill_brewer(palette = "Paired") +
  facet_wrap(flag_dropout~.) +
  stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) + 
  stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Lambda values of 0.01 and 0.1 yield very good results for all batch sizes considered (small std and high medians). Specifically a batch size of 100 and lambda value of 0.01.
# Conclusion: Consider using lambda value of 0.01 and batch size of 100.


# epochs
runs_3_plot  %>% 
  ggplot(aes(x = flag_epochs, y = metric_val_acc, fill = flag_epochs)) +
  geom_boxplot() +
  labs(title = "Grid of different dropout rates", x = "Epochs", y = "Validation accuracy") +
  facet_wrap(flag_dropout~.) +
  scale_fill_brewer(palette = "Set3") +
  stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) +
  stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Again, Lambda values of 0.01 and 0.1 yield very good results for all epoch sizes considered. Specifically a lambda value at 0.01 or 0.1 and 30 epochs.
# Conclusion: Consider using lambda value range between 0.01 or 0.1 at 30 epochs.

# hidden units
runs_3_plot  %>% 
  ggplot(aes(x = flag_dense_units, y = metric_val_acc, fill = flag_dense_units)) +
  geom_boxplot() +
  labs(title = "Grid of different dropout rates", x = "Hidden layer units", y = "Validation accuracy") +
  facet_wrap(flag_dropout~.) +
  scale_fill_brewer(palette = "Set3") +
  #stat_summary(fun = median, geom = "line", aes(group = 1), col = "red", size = 1) +
  #stat_summary(fun = q1_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") +
  #stat_summary(fun = q3_func, geom = "line", aes(group = 1), col = "red", size = 0.85, linetype = "dashed") + 
  theme_bw() + 
  theme(legend.position = "none")

# Finding: i) Again, Lambda values of 0.01 and 0.1 yield very good results for all hidden layer units.Specifically, 32 hidden layer units seem to perform very well at lamda 0.01 and 64 hidden layer units seem to perform very well with lambda value of 0.1.
# Conclusion: Consider using lambda value range between 0.1 with 64 hidden layer units or lambda value of 0.01 with 32 hidden units.

#3.1.3 Bar charts

# extracting 90th percentile for validation accuracy

perc_90th <- quantile(runs_3$metric_val_acc, probs = 0.9)

runs_3_plot <- runs_3 %>% 
  filter(metric_val_acc > perc_90th)

# lambda values 
exp_3_bar_dropout <-  runs_3_plot %>% 
  group_by(flag_dropout) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_dropout,-count), y = count, fill = as.factor(flag_dropout))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Dropout rate", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")


# hidden layer units
exp_3_bar_dense_units <-  runs_3_plot %>% 
  group_by(flag_dense_units) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_dense_units,-count), y = count, fill = as.factor(flag_dense_units))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Hidden layer units", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# batch size
exp_3_bar_batch_size <- runs_3_plot %>% 
  group_by(flag_batch_size) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_batch_size,-count), y = count, fill = as.factor(flag_batch_size))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Batch size", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# epochs
exp_3_bar_epochs <- runs_3_plot %>% 
  group_by(flag_epochs) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(flag_epochs,-count), y = count, fill = as.factor(flag_epochs))) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = count), vjust = -0.4) + 
  labs(x = "Epochs", y = " ") + 
  scale_fill_brewer(palette = "Paired") +
  theme_bw() +
  theme(legend.position = "none")

# defining grid containing all bar plots
grid.arrange(arrangeGrob(exp_3_bar_dropout, 
                         exp_3_bar_dense_units, 
                         exp_3_bar_batch_size,
                         exp_3_bar_epochs,
                         nrow = 2,
                         left = textGrob(paste0("Count of validation accuracy above ", perc_90th, " (90th percentile)"), rot = 90))
)


# 4. Defining fourth experiment:

# Note: Checking which training optimisers deliver best results

epochs_vec <- c(100)
batch_size_vec <- c(10, 15)
dense_units_vec <- c(128)
dropout_vec <- c(0.6, 0.5)
optimizer_vec <- c('adam','sgd', 'rmsprop', 'adagrad', 'adadelta', 'adamax', 'nadam')

start <- Sys.time()
runs_4 <- tuning_run("experiment_4.R",
                   flags = list(batch_size = batch_size_vec,
                                epochs = epochs_vec,
                                dense_units = dense_units_vec,
                                dropout = dropout_vec,
                                optimizer = optimizer_vec)) 

end <- Sys.time()

# Note: Took 22.5 mins to run

# 4.1 Exploring results for experiment via visualizations

# 4.1.1 box plots 

runs_4_plot <- runs_4


exp_4_density <- runs_4_plot %>% 
ggplot(aes(x = metric_val_acc, fill = flag_optimizer)) + 
  geom_density(alpha=.3) +
  labs(y = "Density", x = "Validation accuracy") +
  theme_bw() + 
  scale_fill_brewer(palette = "Dark2") + 
  theme(legend.title = element_blank())

exp_4_box <- runs_4_plot  %>% 
  ggplot(aes(x = metric_val_acc , fill = as.factor(flag_optimizer))) +
  geom_boxplot() +
  labs(x = "Training optimizer", y = "Validation accuracy") +
  scale_fill_brewer(palette = "Dark2")  +
  theme_bw() + 
  theme(legend.title = element_blank()) 

grid.arrange(arrangeGrob(exp_4_density, exp_4_box, nrow = 2, ncol = 1))


# Finding: Adam, adamax, nadam. rmsprop, and sgd dilivered good results. Specifically Adam and rmsprop performed very well.
# Conclusion: Use either adam or rmsprop


# 5. Defining fith experiment:

epochs_vec <- c(100)
batch_size_vec <- c(10, 15)
dense_units_vec <- c(128)
dropout_vec <- c(0.6, 0.5)
learning_rate_vec <- c(0.001, 0.01, 0.1, 0.2, 0.3)

start <- Sys.time()
runs_5 <- tuning_run("experiment_5.R",
                   flags = list(batch_size = batch_size_vec,
                                epochs = epochs_vec,
                                dense_units = dense_units_vec,
                                dropout = dropout_vec,
                                learning_rate = learning_rate_vec)) 

end <- Sys.time()

# 5.1 Exploring results for experiment via visualizations

# 5.1.1 box plots 

runs_5 <- ls_runs(runs_dir = "experiment_5_runs")

runs_5_plot <- runs_5

exp_5_density <- runs_5_plot %>% 
ggplot(aes(x = metric_val_acc , fill = as.factor(flag_learning_rate))) + 
  geom_density(alpha=.3) +
  labs(y = "Density", x = "Validation accuracy") +
  theme_bw() + 
  scale_fill_brewer(palette = "Dark2") + 
  theme(legend.title = element_blank())


exp_5_box <- runs_5_plot  %>% 
  ggplot(aes(y = as.factor(flag_learning_rate), x = metric_val_acc , fill = as.factor(flag_learning_rate))) +
  geom_boxplot() +
  labs(y = "Learning rate", x = "Validation accuracy") +
  scale_fill_brewer(palette = "Dark2")  +
  theme_bw() + 
  theme(legend.title = element_blank())

grid.arrange(arrangeGrob(exp_5_density, exp_5_box, nrow = 2, ncol = 1))

# Note: 

```

Validation curves generation

Note: Our approach be to fix the architecture of the neural network model (having a certain complexity threshold) and to apply regularization as a mechanism to control the complexity (such that the model generalizes well).

```{r, validation curves generation - Hyper-paramater tuning}

# 1. Defining neural network models

# 1.1 Neural network model 1 (simple model)

# Hidden units: 8
# Batch size: 100
# Epochs: 23
# Optimizer and learning rate: 0.001
# Activation function (hidden layer): relu
# Activation function (final layer): softmax
# Loss function: categorical crossentropy 
# L2 regularization

lambda_vec <- exp(seq(-5.5,-2,length = 15)) # Notice the exp - efficient way to define a optimal set of values

nn_1 <- tuning_run("NN_1.R",
                   flags = list(lambda = lambda_vec)) 


# 1.1.1 Plotting validation curve

## NB! Plot below will only work afer you have defined best lambda value

## High level lambda values plot

nn_1_df <- data.frame(set = c(rep("Train", nrow(nn_1)), rep("Validation", nrow(nn_1))),
                      error = c(nn_1$metric_loss, nn_1$metric_val_loss),
                      lambda = c(nn_1$flag_lambda, nn_1$flag_lambda)
                      ) 

hl_nn_1 <- nn_1_df %>% 
ggplot(aes(x = lambda, y = error, colour = set, fill = set)) +  
  geom_point() + 
  geom_line() +
  labs(x= expression(lambda), y = "Error (categorical crossentropy)") + 
  scale_color_manual(values=c("black", "blue")) + 
  geom_vline(xintercept = min_lambda, linetype = "dashed", col = "red" ) +
  theme_bw() +
  theme(legend.position = "none")

## Focused lambda values plot

min_lambda <- nn_1$flag_lambda[which(nn_1$metric_val_loss == min(nn_1$metric_val_loss))]

nn_1_df <- data.frame(lambda = nn_1$flag_lambda,
                   val_loss = nn_1$metric_val_loss,
                   train_loss = nn_1$metric_loss)

focus_nn_1 <- nn_1_df %>% 
  ggplot(aes(x = lambda)) +  
  geom_point(aes(y = val_loss, color = "Validation")) + 
  geom_line(aes(y = val_loss, color = "Validation")) +
  geom_point(aes(y = train_loss, color = "Train")) + 
  geom_line(aes(y = train_loss, color = "Train")) + 
  geom_smooth(aes(y = val_loss, color = "Validation smooth"), se =F) +
  geom_smooth(aes(y = train_loss, color = "Train smooth"), se =F) + 
  geom_vline(aes(xintercept = min_lambda), color = "red", linetype = "dashed"  ) + 
  scale_color_manual(breaks = c("Validation", "Train", "Validation smooth", "Train smooth"),
                     values = c("Validation" = "blue", "Train" = "black", "Validation smooth" = "lightblue" , "Train smooth" = "gray") ) +
  geom_text(aes(x = min_lambda + 0.03, y = max(val_loss) , label = paste('lambda', '==', round(min_lambda, 4))), parse = T, col = "red") +
  labs(x = expression(lambda), y = " ") + 
  theme_bw() + 
  theme(legend.title = element_blank())

grid.arrange(arrangeGrob(hl_nn_1, focus_nn_1, nrow = 1, ncol = 2))

# 1.2 Neural network model 2 (More complex model)

# Hidden units: 32
# Batch size: 100
# Epochs: 23
# Optimizer and learning rate: 0.001
# Activation function (hidden layer): relu
# Activation function (final layer): softmax
# Loss function: categorical crossentropy 
# L2 regularization

lambda_vec <- exp(seq(-5,-3,length = 15)) # Notice the exp - efficient way to define a optimal set of values

nn_2 <- tuning_run("NN_2.R",
                   flags = list(lambda = lambda_vec)) 



nn_2 <- ls_runs(runs_dir = "nn_2_high_level")

# 1.2.1 Plotting validation curve

## NB! Plot below will only work after you have defined best lambda value

## High level lambda values plot

nn_2_df <- data.frame(set = c(rep("Train", nrow(nn_2)), rep("Validation", nrow(nn_2))),
                      error = c(nn_2$metric_loss, nn_2$metric_val_loss),
                      lambda = c(nn_2$flag_lambda, nn_2$flag_lambda)
                      ) 

hl_nn_2 <- nn_2_df %>% 
ggplot(aes(x = lambda, y = error, colour = set, fill = set)) +  
  geom_point() + 
  geom_line() +
  labs(x= expression(lambda), y = "Error (categorical crossentropy)") + 
  scale_color_manual(values=c("blue", "black")) + 
  geom_vline(xintercept = min_lambda, linetype = "dashed", col = "red" ) +
  theme_bw() + 
  theme(legend.position = "none")

## Focused lambda values plot

min_lambda <- nn_2$flag_lambda[which(nn_2$metric_val_loss == min(nn_2$metric_val_loss))]

nn_2_df <- data.frame(lambda = nn_2$flag_lambda,
                   val_loss = nn_2$metric_val_loss,
                   train_loss = nn_2$metric_loss)

focus_nn_2 <- nn_2_df %>% 
  ggplot(aes(x = lambda)) +  
  geom_point(aes(y = val_loss, color = "Validation")) + 
  geom_line(aes(y = val_loss, color = "Validation")) +
  geom_point(aes(y = train_loss, color = "Train")) + 
  geom_line(aes(y = train_loss, color = "Train")) + 
  geom_smooth(aes(y = val_loss, color = "Validation smooth"), se =F) +
  geom_smooth(aes(y = train_loss, color = "Train smooth"), se =F) + 
  geom_vline(aes(xintercept = min_lambda), color = "red", linetype = "dashed"  ) + 
  scale_color_manual(breaks = c("Validation", "Train", "Validation smooth", "Train smooth"),
                     values = c("Validation" = "blue", "Train" = "black", "Validation smooth" = "lightblue" , "Train smooth" = "gray") ) +
  geom_text(aes(x = min_lambda+0.006, y = max(val_loss) , label = paste('lambda', '==', round(min_lambda, 4))), parse = T, col = "red") +
  labs(x = expression(lambda), y = " ") + 
  theme_bw() + 
  theme(legend.title = element_blank())

grid.arrange(arrangeGrob(hl_nn_2, focus_nn_2, nrow = 1, ncol = 2))


# 1.3 Comparison between model 1 and model 2 architectures

# 1.3.1 Network data




# 1.3.1 Transactional data


# 1.3.1 All data





```


5.2.3 Model construction and testing

```{r, Model construction and testing - Neural network models - modelling }

# 1. Defining network data's models

# Note: Two models are defined, trained and tested with the results obtained from the hyper parameter tuning process. Each model will be tr


# 1.1 Model 1 

# define model
model_1_network <- keras_model_sequential()

model_1_network %>% 
  layer_dense(units = 32, 
              activation = 'relu', 
              input_shape = c(27),
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dense(units = 2, activation = 'softmax')

# Compile model

model_1_network %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = list('accuracy',metric_precision(), metric_recall(), metric_auc()))

# Fit model

history_1_network <- model_1_network %>% 
  fit(train_network_features_nn,
      train_target_nn,
      epochs = 23,
      batch_size = 100,
      validation_split = 0.3)

# Evaluate model's performance on Test data

model_1_network %>% evaluate(test_network_features_nn, test_target_nn)
  
# display confusion matrix
Y_test_hat_network_1 <- predict_classes(model_1_network, test_network_features_nn)
conf_mat_model_1_net <-  table(test_network_features$is_fraud, Y_test_hat_network_1)


# 1.1 Model 2

# define model
model_2_network <- keras_model_sequential()

model_2_network %>% 
  layer_dense(units = 64, 
              activation = 'relu', 
              input_shape = c(27),
              kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dense(units = 2, activation = 'softmax')

# Compile model

model_2_network %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = list('accuracy',metric_precision(), metric_recall(), metric_auc()))

# Fit model

history_2_network <- model_2_network %>% 
  fit(train_network_features_nn,
      train_target_nn,
      epochs = 23,
      batch_size = 100,
      validation_split = 0.3)

# Evaluate model's performance on Test data

model_2_network %>% evaluate(test_network_features_nn, test_target_nn)
  
# display confusion matrix
Y_test_hat_network_2 <- predict_classes(model_2_network, test_network_features_nn)
conf_mat_model_2_net <-  table(test_network_features$is_fraud, Y_test_hat_network_2)


```



### DEMO CODE ###


Test code
```{r, test code}

#### transactional feature function additions #####

transaction_df <- training_transactions_final


# 3. Generating time period stats for accounts
  
  # 3.1 payment made (from)
  
  # all accounts that made a payment
  payment_IDs <- unique(transaction_df$from)
  
  # data frame that stores the results
  payment_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        payment_period_max = rep(NA, length(payment_IDs)),
                                        payment_period_min = rep(NA, length(payment_IDs)),
                                        payment_period_avg = rep(NA, length(payment_IDs)),
                                        payment_period_std = rep(NA, length(payment_IDs)))
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(from == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA,nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- max(current_days_difference, na.rm =T)
      payment_period_stats_df$payment_period_min[i] <- min(current_days_difference, na.rm =T) 
      payment_period_stats_df$payment_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      payment_period_stats_df$payment_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- 1
      payment_period_stats_df$payment_period_min[i] <- 1 
      payment_period_stats_df$payment_period_avg[i] <- 1 
      payment_period_stats_df$payment_period_std[i] <- 0 
      
    }
    
  }
  
  # 3.2 payment received (to)
  
  # all accounts that received a payment
  payment_IDs <- unique(transaction_df$to)
  
  # data frame that stores the results
  receive_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        receive_period_max = rep(NA, length(payment_IDs)),
                                        receive_period_min = rep(NA, length(payment_IDs)),
                                        receive_period_avg = rep(NA, length(payment_IDs)),
                                        receive_period_std = rep(NA, length(payment_IDs)))
  
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(to == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA, nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- max(current_days_difference, na.rm =T)
      receive_period_stats_df$receive_period_min[i] <- min(current_days_difference, na.rm =T) 
      receive_period_stats_df$receive_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      receive_period_stats_df$receive_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- 1
      receive_period_stats_df$receive_period_min[i] <- 1 
      receive_period_stats_df$receive_period_avg[i] <- 1 
      receive_period_stats_df$receive_period_std[i] <- 0 
      
    }
    
  }






#### transactional feature function additions #####

#### EDA TEst training set
train_all_features %>% 
  create_report(y = "is_fraud",
                report_title = "EDA - training data" )

test_all_features %>% 
  create_report(y = "is_fraud",
                report_title = "EDA - testing data" )


####

time_stamp <- transactions_data_raw %>% 
  group_by(TIMESTAMP) %>% 
  summarise(count = n())

time_stamp %>% 
ggplot(aes(x = TIMESTAMP, y = count)) + 
  geom_col()

hist(time_stamp$count)

plot(density(time_stamp$count))

sum(time_stamp$count)*0.75


# splitting the data (out-of-time validation)

transactions_test <- transactions_data_raw %>% 
  mutate(row_index  = 1:nrow(transactions_data_raw)) %>% 
  mutate(set = if_else(row_index < 0.5480962*nrow(transactions_data_raw), "train", "test"))


# visualing some data

transactions_test %>%
  group_by(IS_FRAUD, set) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = set, y = count, fill = IS_FRAUD )) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=count), vjust=1.6, position = position_dodge(0.9), size=3.5) +
  scale_y_log10()


# optimiser

split_point_df <- data.frame(split = seq(from = 0.5, to = 0.8, length.out = 500))

transactions_test <- transactions_data_raw %>% 
  mutate(row_index  = 1:nrow(transactions_data_raw))

test_fraud_ratio <- rep(NA, nrow(split_point_df))

train_fraud_ratio <- rep(NA, nrow(split_point_df))

resulting_ratio <- rep(NA, nrow(split_point_df))

for (i in 1:nrow(split_point_df)) {
  
  
  current_split <- transactions_test %>% 
    mutate(set = if_else(row_index < split_point_df[i,1]*nrow(transactions_data_raw), "train", "test")) %>% 
    group_by(set, IS_FRAUD) %>% 
    summarise(current_count = n())
  
  # test
  test_fraud_total <- current_split$current_count[current_split$set == "test" &  current_split$IS_FRAUD == "True"]
  test_non_fraud_total <- current_split$current_count[current_split$set == "test" &  current_split$IS_FRAUD == "False"]
  
  test_fraud_ratio[i] <-  test_fraud_total/(test_fraud_total + test_non_fraud_total)
  
  train_fraud_total <- current_split$current_count[current_split$set == "train" &  current_split$IS_FRAUD == "True"]
  train_non_fraud_total <- current_split$current_count[current_split$set == "train" &  current_split$IS_FRAUD == "False"]
  
  train_fraud_ratio[i] <-  train_fraud_total/(train_fraud_total + train_non_fraud_total)
  
  resulting_ratio[i] <- test_fraud_ratio[i] - train_fraud_ratio[i]
  
  
}

split_point_df$test_fraud_ratio <- test_fraud_ratio

split_point_df$train_fraud_ratio <- train_fraud_ratio

split_point_df$resulting_ratio <- resulting_ratio


split_point_df %>% 
  ggplot(aes(x = split, y = resulting_ratio)) +
  geom_point()


split_point_df_filt <- split_point_df %>% 
  filter(resulting_ratio >= 0 )


which(split_point_df_filt$resulting_ratio == min(split_point_df_filt$resulting_ratio))



```


Unused code
```{r, unused code}
### Precision recall function

y_true_test <- sample(c(1,0), prob = c(0.05, 0.95), 100, replace = T)
y_pred_test <- sample(c(1,0),prob = c(0.05, 0.95), 100, replace = T)



recall <- function(y_true, y_pred){
  
  # confusion matrix
  table <- table(y_true, y_pred)
  
  # true positive
  TP <- table[2,2]
  
  # false negative
  FN <- table[1,2]
  
  # recall
  recall <- TP/(TP+FN)
  
  return(recall)
  
}


precision <- function(y_true, y_pred){
  
  # confusion matrix
  table <- table(y_true, y_pred)
  
  # true positive
  TP <- table[2,2]
  
  # false positive
  FP <- table[2,1]
  
  # precision
  precision <- TP/(TP+FP)
  
  return(precision)
  
}

f1_score <- function(y_true, y_pred){
  
  precision <- precision(y_true, y_pred)
  recall <- recall(y_true, y_pred)
  
  f1_score <- (2*precision*recall)/(precision + recall)
  
  return(f1_score)
  
}

balanced_accuracy <- function(y_true, y_pred){
  
  # confusion matrix
  table <- table(y_true, y_pred)
  
  TP <-  table[2,2]
  P <- sum(table[2,])
  
  TN <- table[1,1]
  N <- sum(table[1,])
  
  
  balanced_accuracy <- (TP/P + TN/N)/2
  
  return(balanced_accuracy)
  
  
}




###### 1. TEST TRAIN SPLIT ###### 

# split should be:

# 3.1 Stratified splitting of data - 

set.seed(123)
train_index <- createDataPartition(accounts_data_final$is_fraud, p = 0.8, list = F)


# accounts data

accounts_data_final_train <- accounts_data_final[train_index,]
accounts_data_final_test <- accounts_data_final[-train_index,]

# transaction data 

# configuring training data set 
transactions_data_final_train <- transactions_data_final %>% 
  filter(to %in% accounts_data_final_train$client_ID & from %in% accounts_data_final_train$client_ID)

transactions_data_final_test <- transactions_data_final 

# 3. Data split

# split should be:

# 3.1 Stratified splitting of data - 

set.seed(123)
train_index <- createDataPartition(accounts_data_final$is_fraud, p = 0.8, list = F)


# accounts data

accounts_data_final_train <- accounts_data_final[train_index,]
accounts_data_final_test <- accounts_data_final[-train_index,]

# transaction data 

# configuring training data set 
transactions_data_final_train <- transactions_data_final %>% 
  filter(to %in% accounts_data_final_train$client_ID & from %in% accounts_data_final_train$client_ID)

transactions_data_final_test <- transactions_data_final 


#### NEW: Splitting the data based on time stamp variable ###

# grouping the number of transactions per time stamp
transactions_time_stamp <- transactions_data_final %>% 
  group_by(TIMESTAMP) %>% 
  summarise(count = n())

# calculating cumulative count
total_count <- sum(transactions_time_stamp$count)
cum_count <- rep(NA, nrow(transactions_time_stamp))
cum_perc <- rep(NA, nrow(transactions_time_stamp)) 

for (i in 1:nrow(transactions_time_stamp)) {
  
  if(i == 1) {
    
  cum_count[i] <- transactions_time_stamp$count[i]
  cum_perc[i] <- 0
  
  }else{
    
    cum_count[i] <- cum_count[i-1] + transactions_time_stamp$count[i]
    cum_perc[i] <- round((cum_count[i]/total_count)*100, 2)
  }
}

# selecting the time stamp that contains 75% of all the observations (for training)

training_ind <- max(which(cum_perc < 76))
training_time_stamp <- transactions_time_stamp$TIMESTAMP[training_ind]

# defining training set for transactions
training_transactions <- transactions_data_final %>% 
  filter(TIMESTAMP <= training_time_stamp)

# defining training set for accounts

training_unique_acc_id <- unique(c(training_transactions$from,training_transactions$to)) 

# defining training fraudulent accounts

training_fraud_acc <- training_transactions %>% 
  filter(IS_FRAUD == "True")

training_fraud_acc <- unique(c(training_fraud_acc$from, training_fraud_acc$to))

# defining accounts df

training_accounts <- 




time_stamp %>% 
ggplot(aes(x = TIMESTAMP, y = count)) + 
  geom_col()

hist(time_stamp$count)

plot(density(time_stamp$count))

sum(time_stamp$count)*0.75


# splitting the data (out-of-time validation)

transactions_test <- transactions_data_raw %>% 
  mutate(row_index  = 1:nrow(transactions_data_raw)) %>% 
  mutate(set = if_else(row_index < 0.5480962*nrow(transactions_data_raw), "train", "test"))


# visualing some data

transactions_test %>%
  group_by(IS_FRAUD, set) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = set, y = count, fill = IS_FRAUD )) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=count), vjust=1.6, position = position_dodge(0.9), size=3.5) +
  scale_y_log10()


# optimiser

split_point_df <- data.frame(split = seq(from = 0.5, to = 0.8, length.out = 500))

transactions_test <- transactions_data_raw %>% 
  mutate(row_index  = 1:nrow(transactions_data_raw))

test_fraud_ratio <- rep(NA, nrow(split_point_df))

train_fraud_ratio <- rep(NA, nrow(split_point_df))

resulting_ratio <- rep(NA, nrow(split_point_df))

for (i in 1:nrow(split_point_df)) {
  
  
  current_split <- transactions_test %>% 
    mutate(set = if_else(row_index < split_point_df[i,1]*nrow(transactions_data_raw), "train", "test")) %>% 
    group_by(set, IS_FRAUD) %>% 
    summarise(current_count = n())
  
  # test
  test_fraud_total <- current_split$current_count[current_split$set == "test" &  current_split$IS_FRAUD == "True"]
  test_non_fraud_total <- current_split$current_count[current_split$set == "test" &  current_split$IS_FRAUD == "False"]
  
  test_fraud_ratio[i] <-  test_fraud_total/(test_fraud_total + test_non_fraud_total)
  
  train_fraud_total <- current_split$current_count[current_split$set == "train" &  current_split$IS_FRAUD == "True"]
  train_non_fraud_total <- current_split$current_count[current_split$set == "train" &  current_split$IS_FRAUD == "False"]
  
  train_fraud_ratio[i] <-  train_fraud_total/(train_fraud_total + train_non_fraud_total)
  
  resulting_ratio[i] <- test_fraud_ratio[i] - train_fraud_ratio[i]
  
  
}

split_point_df$test_fraud_ratio <- test_fraud_ratio

split_point_df$train_fraud_ratio <- train_fraud_ratio

split_point_df$resulting_ratio <- resulting_ratio


split_point_df %>% 
  ggplot(aes(x = split, y = resulting_ratio)) +
  geom_point()


split_point_df_filt <- split_point_df %>% 
  filter(resulting_ratio >= 0 )


which(split_point_df_filt$resulting_ratio == min(split_point_df_filt$resulting_ratio))

```


Visualisations ideas
```{r, visulaisation ideas}

# visualizing weights

## box plot
boxplot(log(transactions_data_weighted$weight))

## histogram
ggplot(transactions_data_weighted, aes(x= weight)) +
  geom_histogram() +
  scale_y_log10()


# 2. Constructing un-directed weighted network 
AML_net_uw_non_simple <- graph_from_data_frame(d = transactions_data_weighted,
                                 vertices = accounts_data_up,
                                 directed = FALSE)



# degree visualizations:

# Standard degree

## histogram ()
hist(total_degree_loc, main="Histogram of Total Node Degree", xlab = "Total Degree")

## density plot
density_degree <- density(total_degree_loc)
plot(density_degree, main="Density distribution of node degree")

# Weighted degree distribution

## histogram ()
hist(weighted_degree_distribution, main="Histogram of Weighted Degree Dsitribution", xlab = "Weighted Degree")

## density plot
density_weight_degree <- density(weighted_degree_distribution)
plot(density_weight_degree, main="Density distribution of weighted node degree")

```






