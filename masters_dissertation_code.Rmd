---
title: "UCT Masters Minor Disseration"
author: "Gustav Oosthuizen"
date: "22/09/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Importing/Loading packages

```{r, importing/loading packages}

# 1. General workflow
library(tidyverse)
library(stringr)
library(lubridate)
library(DataExplorer)
library(GGally)
library(gridExtra)
library(gridtext)
library(grid)
library(ggpubr)
library(ggrepel)

# 2. Network Analytics
library(igraph)

# 3. ML packages
library(caret)
library(glmnet)
library(ROCR)


# install.packages("keras")
# install.packages("tensorflow")
# 
# install.packages("devtools")
# 
# devtools::install_github("rstudio/keras", dependencies = T)
# devtools::install_github("rstudio/tensorflow", dependencies = T)

library(keras)
library(tensorflow)

# install_keras()
# install_tensorflow()


library(tfruns)

```

# 1. Loading data

```{r, loading data}

# Note: There is three separate data files: i) accounts, ii) alerts, iii) transactions

# 1. Training data
training_accounts_raw <- read.csv("training_set_all_ml_typologies/accounts.csv")
training_transactions_raw <- read.csv("training_set_all_ml_typologies/transactions.csv")
training_alert_accounts_raw <- read.csv("training_set_all_ml_typologies/alert_accounts.csv")
training_alert_transactions_raw <- read.csv("training_set_all_ml_typologies/alert_transactions.csv")

# 1. Training data
testing_accounts_raw <- read.csv("testing_set_all_ml_typologies/accounts.csv")
testing_transactions_raw <- read.csv("testing_set_all_ml_typologies/transactions.csv")
testing_alert_accounts_raw <- read.csv("testing_set_all_ml_typologies/alert_accounts.csv")
testing_alert_transactions_raw <- read.csv("testing_set_all_ml_typologies/alert_transactions.csv")



```

# 2. EDA of raw data table(s)

## 2.1 Training data tables

```{r, training data tables - EDA of raw data table(s)}

# 1.1 Accounts data

# Note: Only 3 variables were actually used in this data set: acct_id, initial_deposit, and prior_sar_count. The rest of the features are not really of use:

# 1) dsply_nm - duplication of acct_id 
# 2) type - contains one class for all observations
# 3) acct_stat - contains one class for all observations
# 4) acct_rptng_crncy - contains one class for all observations
# 5) branch_id - contains one class for all observations
# 6) open_dt - contains one class for all observations
# 7) close_dt - contains one class for all observations
# 8) bank_id - contains one class for all observations
# 9) tx_behavior_id  - distinc variable that indicates which account is fraud (if id = 0)
# 10) bank_id - contains one class for all observations
# NB! All personal clent information was removed (only focused on transactional information). These are
# first_name, last_name, street_addr, city, state, country, zip, gender, birth_date, ssn, lon, lat

create_report(training_accounts_raw, y = "prior_sar_count")

# Visualisations to create

# 1.1.1 Missing values profile

train_acc_missing <- plot_missing(training_accounts_raw, 
                                  ggtheme = theme_bw(),
                                  theme_config =  list(legend.position = c("none")))

# Remarks: No missing values

# 1.1.2 Univariate Disributions

## histograms

### initial deposit
train_acc_init_dep <- training_accounts_raw %>% 
  ggplot(aes(x = initial_deposit)) +
  geom_histogram() +
  theme_bw()

# Remarks: Seems to follow a uniform distribution with min = 50 000 and max = 100 000

## Bar Chart 

### prior_sar_count

train_acc_bar_is_fraud <-  training_accounts_raw %>%
  group_by(prior_sar_count) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = prior_sar_count, y = count, fill = prior_sar_count)) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.4) +
  theme_bw()

# Remarks: About 6.5% is frauduent accounts

# 1.1.4 QQ-plot

## initial depsosit (show with prior SAR account)

qq_accounts_data <- training_accounts_raw[, c("prior_sar_count", "initial_deposit")] %>% 
  mutate(prior_sar_count = if_else(prior_sar_count == "true", "prior_sar_count - true","prior_sar_count - false"))

train_acc_qq_init_dep <-  plot_qq(qq_accounts_data, by = "prior_sar_count", 
        ggtheme = theme_bw(),
        theme_config = list(legend.title = element_blank()))

# Remarks: Both distributions seem not to be from Norm distribution

# 1.1.5 Bi-variate distributions

## box-plots 

train_acc_box_init_dep <-  training_accounts_raw %>% 
  ggplot(aes(x = prior_sar_count, y = initial_deposit, fill = prior_sar_count)) +
  geom_boxplot() +
  theme_bw() +
  theme(legend.position = "none")

# Remarks: The distributions seem identical. Cant distinguish by only using the initial deposit feature.

# Compiling all plots in a single grid

grid.arrange(arrangeGrob(train_acc_missing, train_acc_bar_is_fraud, train_acc_init_dep, train_acc_box_init_dep, nrow = 2, ncol = 2))


# 1.2 Accounts alert data

create_report(training_alert_accounts_raw, y = "alert_type")

# Note: None of the variables from alerts was used in the actual analysis, however it will still be important to check the ML tyoplgies generated (aka alert_type)

# 1.2.1 Missing data profile

train_alrt_acc_missing <- plot_missing(training_alert_accounts_raw, 
                                  ggtheme = theme_bw(),
                                  theme_config =  list(legend.position = c("none")))


# 1.2.2 Univariate distributions

## bar-charts

### alert type
train_alrt_acc_bar_alert_typ <-  training_alert_accounts_raw %>%
  group_by(alert_type) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(alert_type, -count), y = count, fill = alert_type) ) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.2) +
  labs(x = "alert_type") +
  theme_bw() +
  theme(legend.position = "none")

# Remark: i) The total is the same as in the raw_accounts data that is classified as prior_sar_count. ii) Cycle, fan_in, and fan_out seems to be generated in a bigger proportion to the other ML typologies.

# Compiling all plots in a single grid

grid.arrange(arrangeGrob(train_alrt_acc_missing, train_alrt_acc_bar_alert_typ, nrow = 1, ncol = 2))

# 1.3 Transaction data

# Note: As with the accounts data only some features were actually usefull in the analysis. The features that were removed were:

# 1. tran_id - ID variable not needed for transactions
# 2. tx_type - contains one class for all observations
# 3. alert_id - distinc variable that indicates which account is fraud  

create_report(training_transactions_raw, y = "is_sar")

# 1.3.1 Missing values profile

train_tran_missing <- plot_missing(training_transactions_raw, 
                                  ggtheme = theme_bw(),
                                  theme_config =  list(legend.position = c("none")))

# Remarks: No missing values

# 1.3.2 Univariate Disributions

## histograms

### tx amount
train_trans_tx_amount <- training_transactions_raw %>% 
  ggplot(aes(x = base_amt)) +
  geom_histogram() +
  theme_bw()

# Remarks: 

## Bar Chart 

### prior_sar_count

train_tran_bar_is_fraud <-  training_transactions_raw %>%
  group_by(is_sar) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = is_sar, y = count, fill = is_sar)) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.4) +
  theme_bw()

# Remarks: About 1% is fraud transactions 

# 1.3.4 QQ-plot

## initial depsosit (show with prior SAR account)

qq_trans_data <- training_transactions_raw[, c("is_sar", "base_amt")] %>% 
  mutate(is_sar = if_else(is_sar == "True", "is_sar - true","is_sar - false"))

train_trans_qq_tx_amount <-  plot_qq(qq_trans_data, by = "is_sar", 
        ggtheme = theme_bw(),
        sampled_rows = 10000,
        theme_config = list(legend.title = element_blank()))


# Remarks: Both distributions seem not to be from Norm distribution

# 1.1.5 Bi-variate distributions

## box-plots 

train_trans_box_tx_amt <-  training_transactions_raw %>% 
  ggplot(aes(y = is_sar, x = base_amt, fill = is_sar)) +
  geom_boxplot() +
  labs(x = " ") +
  theme_bw() 

## Density plots

train_trans_dens_tx_amt <- training_transactions_raw %>% 
ggplot(aes(x = base_amt, fill = is_sar)) + 
  geom_density(alpha=.3) +
  labs(y = "Density", x = "base_amt") +
  theme_bw() + 
  theme()

# Remarks: The distributions seem identical. Cant distinguish by only using the initial deposit feature.

# Compiling bi-variate plots in a single grid
grid.arrange(arrangeGrob(train_trans_box_tx_amt, train_trans_dens_tx_amt, nrow = 2, ncol = 1))

grid.arrange(arrangeGrob(train_tran_missing, train_trans_tx_amount, train_tran_bar_is_fraud,nrow = 2, ncol = 2))


# 1.4 Transaction alert data

create_report(training_alert_transactions_raw, y = "alert_type")

# Note: None of the variables from alerts was used in the actual analysis, however it will still be important to check the ML tyoplgies generated (aka alert_type)

# 1.4.1 Missing data profile

train_alrt_trans_missing <- plot_missing(training_alert_transactions_raw, 
                                  ggtheme = theme_bw(),
                                  theme_config =  list(legend.position = c("none")))


# 1.4.2 Univariate distributions

## histograms

train_alrt_trans_tx_amnt <- training_alert_transactions_raw %>% 
  ggplot(aes(x = base_amt)) +
  geom_histogram() +
  theme_bw()

# Remark: Seems to be uniformly distributed with a spike in tx amounts at about 125.

## bar-charts

### alert type
train_alrt_trans_bar_alert_typ <-  training_alert_transactions_raw %>%
  group_by(alert_type) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(alert_type, -count), y = count, fill = alert_type) ) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.2) +
  labs(x = "alert_type") +
  theme_bw() +
  theme(legend.position = "none")

# Remark: ML transaction is much more evenly distributed than in the classified in accounts.

# 1.4.3 Bi-variate distributions

## box-plots 

train_alrt_trans_box_tx_amt <-  training_alert_transactions_raw %>% 
  ggplot(aes(y = alert_type , x = base_amt, fill = alert_type)) +
  geom_boxplot() +
  labs(x = " ") +
  theme_bw() 

## Density plots

train_alrt_trans_dens_tx_amt <- training_alert_transactions_raw %>%
ggplot(aes(x = base_amt, fill = alert_type)) + 
  geom_density(alpha=.3) +
  scale_x_log10() +
  labs(y = "Density", x = "base_amt") +
  theme_bw() + 
  theme()

# 1.4.4 timeline plot

# weekly occurance of alert_types
train_time_tx_amount <- training_alert_transactions_raw %>% 
  mutate(tran_timestamp = ymd(str_sub(tran_timestamp, end = 10))) %>% 
  ggplot(aes(x = floor_date(tran_timestamp, "week"), fill = alert_type) ) +
    geom_bar() +
  facet_grid(alert_type~.) +
  labs(x = "tran_timestep") +
  theme_bw() + 
  theme(legend.position = "none")



# Compiling all plots in a single grid

grid.arrange(arrangeGrob(train_alrt_trans_box_tx_amt, train_alrt_trans_dens_tx_amt, nrow = 2, ncol = 1))

grid.arrange(arrangeGrob(train_alrt_trans_missing, train_alrt_trans_tx_amnt, train_alrt_trans_bar_alert_typ, train_time_tx_amount, nrow = 2, ncol = 2))


```

## 2.2 Testing data tables

```{r, testing data tables - EDA of raw data table(s)}

# 1.1 Accounts data

# Note: Only 3 variables were actually used in this data set: acct_id, initial_deposit, and prior_sar_count. The rest of the features are not really of use:

# 1) dsply_nm - duplication of acct_id 
# 2) type - contains one class for all observations
# 3) acct_stat - contains one class for all observations
# 4) acct_rptng_crncy - contains one class for all observations
# 5) branch_id - contains one class for all observations
# 6) open_dt - contains one class for all observations
# 7) close_dt - contains one class for all observations
# 8) bank_id - contains one class for all observations
# 9) tx_behavior_id  - distinc variable that indicates which account is fraud (if id = 0)
# 10) bank_id - contains one class for all observations
# NB! All personal clent information was removed (only focused on transactional information). These are
# first_name, last_name, street_addr, city, state, country, zip, gender, birth_date, ssn, lon, lat

# Visualisations to create

# 1.1.1 Missing values profile

test_acc_missing <- plot_missing(testing_accounts_raw, 
                                 ggtheme = theme_bw(),
                                 geom_label_args = list(fill = "#1B9E77"),
                                 theme_config =  list(legend.position = c("none")))

# Remarks: No missing values

# 1.1.2 Univariate Disributions

## histograms

### initial deposit
test_acc_init_dep <- testing_accounts_raw %>% 
  ggplot(aes(x = initial_deposit)) +
  geom_histogram() +
  theme_bw()

# Remarks: Seems to follow a uniform distribution with min = 50 000 and max = 100 000 (similar to training set)

## Bar Chart 

### prior_sar_count

test_acc_bar_is_fraud <-  testing_accounts_raw %>%
  group_by(prior_sar_count) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = prior_sar_count, y = count, fill = prior_sar_count)) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.4) +
  scale_fill_brewer(palette = "Dark2") + 
  theme_bw()

# Remarks: About 6.5% is frauduent accounts

# 1.1.4 QQ-plot

## initial depsosit (show with prior SAR account)

qq_accounts_data <- testing_accounts_raw[, c("prior_sar_count", "initial_deposit")] %>% 
  mutate(prior_sar_count = if_else(prior_sar_count == "true", "prior_sar_count - true","prior_sar_count - false"))

test_acc_qq_init_dep <-  plot_qq(qq_accounts_data, by = "prior_sar_count", 
        ggtheme = theme_bw(),
        theme_config = list(legend.title = element_blank()))

# Remarks: Both distributions seem not to be from Norm distribution

# 1.1.5 Bi-variate distributions

## box-plots 

test_acc_box_init_dep <-  testing_accounts_raw %>% 
  ggplot(aes(x = prior_sar_count, y = initial_deposit, fill = prior_sar_count)) +
  geom_boxplot() +
  theme_bw() +
  scale_fill_brewer(palette = "Dark2") + 
  theme(legend.position = "none")

# Remarks: The distributions seem identical. Cant distinguish by only using the initial deposit feature.

# Compiling all plots in a single grid

grid.arrange(arrangeGrob(test_acc_missing, test_acc_bar_is_fraud, test_acc_init_dep, test_acc_box_init_dep, nrow = 2, ncol = 2))


# 1.2 Accounts alert data

# Note: None of the variables from alerts was used in the actual analysis, however it will still be important to check the ML tyoplgies generated (aka alert_type)

# 1.2.1 Missing data profile

test_alrt_acc_missing <- plot_missing(testing_alert_accounts_raw, 
                                  ggtheme = theme_bw(),
                                  geom_label_args = list(fill = "#1B9E77"),
                                  theme_config =  list(legend.position = c("none")))


# 1.2.2 Univariate distributions

## bar-charts

### alert type
test_alrt_acc_bar_alert_typ <-  testing_alert_accounts_raw %>%
  group_by(alert_type) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(y = reorder(alert_type, count), x = count, fill = alert_type) ) + 
  geom_col() +
  geom_text(aes(label = count), hjust = 0) +
  labs(y = "alert_type") +
  scale_fill_brewer(palette = "Dark2") +
  theme_bw() +
  theme(legend.position = "none")

# Remark: i) The total is the same as in the raw_accounts data that is classified as prior_sar_count. ii) Cycle, fan_in, and fan_out seems to be generated in a bigger proportion to the other ML typologies.

# Compiling all plots in a single grid

grid.arrange(arrangeGrob(test_alrt_acc_missing, test_alrt_acc_bar_alert_typ, nrow = 1, ncol = 2))

# 1.3 Transaction data

# Note: As with the accounts data only some features were actually usefull in the analysis. The features that were removed were:

# 1. tran_id - ID variable not needed for transactions
# 2. tx_type - contains one class for all observations
# 3. alert_id - distinc variable that indicates which account is fraud  

# 1.3.1 Missing values profile

test_tran_missing <- plot_missing(testing_transactions_raw, 
                                  ggtheme = theme_bw(),
                                  geom_label_args = list(fill = "#1B9E77"),
                                  theme_config =  list(legend.position = c("none")))

# Remarks: No missing values

# 1.3.2 Univariate Disributions

## histograms

### tx amount
test_trans_tx_amount <- testing_transactions_raw %>% 
  ggplot(aes(x = base_amt)) +
  geom_histogram() +
  scale_fill_brewer(palette = "Dark2") +
  theme_bw()

# Remarks: 

## Bar Chart 

### prior_sar_count

test_tran_bar_is_fraud <-  testing_transactions_raw %>%
  group_by(is_sar) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = is_sar, y = count, fill = is_sar)) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.4) +
  scale_fill_brewer(palette = "Dark2") +
  theme_bw()

# Remarks: About 1% is fraud transactions 

# 1.3.4 QQ-plot

## initial depsosit (show with prior SAR account)

qq_trans_data <- testing_transactions_raw[, c("is_sar", "base_amt")] %>% 
  mutate(is_sar = if_else(is_sar == "True", "is_sar - true","is_sar - false"))

test_trans_qq_tx_amount <-  plot_qq(qq_trans_data, by = "is_sar", 
        ggtheme = theme_bw(),
        theme_config = list(legend.title = element_blank()))


# Remarks: Both distributions seem not to be from Norm distribution

# 1.1.5 Bi-variate distributions

## box-plots 

test_trans_box_tx_amt <-  testing_transactions_raw %>% 
  ggplot(aes(y = is_sar, x = base_amt, fill = is_sar)) +
  geom_boxplot() +
  labs(x = " ") +
   scale_fill_brewer(palette = "Dark2") +
  theme_bw() 

## Density plots

test_trans_dens_tx_amt <- testing_transactions_raw %>% 
ggplot(aes(x = base_amt, fill = is_sar)) + 
  geom_density(alpha=.3) +
  labs(y = "Density", x = "base_amt") +
   scale_fill_brewer(palette = "Dark2") +
  theme_bw() + 
  theme()

# Remarks: The distributions seem identical. Cant distinguish by only using the initial deposit feature.

# Compiling bi-variate plots in a single grid
grid.arrange(arrangeGrob(test_trans_box_tx_amt, test_trans_dens_tx_amt, nrow = 2, ncol = 1))

grid.arrange(arrangeGrob(test_tran_missing, test_trans_tx_amount, test_tran_bar_is_fraud,nrow = 2, ncol = 2))


# 1.4 Transaction alert data

# Note: None of the variables from alerts was used in the actual analysis, however it will still be important to check the ML tyoplgies generated (aka alert_type)

# 1.4.1 Missing data profile

test_alrt_trans_missing <- plot_missing(testing_alert_transactions_raw, 
                                  ggtheme = theme_bw(),
                                  geom_label_args = list(fill = "#1B9E77"),
                                  theme_config =  list(legend.position = c("none")))


# 1.4.2 Univariate distributions

## histograms

test_alrt_trans_tx_amnt <- testing_alert_transactions_raw %>% 
  ggplot(aes(x = base_amt)) +
  scale_fill_brewer(palette = "Dark2") +
  geom_histogram() +
  theme_bw()

# Remark: Seems to be uniformly distributed with a spike in tx amounts at about 125.

## bar-charts

### alert type
test_alrt_trans_bar_alert_typ <-  testing_alert_transactions_raw %>%
  group_by(alert_type) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = reorder(alert_type, -count), y = count, fill = alert_type) ) + 
  geom_col() +
  geom_text(aes(label = count), vjust = -0.2) +
  labs(x = "alert_type") +
  scale_fill_brewer(palette = "Dark2") + 
  theme_bw() +
  theme(legend.position = "none")

# Remark: ML transaction is much more evenly distributed than in the classified in accounts.

# 1.4.3 Bi-variate distributions

## box-plots 

test_alrt_trans_box_tx_amt <-  testing_alert_transactions_raw %>% 
  ggplot(aes(x = base_amt, fill = alert_type)) +
  geom_boxplot() +
  labs(x = " ", y = "alert_type") +
  scale_fill_brewer(palette = "Dark2") +
  theme_bw() 

## Density plots

test_alrt_trans_dens_tx_amt <- testing_alert_transactions_raw %>%
ggplot(aes(x = base_amt, fill = alert_type)) + 
  geom_density(alpha=.3) +
  scale_x_log10() +
  labs(y = "Density", x = "base_amt") +
  scale_fill_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme()

# 1.4.4 timeline plot

# weekly occurance of alert_types
test_time_tx_amount <- testing_alert_transactions_raw %>% 
  mutate(tran_timestamp = ymd(str_sub(tran_timestamp, end = 10))) %>% 
  ggplot(aes(x = floor_date(tran_timestamp, "week"), fill = alert_type) ) +
    geom_bar() +
  facet_grid(alert_type~.) +
  labs(x = "tran_timestep") +
  scale_fill_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(legend.position = "none")



# Compiling all plots in a single grid

grid.arrange(arrangeGrob(test_alrt_trans_box_tx_amt, test_alrt_trans_dens_tx_amt, nrow = 2, ncol = 1))

grid.arrange(arrangeGrob(test_alrt_trans_missing, test_alrt_trans_tx_amnt, test_alrt_trans_bar_alert_typ, test_time_tx_amount, nrow = 2, ncol = 2))

```

# 3. Feature engieering 

# 3.1 Data preperation

```{r, data preperation - feature engineering}

# 1. Data preparations

# 1.1 Defining needed data-pre processing functions
# Note: The following function removes and renames the the accounts raw data such that it is ready for downstream processes

# accounts data table

accounts_data_prep_func <- function(accounts_data_raw) {
  
  # 1. selecting variables and renaming them
  
  accounts_data_up <- accounts_data_raw %>% 
    select(acct_id, prior_sar_count, initial_deposit ) %>% 
     rename(client_ID = acct_id,
           is_fraud = prior_sar_count,
           init_balance = initial_deposit) %>% 
    mutate(is_fraud = if_else(is_fraud == "false", F, T))
  
  return(accounts_data_up)
}

# transaction data table

transactions_data_prep_func <- function(transactions_data_raw) {
  
  # 1. selecting variables and renaming them
  
  transactions_data_up <- transactions_data_raw %>% 
    select(orig_acct, bene_acct, base_amt, tran_timestamp) %>% 
     rename(from = orig_acct,
           to = bene_acct,
           weight = base_amt) %>% 
    mutate(tran_timestamp = ymd(str_sub(end =10, tran_timestamp)))
  
  return(transactions_data_up)
  
}

# 2. Applying data pre-processing functions

# training
training_transactions_final <- transactions_data_prep_func(training_transactions_raw)
training_accounts_final <- accounts_data_prep_func(training_accounts_raw)

# testing
testing_transactions_final <- transactions_data_prep_func(testing_transactions_raw)
testing_accounts_final <- accounts_data_prep_func(testing_accounts_raw)


### KAGGLE DATA SET (OLD) ###

# 1. Data preparation

# 1.1 transactions data & alerts data 

# joining alert type col from alerts df to transactions df 
# alerts_types <- alerts_data_raw %>% 
#   select(ALERT_ID, ALERT_TYPE)
# 
# transactions_data_up <- transactions_data_raw %>% 
#   left_join(alerts_types, by = "ALERT_ID") %>% 
#   unique()

# replacing NA values formed by transactions that are not fraud
# transactions_data_up[is.na(transactions_data_up)] <- "No Alert"

# rearranging columns such that sender node and receiver node is mentioned first in the df
# transactions_data_up <- transactions_data_up[,c(2,3,1,4:ncol(transactions_data_up))]

# 1.2 Selecting/renaming only needed cols for networks that will be generated

# nodes

# accounts_data_final <- accounts_data_raw %>% 
#   select(ACCOUNT_ID, INIT_BALANCE, IS_FRAUD) %>% 
#   rename(client_ID = ACCOUNT_ID,
#          is_fraud = IS_FRAUD,
#          init_balance = INIT_BALANCE) %>% 
#   mutate(is_fraud = if_else(is_fraud == "true", T, F))

# edges
# transactions_data_final <- transactions_data_raw %>% 
#   select(SENDER_ACCOUNT_ID, RECEIVER_ACCOUNT_ID, TX_AMOUNT, TIMESTAMP, IS_FRAUD) %>% 
#   rename(from = SENDER_ACCOUNT_ID,
#          to = RECEIVER_ACCOUNT_ID,
#          weight = TX_AMOUNT)


# 2. Data checks 

# 2.1 Checking when account is deemed as fraud

## sender accounts

# sender_transactions_fraud <- transactions_data_up %>% 
#   filter(IS_FRAUD == "True") %>% 
#   select(SENDER_ACCOUNT_ID) %>% 
#   unique()
# 
# receiver_transactions_fraud <- transactions_data_up %>% 
#   filter(IS_FRAUD == "False") %>% 
#   select(RECEIVER_ACCOUNT_ID) %>% 
#   unique()
# 
# clients <- accounts_data_raw %>%
#   select(IS_FRAUD, ACCOUNT_ID) %>% 
#   mutate(sender_account = ACCOUNT_ID %in% sender_transactions_fraud$SENDER_ACCOUNT_ID) %>% 
#   mutate(receiver_account = ACCOUNT_ID %in% receiver_transactions_fraud$RECEIVER_ACCOUNT_ID) %>% 
#   mutate(logic_tester = if_else(((sender_account = T) | (receiver_account = T)) & IS_FRAUD != "true", "Not Valid", "Valid"))
# 
# 
# not_valid_clients <- clients %>% 
#   filter(logic_tester == "Not Valid")

# Conclusion: NB! An account is deemed fraudulent if it is a recipient or sender of a flagged fraudulent transaction. 

# 2.2 Checking if there are any repetitions in node and edge df's

# nodes - accounts
# nrow(accounts_data_raw)
# 
# accounts_data_raw$ACCOUNT_ID %>%
#   unique() %>% 
#   length()
  
# Note: there are 1000 different accounts

# 2.2 links - transactions

# nrow(transactions_data_up)
# 
# nrow(unique(transactions_data_up[,c("SENDER_ACCOUNT_ID", "RECEIVER_ACCOUNT_ID")]))

# Note: There are more links than sender-receiver combination, therefore we have cases where there is multiple links between the same nodes. This will be adresed in the next code section.


```

# 3.2 Network construction - Undirected weighted network (simplified) function(s)

*Note:* The simplified graph will only consider one edge between incident nodes.

```{r, network construction and feature extraction - undirected weighted network (simplified) - feature engineering}

#### Un-directed weighted Network feature generation function ####

# Description: The function will take as input a sub-graph with more than 2 nodes and calculate the relevant network metrics.

# input: a weighted undirected sub-graph with more than two nodes
# output: a data frame containing all the network metrics extracted from the sub-graph. 

# NB! The function will only  be able to take as input an un-directed-weighted network.

# 1. Defining additional functions that will be used within the network feature generation function

# 1.1 Degree functions

# 1.1.1 Fraud degree calculations
fraud_degree_counter_func <- function(graph){
  
  # names of all the nodes in the network
  node_names <- V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  fraud_degree_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_fraud_degree <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      filter(name != node_names[i] & is_fraud == T) %>% 
      nrow()
    
    # saving fraud degree
    fraud_degree_vec[i] <- temp_fraud_degree
    
  }
  
  return(fraud_degree_vec)
  
}

# 1.1.2  Non-fraud degree calculations
non_fraud_degree_counter_func <- function(graph){
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  non_fraud_degree_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_non_fraud_degree <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      filter(name != node_names[i] & is_fraud == F) %>% 
      nrow()
    
    # saving fraud degree
    non_fraud_degree_vec[i] <- temp_non_fraud_degree
    
  }
  
  return(non_fraud_degree_vec)
  
}


# 1.2 Node density function

node_density_func <- function(graph) {
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  node_density_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    # calculating density and saving value
    node_density_vec[i] <- edge_density(temp_sub_graph)
    
  }
  
  return(node_density_vec)
  
}

# 1.3 Relational neighbor (classifier)

relational_neighbour_classifier_func <- function(graph) {
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store the probability of node being fraudulent or non-fraudulent according to its neighborhood
  prob_fraud_vec <- rep(NA, length(node_names))
  prob_non_fraud_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_df <- as_data_frame(temp_sub_graph, what = "vertices")
    
    # calculating number of # fraud neighbors
    temp_count_fraud <- temp_df %>% 
      filter(name != node_names[i] & is_fraud == T) %>% 
      nrow()
    
    # calculating number of # non-fraud neighbors
    temp_count_non_fraud <- temp_df %>% 
      filter(name != node_names[i] & is_fraud == F) %>% 
      nrow()
    
    # calculating nrmalisation factor (Z)
    Z =  temp_count_fraud + temp_count_non_fraud
    
    prob_fraud_vec[i] <- temp_count_fraud/Z
    
    prob_non_fraud_vec[i] <- temp_count_non_fraud/Z
    
    }
  
  probability_res <- list(prob_fraud = prob_fraud_vec,
                          prob_non_fraud = prob_non_fraud_vec) 
  
  
  return(probability_res)
  
}

# 1.4 Probabilistic relational neighbor (classifier)

prob_relational_neighbor_classifier_func <- function(graph, relational_neig_prob_fraud, relational_neig_prob_non_fraud){
  
  # adding relational neighborhood classifier results to graph
  V(graph)$relational_neig_prob_fraud <- relational_neig_prob_fraud
  V(graph)$relational_neig_prob_non_fraud <- relational_neig_prob_non_fraud
  
  # names of all the nodes in the network
  node_names <-V(graph)$name 
  
  # creating variable to store the probability of node being fraudulent or non-fraudulent according to its neighborhood
  prob_fraud_vec <- rep(NA, length(node_names))
  prob_non_fraud_vec <- rep(NA, length(node_names))
  
  # looping to calculate for each fraud degree
  for (i in 1:length(node_names)) {
    
    # extracting temp sub-graph for a single node
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                       impl = "auto")
    
    # converting sub-graph to data_frame and filtering
    temp_df <- as_data_frame(temp_sub_graph, what = "vertices")
    
    # calculating number of # fraud neighbors
    temp_count_fraud <- temp_df %>% 
      filter(name != node_names[i]) %>% 
      select(relational_neig_prob_fraud) %>% 
      sum()
    
    # calculating number of # non-fraud neighbor
    temp_count_non_fraud <- temp_df %>% 
      filter(name != node_names[i]) %>% 
      select(relational_neig_prob_non_fraud) %>% 
      sum()
    
    # calculating normalization factor (Z)
    Z =  temp_count_fraud + temp_count_non_fraud
    
    prob_fraud_vec[i] <- temp_count_fraud/Z
    
    prob_non_fraud_vec[i] <- temp_count_non_fraud/Z
    
  }
    

  
  probability_res <- list(prob_fraud = prob_fraud_vec,
                          prob_non_fraud = prob_non_fraud_vec) 
  
}

# 1.5 Calculating, Fraud, Semi-Fraud, and Legit triangles

# The function below classifies all the triangles found in a network as either being legit, fraud or semi-fraud
triangle_classifier_func <- function(graph){
  
  
  if(sum(count_triangles(graph)) > 0){
  
  # calculates the complete sub-graphs with 3 vertices (i.e triangles)
  cl.tri = cliques(graph,
                   min=3,
                   max=3)
  
  # constructing a data frame where each row corresponds to a triangle and each column to a node in that triangle
  df <- lapply(cl.tri, function(x){V(graph)$name[x]})
  triangles_df = data.frame(matrix(unlist(df),ncol=3,byrow=T))
  
  # creating class label for triangles (will be important later)
  triangles_df$triangle_label <- rep(NA,nrow(triangles_df))
  
  # creating a sub graph for each triangle
  for (i in 1:nrow(triangles_df)) {
    
    # counting fraud triangles
    temp_triangle <- c(triangles_df$X1[i], triangles_df$X2[i], triangles_df$X3[i])
    
    # creating sub_graph
    temp_sub_graph <- induced_subgraph(graph, 
                                       vids = temp_triangle, # triangle nodes
                                       impl = "auto")
    
    fraud_sum <- as_data_frame(temp_sub_graph, what = "vertices") %>% 
      select(is_fraud) %>% 
      sum()
    
    
    if(fraud_sum == 0){
      
      triangles_df$triangle_label[i] <- "legit_triangles"
      
    }else if(fraud_sum == 1 | fraud_sum == 2){
      
      triangles_df$triangle_label[i] <- "semi_fraud_triangles"
    }else{
      
      triangles_df$triangle_label[i] <- "fraud_triangles"
      
      } 
  }
  
  # changing df in long format
  triangles_df_long <- pivot_longer(triangles_df,
                                        cols = 1:3,
                                        names_to = "col_names",
                                        values_to = "client_ID"
                                        ) %>% 
  select(-col_names)
  
  # tabulating triangles result
  triangles_df_table_long <-  as.data.frame(t(table(triangles_df_long)))
  
  # converting to wide format
  triangles_df <- pivot_wider(triangles_df_table_long,
                              names_from = triangle_label,
                              values_from = Freq)
  
  }else{
    
   number_vertices <-  vcount(graph)  
   null_traingle_df <- as.data.frame(matrix(0, number_vertices, 3)) %>% 
     rename(legit_triangles = V1,
            semi_fraud_triangles = V2,
            fraud_triangles = V3)
   
   client_ID <- V(graph)$name
   
   triangles_df <- cbind(client_ID, null_traingle_df)
   
   }
  
  
  return(triangles_df)
  
}

# 2. Defining weighted network feature generation function 


undirected_weighted_network_feature_func <- function(sub_graph, verbose = c(TRUE,FALSE)){
  
  sample_network <- sub_graph
  
  # Note: Sample network refers to the current sub-graph that is used.
  
  # 1. Extracting network metrics
  
  # 1.1 Neighborhood metrics
  
  if(verbose){print("1. Calculating neighbourhood metrics")}
  
  # 1.1.1 Transitivity - (local) ratio of triangles to connected triples each vertex is part of. Can be interpreted as a probability for the network to have adjacent nodes interconnected, thus revealing the existence of tightly connected communities (or clusters, subgroups, cliques).
  
  if(verbose){print("1.1 Calculating transitivity...")}
  
  transitivity_loc_uw <- transitivity(sample_network, 
                                      type = "local",
                                      vids = V(sample_network),
                                      isolates = "zero",
                                      weights = E(sample_network)$weight) 

  # 1.1.2 Total Degree (local) - Here the degree is defined as the number of edges between connected nodes.
  
  if(verbose){print("1.2 Calculating total degree...")}
  
  total_degree_loc_uw <- degree(sample_network, 
                                       loops = F, 
                                       mode = "all")
  # Note: Loops are not counted.
  
  # 1.1.3  Calculating fraud degree
  
  # NB! Check the name and is_fraud column in the filtering operation when bigger network is used.
  
  if(verbose){print("1.3 Calculating fraud degree...")}
  
  is_fraud_degree_loc_uw <- fraud_degree_counter_func(graph = sample_network)
  
  # 1.1.4 Calculating  Non-fraud degree
  
  if(verbose){print("1.4 Calculating non-fraud degree...")}
  
  # calculating non-fraud degree
  non_fraud_degree_loc_uw <- non_fraud_degree_counter_func(graph = sample_network)
  
  # 1.1.5 Strength (weighted degree) - Summing up the edge weights of the adjacent edges for each vertex
  
  if(verbose){print("1.5 Calculating strength...")}
  
  degree_strength_loc_uw <- strength(sample_network,
                                     loops = F,
                                     mode = "all",
                                     weights = E(sample_network)$weight # NB! will change when  weights are incorporated
  )
  
  # NB!! Try to implement degree strength for fraud and non-fraud nodes (when implementing weighted network)

  # 1.1.6  Node density: considering an each node and its neighborhood of 1, then the node density is defined as the ratio of the number of edges and the number of possible edges (which you will find in a connected graph). 
  
  if(verbose){print("1.6 Calculating node density...")}
  
  node_density_loc_uw <- node_density_func(graph = sample_network)
  
  
  # 1.1.7 Relational neighbor: Assigns a probability to node i is part of class fraud or non-fraud given the class labels of node i's neighborhood. 
  
  if(verbose){print("1.7 Calculating relational neighbour...")}
  
  # results of the relational neighborhood classifier
  relational_neig_results <- relational_neighbour_classifier_func(graph = sample_network)
  
  # probability of non-fraud
  relational_neig_prob_non_fraud_loc_uw <- relational_neig_results$prob_non_fraud
  
  # probability of fraud
  relational_neig_prob_fraud_loc_uw <- relational_neig_results$prob_fraud
  
# 1.1.8 Probabilistic relational neighbor classifier
  
  if(verbose){print("1.8 Calculating probabilistic relational neighbour...")}
  
  # results of the prob relational neighborhood classifier
  prob_relational_neig_results <- prob_relational_neighbor_classifier_func(graph = sample_network,
                                                                           relational_neig_prob_fraud = relational_neig_prob_fraud_loc_uw,
                                                                           relational_neig_prob_non_fraud = relational_neig_prob_non_fraud_loc_uw)
  
  
  # probability of non-fraud
  prob_relational_neig_prob_non_fraud_loc_uw <- prob_relational_neig_results$prob_non_fraud
  
  # probability of fraud
  prob_relational_neig_prob_fraud_loc_uw <- prob_relational_neig_results$prob_fraud
  
  
  if(verbose){print("1.9 Calculating triangles...")}
  
  # 1.1.9 Triangles - Count how many triangles a vertex is part of, in a graph, or just list the triangles of a graph.
  
  # Total triangles
  
  triangles_loc_uw <- count_triangles(sample_network)
  
  # Note: This will be the total triangles Baesens et al. (2015) broke this up into: total fraud triangles, total legit triangles, and total semi-fraud triangles. 
  
  # Calculating, Fraud, Semi-Fraud, and Legit triangles
  
  sample_triangle_df <- triangle_classifier_func(sample_network)
  
  # creating dummy data frame
  sample_triangle_df_full <- data.frame(client_ID = V(sample_network)$name)
  
  # join operation
  sample_triangle_df_full <- left_join(sample_triangle_df_full,sample_triangle_df, by = "client_ID")
  
  # replacing NA values
  sample_triangle_df_full[is.na(sample_triangle_df_full)] <- 0
  

  # 1.2 Centrality metrics
  
  if(verbose){print("2. Calculating centrality metrics")}
  
  # 1.2.1 Closeness centrality and farness - Measures the average farness (inverse distance) to all other nodes. Nodes with a high closeness score have the shortest distances to all other nodes.
  
  if(verbose){print("2.1 Calculating closeness centrality and Farness...")}
  
  centrality_closeness_loc_uw <- closeness(sample_network,
                                           normalized = T, # can think of using TRUE on full network
                                           weights = E(sample_network)$weight) # Can use weights in full network  
  
  
  farness_loc_uw <- (centrality_closeness_loc_uw)^-1
  
  
  # 1.2.2 Eigenvector centrality - Eigenvector centrality scores correspond to the values of the first eigenvector of the graph adjacency matrix; these scores may, in turn, be interpreted as arising from a reciprocal process in which the centrality of each actor is proportional to the sum of the centralities of those actors to whom he or she is connected.
  
  if(verbose){print("2.2 Calculating eigenvector centrality...")}
  
  centrality_eigen_loc_uw_res <- eigen_centrality(sample_network,
                                              scale = T,
                                              directed = F,
                                              weights = E(sample_network)$weight) #weights can be included 
  
  centrality_eigen_loc_uw <- centrality_eigen_loc_uw_res$vector
  
  
  # 1.2.3 betweeness - the number of geodesics (shortest paths) going through a vertex or an edge
  
  if(verbose){print("2.3 Calculating betweeness...")}
  
  centrality_betweenness_loc_uw <-  betweenness(sample_network,
                                                directed = F,
                                                weights = E(sample_network)$weight,
                                                normalized = F) #weights can be included
  
  # 1.2.4  Average Geodesic - average length of all shortest paths (hops to nodes)
  
  if(verbose){print("2.4 Calculating average geodesic...")}
  
  geodesic_loc_uw <- distances(sample_network,
                                      mode = "all",
                                      weights = E(sample_network)$weight, # weights can be added 
                                      algorithm = "automatic")
  
  
  # excluding nodes that are unconnected
  avg_geodesic_loc_uw <- apply(geodesic_loc_uw, 1, mean)
  
  
  # 1.3 Inference algorithms 
  
  if(verbose){print("3. Calculating collective inference algorithms...")}
  
  # 1.3.1 PageRank
  
  # base PageRank algorithm
  pr_base_loc_uw_res <- page_rank(sample_network,
                            algo = "prpack",
                            directed = F,
                            damping = 0.85,
                            weights = E(sample_network)$weight) # can add weights 
  
  pr_base_loc_uw <- pr_base_loc_uw_res$vector
  
  # PageRank algorithm with emphasis on fraud nodes
  
  # defining start vector for algorithm (0 for legitimate clients and non-zero for fraudsters)
  total_fraud_nodes <- sum(V(sample_network)$is_fraud)

  
  # adding condition if there are any fraud nodes in the network then starting vector for page rank algorithm should be personalised
  if(total_fraud_nodes > 0){
  
  fraud_nodes_start_value <- 1/(total_fraud_nodes)
  start_vec_fraud <- if_else(V(sample_network)$is_fraud == F, 0, fraud_nodes_start_value)
  
  # calculating PageRank
  pr_fraud_loc_uw_res <- page_rank(sample_network,
                            algo = "prpack",
                            directed = F,
                            damping = 0.85,
                            personalized = start_vec_fraud,
                            weights = E(sample_network)$weight) # can add weights
  
  pr_fraud_loc_uw <- pr_fraud_loc_uw_res$vector
  
  }else{
    
  pr_fraud_loc_uw <-   pr_base_loc_uw
    
  }
  
  # 1.4 Compiling results in data frame
  
  if(verbose){print("Compiling calculated metrics in data frame...")}
  
  sample_network_feature_df <- data_frame(client_ID = V(sub_graph)$name,
                                          transitivity = transitivity_loc_uw,
                                          total_degree = total_degree_loc_uw,
                                          fraud_degree = is_fraud_degree_loc_uw,
                                          non_fraud_degree = non_fraud_degree_loc_uw,
                                          degree_strenght = degree_strength_loc_uw,
                                          node_density = node_density_loc_uw,
                                          relational_neighbour_not_fraud = relational_neig_prob_non_fraud_loc_uw,
                                          relational_neighbour_fraud = relational_neig_prob_fraud_loc_uw,
                                          probabilistic_relational_neighbour_not_fraud = prob_relational_neig_prob_non_fraud_loc_uw,
                                          probabilistic_relational_neighbour_fraud = prob_relational_neig_prob_fraud_loc_uw,
                                          total_triangles = triangles_loc_uw,
                                          legit_triangles = sample_triangle_df_full$legit_triangles,
                                          semi_fraud_triangles = sample_triangle_df_full$semi_fraud_triangles,
                                          fraud_triangles = sample_triangle_df_full$fraud_triangles,
                                          closeness_centrality = centrality_closeness_loc_uw,
                                          farness = farness_loc_uw,
                                          eigen_vector_centrality = centrality_eigen_loc_uw,
                                          betweeness = centrality_betweenness_loc_uw,
                                          avg_geodesic = avg_geodesic_loc_uw,
                                          page_rank_base = pr_base_loc_uw,
                                          page_rank_fraud = pr_fraud_loc_uw)
  

return(sample_network_feature_df)  

}

# undirected_weighted_network_feature_df <-  undirected_weighted_network_feature_func(sub_graph = sample_network_undirected_weighted, verbose = T)


```

# 3.3 Network construction - Directed network (non-simplified) function(s)

```{r, network construction - directed network (non-simplified) - feature engineering}

#### Directed Network feature generation function ####

# Description: The function will take as input a sub-graph with more than 2 nodes and calculate the relevant network metrics.

# input: a directed sub-graph with more than two nodes
# output: a data frame containing all the network metrics extracted from the sub-graph. 

# 1. Defining additional functions that will be used within the network feature generation function

# 1.1 Degree functions

# 1.1.1 Fraud degree calculations
directed_degree_counter_func <- function(graph, fraud_selection = c(TRUE, FALSE), degree_direction = c("in", "out")){
  
  # names of all the nodes in the network
  node_names <- V(graph)$name 
  
  # creating variable to store fraud degree counts for each node
  fraud_degree_vec <- rep(NA, length(node_names))
  
  if(degree_direction == "in"){
    
    # looping to calculate for each fraud degree
    for (i in 1:length(node_names)) {
      
      # extracting temp sub-graph for a single node
      temp_sub_graph <- induced_subgraph(graph, 
                                         vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                         impl = "auto")
      
      # converting sub-graph to data_frame and filtering
      temp_df <- as_data_frame(temp_sub_graph, what = "both")
      
      temp_edge_df <- temp_df$edges %>%
        rename(name = from)
      
      temp_fraud_count <- left_join(temp_edge_df,temp_df$vertices, by = "name") %>% 
        filter(name != node_names[i] & to == node_names[i] & is_fraud == fraud_selection) %>% 
        nrow() 
      
      # saving fraud degree
      fraud_degree_vec[i] <- temp_fraud_count
    }
  }
  
  if(degree_direction == "out"){
    
    # looping to calculate for each fraud degree
    for (i in 1:length(node_names)) {
      
      # extracting temp sub-graph for a single node
      temp_sub_graph <- induced_subgraph(graph, 
                                         vids = unlist(neighborhood(graph = graph, order = 1, nodes = node_names[i])),
                                         impl = "auto")
      
      # converting sub-graph to data_frame and filtering
      temp_df <- as_data_frame(temp_sub_graph, what = "both")
      
      temp_edge_df <- temp_df$edges %>%
        rename(name = to)
      
      temp_fraud_count <- left_join(temp_edge_df,temp_df$vertices, by = "name") %>% 
        filter(name != node_names[i] & from == node_names[i] & is_fraud == fraud_selection) %>% 
        nrow() 
      
      # saving fraud degree
      fraud_degree_vec[i] <- temp_fraud_count
    }
  }
  
  return(fraud_degree_vec)
  
}

# 2. Defining directed network feature generation function 

directed_network_feature_function <- function(sub_graph, verbose = c(TRUE, FALSE)){
  
  # 1. Extracting network metrics
  
  if(verbose == T){print("1. Calculating neighbourhood metrics (directed)...")}
  
  # 1.1 Neighborhood metrics
  
  if(verbose == T){print("1.1 Calculating in-degree...")}
  # 1.1.1 in-degree
  in_degree_loc_d <- degree(sub_graph, 
                            mode = "in",
                            loops = F,
                            normalized = F)
  
  if(verbose == T){print("1.2 Calculating out-degree...")}
  
  # 1.1.2 out-degree
  out_degree_loc_d <- degree(sub_graph, 
                            mode = "out",
                            loops = F,
                            normalized = F)
  
  # 1.1.3 in-degree (fraudulent/non-fraudulent clients)
  
  if(verbose == T){print("1.3 Calculating fraud in-degree...")}
  # number of transactions made by fraudulent node to the specific reference node
  in_degree_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = T, degree_direction = "in")
  
  if(verbose == T){print("1.4 Calculating non-fraud in-degree...")}
  # number of transactions made by non-fraudulent node to the specific reference node
  in_degree_non_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = F, degree_direction = "in")
  
  # 1.1.4 out-degree (fraudulent/non-fraudulent clients)
  
  if(verbose == T){print("1.5 Calculating fraud out-degree...")}
  # number of transactions made by fraudulent node to the specific reference node
  out_degree_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = T, degree_direction = "out")
  
  if(verbose == T){print("1.6 Calculating non-fraud out-degree...")}
  # number of transactions made by non-fraudulent node to the specific reference node
  out_degree_non_fraud_loc_d <- directed_degree_counter_func(sub_graph, fraud_selection = F, degree_direction = "out")
  
  
  # 1.2 Compiling results in data frame
  
  if(verbose){print("Compiling calculated metrics in data frame...")}
  
  sample_network_feature_df <- data_frame(client_ID = V(sub_graph)$name,
                                          in_degree = in_degree_loc_d,
                                          in_degree_fraud = in_degree_fraud_loc_d,
                                          in_degree_non_fraud = in_degree_non_fraud_loc_d,
                                          out_degree = out_degree_loc_d,
                                          out_degree_fraud = out_degree_fraud_loc_d,
                                          out_degree_non_fraud = out_degree_non_fraud_loc_d
                                          )
  
return(sample_network_feature_df) 
  
  
  
  }

#directed_network_feature_df <-  directed_network_feature_function(sub_graph = sample_network_directed_weighted, verbose = T)


```

# 3.4 Feature extraction - Transactional data function(s)

```{r, feature extraction - transactional data function(s) - feature engineering}

#### Transaction feature generation function ####

# Description: The function will take as input the accounts_df and the transactions df and output the metrics  

# input: accounts_df and the transactions df
# output: a data frame containing transaction metrics 

transaction_feature_function <- function(transaction_df, accounts_df){ 
  
  # Can drill each of down into fraud and non-fraud
  
  # 1. Generating incoming transaction stats
  
  # basic incoming stats
  incoming_transaction_stats <- transaction_df %>% 
    mutate(is_round = if_else(plyr::round_any(weight, 10) == weight, T, F)) %>% 
    group_by(to) %>% 
    summarise(incoming_total = sum(weight),
              incoming_avg = mean(weight),
              incoming_sd = sd(weight),
              incoming_median = median(weight),
              incoming_max = max(weight),
              incoming_min = min(weight),
              incoming_round_numbers_count = sum(is_round))
  
  # fraud/non-fraud incoming transaction totals - receiving from fraudsters/non-fraudsters
  incoming_transactions_fraud_non_fraud <- transaction_df %>% 
    rename(client_ID = from) %>% 
    left_join(accounts_df, by = "client_ID")%>% 
    group_by(to, is_fraud) %>% 
    summarise(total = sum(weight)) %>% 
    pivot_wider(names_from = is_fraud,
                values_from = total) %>% 
    rename(fraud_total_income = `TRUE`,
           non_fraud_total_income = `FALSE`)
  
  # removing NA's
  incoming_transactions_fraud_non_fraud[is.na(incoming_transactions_fraud_non_fraud)] <- 0
  
  # joining and creating final df 
  incoming_transaction_stats_final <- left_join(incoming_transaction_stats,incoming_transactions_fraud_non_fraud, by = "to" ) %>% 
    rename(client_ID = to)
  
  
  # 2. Generating outgoing transaction stats
  
  # basic outgoing stats
  outgoing_transaction_stats <- transaction_df %>% 
    mutate(is_round = if_else(plyr::round_any(weight, 10) == weight, T, F)) %>%
    group_by(from) %>% 
    summarise(outgoing_total = sum(weight),
              outgoing_avg = mean(weight),
              outgoing_sd = sd(weight),
              outgoing_median = median(weight),
              outgoing_max = max(weight),
              outgoing_min = min(weight),
              outgoing_round_numbers_count = sum(is_round))
  
  # fraud/non-fraud incoming transaction totals - paying fraudsters/non-fraudsters
  outgoing_transactions_fraud_non_fraud <- transaction_df %>% 
    rename(client_ID = to) %>% 
    left_join(accounts_df, by = "client_ID")%>% 
    group_by(from, is_fraud) %>% 
    summarise(total = sum(weight)) %>% 
    pivot_wider(names_from = is_fraud,
                values_from = total) %>% 
    rename(total_fraud_payments = `TRUE`,
           total_non_fraud_payments = `FALSE`)
  
  # removing NA's
  outgoing_transactions_fraud_non_fraud[is.na(outgoing_transactions_fraud_non_fraud)] <- 0
  
  # joining and creating final df 
  outgoing_transaction_stats_final <- left_join(outgoing_transaction_stats,outgoing_transactions_fraud_non_fraud, by = "from" ) %>% 
    rename(client_ID = from)
  
  #### Period stats (in funcrion) ####
  
  # 3. Generating time period stats for accounts
  
  # 3.1 payment made (from)
  
  # all accounts that made a payment
  payment_IDs <- unique(transaction_df$from)
  
  # data frame that stores the results
  payment_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        payment_period_max = rep(NA, length(payment_IDs)),
                                        payment_period_min = rep(NA, length(payment_IDs)),
                                        payment_period_avg = rep(NA, length(payment_IDs)),
                                        payment_period_std = rep(NA, length(payment_IDs)))
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(from == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA,nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- max(current_days_difference, na.rm =T)
      payment_period_stats_df$payment_period_min[i] <- min(current_days_difference, na.rm =T) + 1
      payment_period_stats_df$payment_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      payment_period_stats_df$payment_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      payment_period_stats_df$payment_period_max[i] <- 1
      payment_period_stats_df$payment_period_min[i] <- 1 
      payment_period_stats_df$payment_period_avg[i] <- 1 
      payment_period_stats_df$payment_period_std[i] <- 0 
      
    }
    
  }
  
  # 3.2 payment received (to)
  
  # all accounts that received a payment
  payment_IDs <- unique(transaction_df$to)
  
  # data frame that stores the results
  receive_period_stats_df <- data.frame(client_ID = payment_IDs,
                                        receive_period_max = rep(NA, length(payment_IDs)),
                                        receive_period_min = rep(NA, length(payment_IDs)),
                                        receive_period_avg = rep(NA, length(payment_IDs)),
                                        receive_period_std = rep(NA, length(payment_IDs)))
  
  
  for (i in 1:length(payment_IDs)) {
    
    # filtering a specific account that made payment(s)
    current_df <- transaction_df %>% 
      filter(to == payment_IDs[i]) %>% 
      arrange(tran_timestamp)
    
    if(nrow(current_df) > 1){
      
      # creating vector that stores the time difference between payments made by account
      current_days_difference <- rep(NA, nrow(current_df))
      
      for (j in 2:nrow(current_df)) {
        
        current_days_difference[j] <- difftime(current_df$tran_timestamp[j],current_df$tran_timestamp[j-1], units = "days")
        
      }
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- max(current_days_difference, na.rm =T)
      receive_period_stats_df$receive_period_min[i] <- min(current_days_difference, na.rm =T) + 1 # Assumption that payment is made in the morning
      receive_period_stats_df$receive_period_avg[i] <- mean(current_days_difference, na.rm =T) 
      receive_period_stats_df$receive_period_std[i] <- sd(current_days_difference, na.rm =T) 
      
    }else{
      
      # saving stats variables to created data frame
      receive_period_stats_df$receive_period_max[i] <- 1
      receive_period_stats_df$receive_period_min[i] <- 1 
      receive_period_stats_df$receive_period_avg[i] <- 1 
      receive_period_stats_df$receive_period_std[i] <- 0 
      
    }
    
  }
  
  
  # 4. Creating transaction feature df
  
  transaction_feature_df <- data.frame(client_ID = accounts_df$client_ID,
                                       init_balance = accounts_df$init_balance)
  
  transaction_feature_df <- left_join(transaction_feature_df, incoming_transaction_stats_final, by = "client_ID") %>% 
    left_join(outgoing_transaction_stats_final, by = "client_ID") %>% 
    left_join(receive_period_stats_df, by = "client_ID") %>% 
    left_join(payment_period_stats_df, by = "client_ID")
  
  # replacing NA values
  
  cols_to_replace <- c("incoming_total", "incoming_avg", "incoming_max", "incoming_min", "incoming_round_numbers_count", "non_fraud_total_income", "fraud_total_income" , "outgoing_total", "outgoing_avg", "outgoing_max", "outgoing_min", "outgoing_round_numbers_count", "total_non_fraud_payments", "total_fraud_payments", "receive_period_min", "payment_period_min", "receive_period_max" , "payment_period_max", "payment_period_avg", "receive_period_avg")
  
  transaction_feature_df[cols_to_replace][is.na(transaction_feature_df[cols_to_replace])] <- 0
  
  # Note: SD and Medians have NA values
  
  return(transaction_feature_df)
  
}


```

# 3.5  Constructing structured data tables

*Note:*
1. For the network feature data set a function will:
 i) Extract the transaction features from data.
 ii) Construct the needed type of networks (weighted-undirected network & Directed network).
 iii) For each graph component (V > 2) extract the weighted-undirected features & directed features.

2. The transactional and combined feature data sets will be generated at the end of the chunk (does not form part of above function)  

```{r, constructing structured data tables - feature engineering}

# 1. Defining function that outputs final pre-processed data

feature_generation_func <- function(final_transactional_df, final_accounts_df) {
  
  # 1. Extracting transaction features
  
  print("*** Generating transactional data features ***")
  
  transaction_feature_df <- transaction_feature_function(transaction_df = final_transactional_df , accounts_df = final_accounts_df)
  transaction_feature_df$client_ID <- as.character(transaction_feature_df$client_ID)
  
  # 2. Constructing needed networks
  
  print("*** Constructing networks ***")
  
  # 2.1 Un-directed-weighted network
  
  # constructing raw network
  undirected_weighted_graph_raw <- graph_from_data_frame(d = final_transactional_df,
                                                               directed = F,
                                                               vertices = final_accounts_df) 
  
  # Simplifying the graph (removing multiple edges)
  undirected_weighted_graph_simp <- simplify(undirected_weighted_graph_raw,
                                                   remove.multiple = T,
                                                   remove.loops = T,
                                                   edge.attr.comb = c(weight = "sum"))
  
  # 2.2 Directed graph
  
  weight_ind <- which(names(final_transactional_df) == 'weight')
  
  directed_graph <- graph_from_data_frame(d = final_transactional_df[,-weight_ind],
                                                directed = T,
                                                vertices = final_accounts_df)
  
  # 3. Extract the network features for each component in the generated  graphs
  
  print("*** Extracting network features from each component in the graph(s) ***")
  
  # 3.1 Un-directed weighted graph
  
  print("* Un-direceted weighted graph  *")
  
  # decomposing graph into its graph components
  undirected_weighted_graph_components <- decompose.graph(graph = undirected_weighted_graph_simp,
                                                                min.vertices = 3) 
  
  # creating loop to extract the network features from each graph component
  
  undirected_weighted_graph_features <- data.frame()
  
  for (i in 1:length(undirected_weighted_graph_components)) {
    
    print(paste0("Busy with graph component ", i) )
    
    # assigning the ith component
    current_undirected_weighted_component <- undirected_weighted_graph_components[[i]]
    
    # extracting network features
    current_undirected_weighted_graph_features <- undirected_weighted_network_feature_func(sub_graph = current_undirected_weighted_component, verbose = T)
    
    # constructing final feature df
    undirected_weighted_graph_features <- rbind(undirected_weighted_graph_features, current_undirected_weighted_graph_features)
  
  }
  
  # Note: The number of accounts we are investigating decreased due to graph components that have less than 3 vertices are ommitted
  
  # 3.2 Directed graph
  
  print("* Direceted graph  *")

  # decomposing graph into its graph components
  directed_graph_components <- decompose.graph(graph = directed_graph,
                                                     min.vertices = 3) 
  
  # Note: Multiple graph components identified
  
  # creating loop to extract the network features from each graph component
  
  directed_graph_features <- data.frame()
  
  for (i in 1:length(directed_graph_components)) {
    
    print(paste0("Busy with graph component ", i) )
    
    # assigning the ith component
    current_directed_component <- directed_graph_components[[i]]
    
    # extracting network features
    current_directed_graph_features <- directed_network_feature_function(sub_graph = current_directed_component, verbose = T)
    
    # constructing final feature df
    directed_graph_features <- rbind(directed_graph_features, current_directed_graph_features)
    
  }

  
  # 4. Combing generated data tables into one feature df
  
  print("*** Creating feature data tables  ***")
  
  client_df <- data.frame(client_ID = as.character(final_accounts_df$client_ID),
                               is_fraud = final_accounts_df$is_fraud)
  
  network_feature_df <- left_join(undirected_weighted_graph_features, directed_graph_features, by = "client_ID") %>%  
    left_join(client_df, by = "client_ID")
  
  network_feature_clients <- data.frame(client_ID = network_feature_df$client_ID)
  
  transaction_feature_df <- left_join(network_feature_clients, transaction_feature_df, by = "client_ID") %>% 
    left_join(client_df, by = "client_ID")

  feature_tables <- list(network_features = network_feature_df,
                         transactional_features = transaction_feature_df) 
  
  return(feature_tables)
  
}



# 2. Configuring training data features

training_features <- feature_generation_func(final_transactional_df = training_transactions_final,
                                             final_accounts_df = training_accounts_final)

train_network_features <- training_features$network_features
train_transactional_features <- training_features$transactional_features

train_fraud_ind <- which(names(train_network_features) == "is_fraud")
train_all_features <- left_join(train_network_features[,-train_fraud_ind], train_transactional_features, by = "client_ID")

# 3. Configuring testing data features

testing_features <- feature_generation_func(final_transactional_df = testing_transactions_final,
                                             final_accounts_df = testing_accounts_final)

test_network_features <- testing_features$network_features
test_transactional_features <- testing_features$transactional_features

test_fraud_ind <- which(names(train_network_features) == "is_fraud")
test_all_features <- left_join(test_network_features[,-test_fraud_ind], test_transactional_features, by = "client_ID")


```

# 4. EDA of structured data table(s)

```{r, EDA of structured data}

#(can include if needed)

```

# 5. Modelling

In the following code chunks we will examine the performance of different models:
i) Only using network features as inputs (network feature data set).
ii) Only using transactional features as inputs (transactional feature data set).
iii) Using network and transactional features (combined feature data set).

# 5.1 Logistic regression models 

# 5.1.1 Data pre-processing

```{r, data pre-processing - logistic regression models - modelling}

# 1. Data-pre processing for LR model

# 1.1 transforming response to factor variable
train_network_features$is_fraud <- as.factor(train_network_features$is_fraud)
train_transactional_features$is_fraud <- as.factor(train_transactional_features$is_fraud)
train_all_features$is_fraud <- as.factor(train_all_features$is_fraud)

test_network_features$is_fraud <- as.factor(test_network_features$is_fraud)
test_transactional_features$is_fraud <- as.factor(test_transactional_features$is_fraud)
test_all_features$is_fraud <- as.factor(test_all_features$is_fraud)

# 1.2 omitting all columns that have NA values
train_transactional_features <- train_transactional_features[, colSums(is.na(train_transactional_features)) == 0]
train_all_features <- train_all_features[, colSums(is.na(train_all_features)) == 0]

test_transactional_features <- test_transactional_features[, colSums(is.na(test_transactional_features)) == 0]
test_all_features <- test_all_features[, colSums(is.na(test_all_features)) == 0]

# 1.3 scaling features
train_network_features[,2:28] <- scale(train_network_features[,2:28])
train_transactional_features[,2:22] <- scale(train_transactional_features[,2:22])
train_all_features[,2:49] <- scale(train_all_features[,2:49])

test_network_features[,2:28] <- scale(test_network_features[,2:28])
test_transactional_features[,2:22] <- scale(test_transactional_features[,2:22])
test_all_features[,2:49] <- scale(test_all_features[,2:49])

# 1.4 dropping client_ID column
train_network_features <- train_network_features[,-1]
train_transactional_features <- train_transactional_features[,-1]
train_all_features <- train_all_features[,-1]

test_network_features <- test_network_features[,-1]
test_transactional_features <- test_transactional_features[,-1]
test_all_features <- test_all_features[,-1]


```

# 5.1.2 Hyper-parameter tuning 

```{r, hyper-parameter tuning - logistic regression models - modelling}

# 0. Defining function that extracts Beta coefficients of the predictors

LR_coef_extraction <- function(lr_model, training_set_full){
  
  # extracting beta coefficients from LR model
  LR_coef_df <- as.data.frame(summary(coef(lr_model, s = "lambda.1se")))
  
  # creating predictor data frame to store results
  LR_predictors <- data.frame(i = c(1, 2:(ncol(training_set_full))),
                                    predictor_name = c("(Intercept)", names(training_set_full[,-c(ncol(training_set_full))])))
  
  # combining coef and predictor df
  LR_predictors_up <- LR_predictors %>% 
    left_join(LR_coef_df, by = "i") %>% 
    rename(beta_coef = x) %>% 
    select(predictor_name, beta_coef)
  
  # remvoving NA values
  LR_predictors_up[is.na(LR_predictors_up)] <- 0
  
  
  return(LR_predictors_up)
  
}



# 1. Constructing standard logistic regression models

# network features LR model
log_std_model_network <- glm(is_fraud ~ . , data = train_network_features, family = "binomial")
summary(log_std_model_network)
round(exp(coef(log_std_model_network)),3)

# transactional features LR model
log_std_model_trans <- glm(is_fraud ~ . , data = train_transactional_features, family = "binomial")
summary(log_std_model_trans)
round(exp(coef(log_std_model_trans)),3)

# all features LR model
log_std_model_all <- glm(is_fraud ~ . , data = train_all_features, family = "binomial")
summary(log_std_model_all)
round(exp(coef(log_std_model_all)),3)

# 2. Constructing logistic regression with LASSO penalty

# network features
log_LASSO_model_network <- glmnet(x = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                                                y = train_network_features$is_fraud , 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_network, xvar = 'lambda', label=TRUE)


# 2.1 Generating coeficient plots

coef_matrix_df <- data.frame(as.list(log_LASSO_model_network$beta[1,]))


for (i in 2:dim(log_LASSO_model_network$beta)[1]) {
  
  current_predictor <- data.frame(as.list(log_LASSO_model_network$beta[i,])) 

  coef_matrix_df <- rbind(coef_matrix_df, current_predictor)
  
}


coef_matrix_df$predictors <- names(train_network_features[,-c(ncol(train_network_features))])

# pivot longer

coef_df <- pivot_longer(coef_matrix_df,
                        cols = 1:ncol(coef_matrix_df)-1,
                        names_to = "lambda_itter", 
                        values_to = "coef_value" ) %>% 
  mutate(log_lambda_value = rep(log(log_LASSO_model_network$lambda),dim(log_LASSO_model_network$beta)[1])) %>% 
  mutate(number =  rep(1:dim(log_LASSO_model_network$beta)[2],dim(log_LASSO_model_network$beta)[1])) %>% 
  mutate(number_char = as.character(rep(1:dim(log_LASSO_model_network$beta)[1], each = dim(log_LASSO_model_network$beta)[2])))

coef_df$label <- NA
coef_df$label[which(coef_df$number == max(coef_df$number))] <- coef_df$number_char[which(coef_df$number == max(coef_df$number))]

zero_coef_predictors <- coef_df %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  filter(sum_coef == 0)


## Coefficient visualizations


predictors_non_zero_coef <- coef_df %>% 
  filter(!predictors %in% zero_coef_predictors$predictors)

large_coef_useful_LR_predictors <-  predictors_non_zero_coef %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  arrange(desc(sum_coef)) %>% 
  head(8)

LR_plot_net_predictors_1 <- predictors_non_zero_coef %>%
  filter(predictors %in% large_coef_useful_LR_predictors$predictors)
  
LR_plot_net_predictors_2 <- predictors_non_zero_coef %>%
  filter(!predictors %in% large_coef_useful_LR_predictors$predictors)

# plot 1
LR_plot_net_predictors_1 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.8) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  geom_vline(xintercept = min(log(log_LASSO_model_network$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())


# plot 1
LR_plot_net_predictors_2 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.8) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  ylim(-1, 1) +
  geom_vline(xintercept = min(log(log_LASSO_model_network$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())



# transaction features

log_LASSO_model_trans <- glmnet(x = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                                                y = train_transactional_features$is_fraud, 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_trans, xvar = 'lambda', label=TRUE)


# Generating coeficient plots

coef_matrix_df <- data.frame(as.list(log_LASSO_model_trans$beta[1,]))


for (i in 2:dim(log_LASSO_model_trans$beta)[1]) {
  
  current_predictor <- data.frame(as.list(log_LASSO_model_trans$beta[i,])) 

  coef_matrix_df <- rbind(coef_matrix_df, current_predictor)
  
}


coef_matrix_df$predictors <- names(train_transactional_features[,-c(ncol(train_transactional_features))])

# pivot longer

coef_df <- pivot_longer(coef_matrix_df,
                        cols = 1:ncol(coef_matrix_df)-1,
                        names_to = "lambda_itter", 
                        values_to = "coef_value" ) %>% 
  mutate(log_lambda_value = rep(log(log_LASSO_model_trans$lambda),dim(log_LASSO_model_trans$beta)[1])) %>% 
  mutate(number =  rep(1:dim(log_LASSO_model_trans$beta)[2],dim(log_LASSO_model_trans$beta)[1])) %>% 
  mutate(number_char = as.character(rep(1:dim(log_LASSO_model_trans$beta)[1], each = dim(log_LASSO_model_trans$beta)[2])))

coef_df$label <- NA
coef_df$label[which(coef_df$number == max(coef_df$number))] <- coef_df$number_char[which(coef_df$number == max(coef_df$number))]

zero_coef_predictors <- coef_df %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  filter(sum_coef == 0)


## Coefficient visualizations


predictors_non_zero_coef <- coef_df %>% 
  filter(!predictors %in% zero_coef_predictors$predictors)

large_coef_useful_LR_predictors <-  predictors_non_zero_coef %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  arrange(desc(sum_coef)) %>% 
  head(7)

LR_plot_trans_predictors_1 <- predictors_non_zero_coef %>%
  filter(predictors %in% large_coef_useful_LR_predictors$predictors)
  
LR_plot_trans_predictors_2 <- predictors_non_zero_coef %>%
  filter(!predictors %in% large_coef_useful_LR_predictors$predictors)

# plot 1
LR_plot_trans_predictors_1 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.8) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  geom_vline(xintercept = min(log(log_LASSO_model_trans$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())


# plot 1
LR_plot_trans_predictors_2 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.8) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  geom_vline(xintercept = min(log(log_LASSO_model_trans$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())



# all features

log_LASSO_model_all <- glmnet(x = as.matrix(train_all_features[,-c(ncol(train_all_features))]),
                                                y = train_all_features$is_fraud, 
                                                alpha = 1, 
                                                standardize = FALSE, 
                                                family = 'binomial')

plot(log_LASSO_model_all, xvar = 'lambda', label=TRUE)

# Generating coefficient plots

coef_matrix_df <- data.frame(as.list(log_LASSO_model_all$beta[1,]))


for (i in 2:dim(log_LASSO_model_all$beta)[1]) {
  
  current_predictor <- data.frame(as.list(log_LASSO_model_all$beta[i,])) 

  coef_matrix_df <- rbind(coef_matrix_df, current_predictor)
  
}


coef_matrix_df$predictors <- names(train_all_features[,-c(ncol(train_all_features))])

# pivot longer

coef_df <- pivot_longer(coef_matrix_df,
                        cols = 1:ncol(coef_matrix_df)-1,
                        names_to = "lambda_itter", 
                        values_to = "coef_value" ) %>% 
  mutate(log_lambda_value = rep(log(log_LASSO_model_all$lambda),dim(log_LASSO_model_all$beta)[1])) %>% 
  mutate(number =  rep(1:dim(log_LASSO_model_all$beta)[2],dim(log_LASSO_model_all$beta)[1])) %>% 
  mutate(number_char = as.character(rep(1:dim(log_LASSO_model_all$beta)[1], each = dim(log_LASSO_model_all$beta)[2])))

coef_df$label <- NA
coef_df$label[which(coef_df$number == max(coef_df$number))] <- coef_df$number_char[which(coef_df$number == max(coef_df$number))]

zero_coef_predictors <- coef_df %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  filter(sum_coef < 0.00001)


## Coefficient visualizations


predictors_non_zero_coef <- coef_df %>% 
  filter(!predictors %in% zero_coef_predictors$predictors)

large_coef_useful_LR_predictors <-  predictors_non_zero_coef %>% 
  mutate(abs_coef = abs(coef_value)) %>% 
  group_by(predictors, number_char) %>% 
  summarise(sum_coef = sum(abs_coef)) %>% 
  arrange(desc(sum_coef)) %>% 
  head(16)

LR_plot_trans_predictors_1 <- predictors_non_zero_coef %>%
  filter(predictors %in% large_coef_useful_LR_predictors$predictors)
  
LR_plot_trans_predictors_2 <- predictors_non_zero_coef %>%
  filter(!predictors %in% large_coef_useful_LR_predictors$predictors)

# plot 1
LR_plot_trans_predictors_1 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.6) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  #ylim(-5,5) +
  geom_vline(xintercept = min(log(log_LASSO_model_all$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())


# plot 2
LR_plot_trans_predictors_2 %>% 
  ggplot(aes(y = coef_value, x = log_lambda_value, col = predictors, lty = )) +
  geom_line(size = 0.6) +
  geom_hline(yintercept = 0) +
  # geom_label_repel(aes(label = label),
  #                  nudge_x = -4,
  #                  nudge_y = 0.5,
  #                  na.rm = TRUE,
  #                  max.overlaps = dim(log_LASSO_model_network$beta)[1]) +
  # xlim(-10.5,max(log(log_LASSO_model_network$lambda))) +
  geom_vline(xintercept = min(log(log_LASSO_model_all$lambda)), 
             linetype = "dashed") +
  labs(y = "Predictor coefficient value", x = expression(Log(lambda))) +
  
  theme_bw() +
  theme(legend.position = "top", legend.title = element_blank())

# 3.1 Applying 10-fold CV to determine optimal lambda value

# network features
log_cv_model_network <- cv.glmnet(x = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                               y = train_network_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_network)
log_cv_model_network$lambda.1se

### NEW ###
network_coef_df <- LR_coef_extraction(lr_model = log_cv_model_network, 
                                      training_set_full = train_network_features)

# visualisation network coefficent

network_coef_df[-1,] %>% # removing intercept 
  ggplot(aes(x = predictor_name, y = beta_coef)) +
  geom_segment(
    aes(x=predictor_name, xend=predictor_name, y=0, yend=beta_coef), 
    color=ifelse(network_coef_df[-1,]$beta_coef != 0, "blue", "grey"), 
    size=ifelse(network_coef_df[-1,]$beta_coef != 0, 1, 0.7)) +
  geom_point(
    color=ifelse(network_coef_df[-1,]$beta_coef != 0, "blue", "grey"), 
    size=ifelse(network_coef_df[-1,]$beta_coef != 0,  1, 0.7)
  ) + 
  labs(x = "Predictors", y = "Coefficients") +
  coord_flip() +
  theme_bw()
### NEW ###

# transactional features
log_cv_model_trans <- cv.glmnet(x = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                               y = train_transactional_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_trans)
log_cv_model_trans$lambda.1se


### NEW ###
trans_coef_df <- LR_coef_extraction(lr_model = log_cv_model_trans, 
                                      training_set_full = train_transactional_features)

# visualisation network coefficent

trans_coef_df[-1,] %>% # removing intercept 
  ggplot(aes(x = predictor_name, y = beta_coef)) +
  geom_segment(
    aes(x=predictor_name, xend=predictor_name, y=0, yend=beta_coef), 
    color=ifelse(trans_coef_df[-1,]$beta_coef != 0, "red", "grey"), 
    size=ifelse(trans_coef_df[-1,]$beta_coef != 0, 1, 0.7)) +
  geom_point(
    color=ifelse(trans_coef_df[-1,]$beta_coef != 0, "red", "grey"), 
    size=ifelse(trans_coef_df[-1,]$beta_coef != 0,  1, 0.7)
  ) + 
  labs(x = "Predictors", y = "Coefficients") +
  coord_flip() +
  theme_bw()
### NEW ###


# all features
log_cv_model_all <- cv.glmnet(x = as.matrix(train_all_features[,-c(ncol(train_all_features))]),
                               y = train_all_features$is_fraud,
                               alpha = 1, # LASSO
                               nfolds = 10,
                               type.measure = "mse",
                               family = "binomial",
                               standardize = F)

plot(log_cv_model_all)
log_cv_model_all$lambda.1se

### NEW ###
all_coef_df <- LR_coef_extraction(lr_model = log_cv_model_all, 
                                      training_set_full = train_all_features)

# visualisation network coefficent
 all_coef_df[-1,] %>% # removing intercept 
  mutate(predictor_color = if_else(predictor_name %in% names(train_network_features), "blue", "red")) %>% # removing intercept
  ggplot(aes(x = predictor_name, y = beta_coef)) +
  geom_segment(
    aes(x=predictor_name, xend=predictor_name, y=0, yend=beta_coef), 
    color=ifelse(test$predictor_color == "blue", "blue", "red"), 
    size=ifelse(test$beta_coef != 0, 1, 0.5)) +
  geom_point(
    color=ifelse(test$predictor_color == "blue", "blue", "red"), 
    size=ifelse(test$beta_coef != 0,  1, 0.5)
  ) + 
  labs(x = "Predictors", y = "Coefficients") +
  coord_flip() +
  theme_bw()
### NEW ###


# 4. Probability Predictions (Training)

# 4.1 Network features

# Regular LR model
pi_hat_log_std_network <- predict(log_std_model_network, type = "response")

# LASSO LR model
pi_hat_log_lasso_network <- predict(log_cv_model_network, 
                                    newx = as.matrix(train_network_features[,-c(ncol(train_network_features))]),
                                    s = log_cv_model_network$lambda.1se,
                                    type = "response")


# 4.2 Transactions features

# Regular LR model
pi_hat_log_std_trans <- predict(log_std_model_trans, type = "response")

# LASSO LR model
pi_hat_log_lasso_trans <- predict(log_cv_model_trans, 
                                    newx = as.matrix(train_transactional_features[,-c(ncol(train_transactional_features))]),
                                    s = log_cv_model_trans$lambda.1se,
                                    type = "response")


# 4.3 all features
pi_hat_log_std_all <- predict(log_std_model_all, type = "response")

# LASSO LR model
pi_hat_log_lasso_all <- predict(log_cv_model_all, 
                                    newx = as.matrix(train_all_features[,-c(ncol(train_all_features))]),
                                    s = log_cv_model_all$lambda.1se,
                                    type = "response")


# 5. Choosing threshold value

# 5.1 Defining function that extract the optimal threshold value according to ROC and precision recall curves

# 5.1.1 ROC
optimal_threshold_ROC <- function(model_performance){
  
  # 1. Calculating G-mean (Geometric mean)
  gmeans <- sqrt(model_performance@y.values[[1]]*(1-model_performance@x.values[[1]]) )
  
  # 2. Extracting threshold values
  thresholds <- model_performance@alpha.values[[1]]
  
  # 3. Getting index where geometric mean is the highes
  max_gmean_ind <- which(gmeans == max(gmeans, na.rm = T))
  
  # Extracting Index
  optimal_ROC_threshold <- thresholds[max_gmean_ind]
  
  if(length(optimal_ROC_threshold) > 1){
    
    optimal_ROC_threshold <- optimal_ROC_threshold[1]
    
  }

  
  # 4. Plotting results
  plot(model_performance, colorize = FALSE, col = 'black')
  lines(c(0,1), c(0,1), col = 'gray', lty = 4)
  points(model_performance@x.values[[1]][max_gmean_ind] , model_performance@y.values[[1]][max_gmean_ind] , col = "red", pch = 19)
  text(model_performance@x.values[[1]][max_gmean_ind] , model_performance@y.values[[1]][max_gmean_ind], labels = round(optimal_ROC_threshold,3), col = "red", pos = 3)

   op <- par(cex = 0.8)  
  
  legend("bottomright", legend=c("Optimal threshold", "Logistic regression model", "No skill classifier"), col=c("red", "black", "gray"), pch = c(19, NA, NA), lty = c(0,1,4))
  
   ROC_plot <- recordPlot()
   plot.new()
  
  
  ROC_threshold_res <- list(optimal_threshold = optimal_ROC_threshold,
                            tpr = model_performance@y.values[[1]][max_gmean_ind],
                            fpr = model_performance@x.values[[1]][max_gmean_ind],
                            max_G_mean = max(gmeans, na.rm = T),
                            ROC_plot = ROC_plot )
  
  return(ROC_threshold_res)
  
}


# 5.1.2 Precision-recall  
optimal_threshold_PR <- function(model_performance){
  
  
  # 1. Calculating PR F-measure
  f_measure <- (2 * model_performance@y.values[[1]] * model_performance@x.values[[1]])/(model_performance@x.values[[1]] + model_performance@y.values[[1]])
  
  # 2. Find index of largest F-measure
  max_f_ind <- which(f_measure == max(f_measure, na.rm = T))
  
  # 3. Optimal threshold value
  
  optimal_threshold <- model_performance@alpha.values[[1]][max_f_ind]
  
  # 4. Plotting results
  plot(model_performance, colorize = FALSE, col = 'black', ylim = c(-0.05,1))
  abline(h=0, col="gray", lty = 4)
  points(model_performance@x.values[[1]][max_f_ind] , model_performance@y.values[[1]][max_f_ind] , col = "red", pch = 19)
  text(model_performance@x.values[[1]][max_f_ind] , model_performance@y.values[[1]][max_f_ind], labels = round(optimal_threshold,3), col = "red", pos = 3)
  
  op <- par(cex = 0.8) 
  
  legend("left",box.lty=0, inset = 0.02, legend=c("Optimal threshold", "Logistic regression model", "No skill classifier"), col=c("red", "black", "gray"), pch = c(19, NA, NA), lty = c(0,1,4))
  
  PR_plot <- recordPlot()
  plot.new()
  
  PR_threshold_res <- list(optimal_threshold = optimal_threshold,
                           recall = model_performance@y.values[[1]][max_f_ind],
                           precision = model_performance@x.values[[1]][max_f_ind],
                           max_f_measure = max(f_measure, na.rm = T),
                           PR_plot = PR_plot)
  
  return(PR_threshold_res)
  
}


# 5.2 Calculating performance of each model based on ROC curve and optimal threshold value

# 5.2.1  Network features

# standard LR
pred_std_network <- prediction(pi_hat_log_std_network, train_network_features$is_fraud)
perf_std_network <- performance(pred_std_network, "tpr", "fpr")


# LASSO LR
pred_lasso_network <- prediction(pi_hat_log_lasso_network, train_network_features$is_fraud)
perf_lasso_network <- performance(pred_lasso_network, "tpr", "fpr")


# 5.2.2 Transactional features

# standard LR
pred_std_trans <- prediction(pi_hat_log_std_trans, train_transactional_features$is_fraud)
perf_std_trans <- performance(pred_std_trans, "tpr", "fpr")

# LASSO LR
pred_lasso_trans <- prediction(pi_hat_log_lasso_trans, train_transactional_features$is_fraud)
perf_lasso_trans <- performance(pred_lasso_trans, "tpr", "fpr")


# 5.2.3 all features

# standard LR
pred_std_all <- prediction(pi_hat_log_std_all, train_all_features$is_fraud)
perf_std_all <- performance(pred_std_all, "tpr", "fpr")


# LASSO LR
pred_lasso_all <- prediction(pi_hat_log_lasso_all, train_all_features$is_fraud)
perf_lasso_all <- performance(pred_lasso_all, "tpr", "fpr")


# Calculating optimal threshold values according to ROC curves 

#  Network features
threshold_std_network <- optimal_threshold_ROC(perf_std_network)
threshold_lasso_network <- optimal_threshold_ROC(perf_lasso_network)

#  transaction features
threshold_std_trans <- optimal_threshold_ROC(perf_std_trans)
threshold_lasso_trans <- optimal_threshold_ROC(perf_lasso_trans)

# transaction features
threshold_std_all <- optimal_threshold_ROC(perf_std_all)
threshold_lasso_all <- optimal_threshold_ROC(perf_lasso_all)


# 5.3 Calculating performance of each model based on precision-recall curves and determining optimal threshold value

# Note: Unlike the ROC Curve, a precision-recall curve focuses on the performance of a classifier on the positive (minority class) only.

# 5.3.1  Network features

# standard LR
perf_std_network_pr <- performance(pred_std_network, "prec", "rec")

# LASSO LR

perf_lasso_network_pr <- performance(pred_lasso_network, "prec", "rec")

# 5.3.2 Transactional features

# standard LR
perf_std_trans_pr <- performance(pred_std_trans, "prec", "rec")

# LASSO LR
perf_lasso_trans_pr <- performance(pred_lasso_trans, "prec", "rec")

# 5.3.3 all features

# standard LR
perf_std_all_pr <- performance(pred_std_all, "prec", "rec")

# LASSO LR
perf_lasso_all_pr <- performance(pred_lasso_all, "prec", "rec")


# Calculating optimal threshold values according to PR curves 

#  Network features
threshold_std_network_pr <- optimal_threshold_PR(perf_std_network_pr)
threshold_lasso_network_pr <- optimal_threshold_PR(perf_lasso_network_pr)

#  transaction features
threshold_std_trans_pr <- optimal_threshold_PR(perf_std_trans_pr)
threshold_lasso_trans_pr <- optimal_threshold_PR(perf_lasso_trans_pr)

# transaction features
threshold_std_all_pr <- optimal_threshold_PR(perf_std_all_pr)
threshold_lasso_all_pr <- optimal_threshold_PR(perf_lasso_all_pr)

# 6. Calculating the models performance on training set

Y_train <- train_network_features$is_fraud

# 6.1 Network features

# standard LR
y_hat_log_std_network_ROC <- if_else(pi_hat_log_std_network >= threshold_std_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_network_PR <- if_else(pi_hat_log_std_network >= threshold_std_network_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_network_ROC <- if_else(pi_hat_log_lasso_network >= threshold_lasso_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_network_PR <- if_else(pi_hat_log_lasso_network >= threshold_lasso_network_pr$optimal_threshold, "TRUE", "FALSE" )

# 6.2 Transactional features

# standard LR
y_hat_log_std_trans_ROC <- if_else(pi_hat_log_std_trans >= threshold_std_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_trans_PR <- if_else(pi_hat_log_std_trans >= threshold_std_trans_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_trans_ROC <- if_else(pi_hat_log_lasso_trans >= threshold_lasso_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_trans_PR <- if_else(pi_hat_log_lasso_trans >= threshold_lasso_trans_pr$optimal_threshold, "TRUE", "FALSE" )


# 6.3 All features

# standard LR
y_hat_log_std_all_ROC <- if_else(pi_hat_log_std_all >= threshold_std_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_all_PR <- if_else(pi_hat_log_std_all >= threshold_std_all_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_all_ROC <- if_else(pi_hat_log_lasso_all >= threshold_lasso_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_all_PR <- if_else(pi_hat_log_lasso_all >= threshold_lasso_all_pr$optimal_threshold, "TRUE", "FALSE" )


# saving all confusion matrices of all the models in a list

log_train_results <- list(std_network_ROC = table(y_hat_log_std_network_ROC, Y_train),
                          std_network_PR = table(y_hat_log_std_network_PR, Y_train),
                          lasso_network_ROC = table(y_hat_log_lasso_network_ROC, Y_train),
                          lasso_network_PR = table(y_hat_log_lasso_network_PR, Y_train),
                          std_trans_ROC = table(y_hat_log_std_trans_ROC, Y_train),
                          std_trans_PR = table(y_hat_log_std_trans_PR, Y_train),
                          lasso_trans_ROC = table(y_hat_log_lasso_trans_ROC, Y_train),
                          lasso_trans_PR = table(y_hat_log_lasso_trans_PR, Y_train),
                          std_all_ROC = table(y_hat_log_std_all_ROC, Y_train),
                          std_all_PR = table(y_hat_log_std_all_PR, Y_train),
                          lasso_all_ROC = table(y_hat_log_lasso_all_ROC, Y_train),
                          lasso_all_PR = table(y_hat_log_lasso_all_PR, Y_train))

# defining function that calculates classification accuracy

clas_accuracy_func <- function(conf_matrix_list){
  
  # 1. Creating vector that will contain the classification accuracy 
  class_accur_res <- rep(NA, length(conf_matrix_list))
  
  # 2. Loop through to calculate classification accuracy
  for (i in 1:length(conf_matrix_list)) {
    
    current_confusion_mat <- conf_matrix_list[[i]]
    
    current_class_accuracy <- round((current_confusion_mat[1,1] + current_confusion_mat[2,2])/sum(current_confusion_mat),3)
    
    class_accur_res[i] <- current_class_accuracy
    
  }
  
  result_df <- data.frame(log_model = names(conf_matrix_list),
                          class_accur = class_accur_res,
                          lasso_aaplied = rep(c(F,F,T,T),3),
                          threshold_tuning = rep(c("ROC threshold-tuning","Precision-recall threshold-tuning"),6))
  
  return(result_df)
  
}

# calculating training result

log_train_res <- clas_accuracy_func(log_train_results)


# visualizing results

log_train_res %>% 
ggplot(aes(x = reorder(log_model, -class_accur), y = class_accur, group = 1, colour = lasso_aaplied)) +
  geom_point() +
  geom_line(colour = "gray") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(colour = "Lasso applied", x = "Logistic regression models", y = "Classification accuracy" ) +
  facet_wrap(threshold_tuning~.)
  

# 7. Testing models performance

# 7.1 Probability predictions with test data set

# 7.1.1 Network features

# Regular LR model
pi_hat_log_std_network_test <- predict(log_std_model_network, newdata = test_network_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_network_test <- predict(log_cv_model_network, 
                                    newx = as.matrix(test_network_features[,-c(ncol(test_network_features))]),
                                    s = log_cv_model_network$lambda.1se,
                                    type = "response")

# 7.1.2 Transactions features

# Regular LR model
pi_hat_log_std_trans_test <- predict(log_std_model_trans, newdata = test_transactional_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_trans_test <- predict(log_cv_model_trans, 
                                    newx = as.matrix(test_transactional_features[,-c(ncol(test_transactional_features))]),
                                    s = log_cv_model_trans$lambda.1se,
                                    type = "response")


# 7.1.3 all features
pi_hat_log_std_all_test <- predict(log_std_model_all, newdata = test_all_features, type = "response")

# LASSO LR model
pi_hat_log_lasso_all_test <- predict(log_cv_model_all, 
                                    newx = as.matrix(test_all_features[,-c(ncol(test_all_features))]),
                                    s = log_cv_model_all$lambda.1se,
                                    type = "response")

# 8. Actual test prediction given probabilities

Y_test <- test_network_features$is_fraud

# 8.1 Network features

# standard LR
y_hat_log_std_network_ROC_test <- if_else(pi_hat_log_std_network_test >= threshold_std_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_network_PR_test <- if_else(pi_hat_log_std_network_test >= threshold_std_network_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_network_ROC_test <- if_else(pi_hat_log_lasso_network_test >= threshold_lasso_network$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_network_PR_test <- if_else(pi_hat_log_lasso_network_test >= threshold_lasso_network_pr$optimal_threshold, "TRUE", "FALSE" )

# 8.2 Transactional features

# standard LR
y_hat_log_std_trans_ROC_test <- if_else(pi_hat_log_std_trans_test >= threshold_std_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_trans_PR_test <- if_else(pi_hat_log_std_trans_test >= threshold_std_trans_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_trans_ROC_test <- if_else(pi_hat_log_lasso_trans_test >= threshold_lasso_trans$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_trans_PR_test <- if_else(pi_hat_log_lasso_trans_test >= threshold_lasso_trans_pr$optimal_threshold, "TRUE", "FALSE" )


# 8.3 All features

# standard LR
y_hat_log_std_all_ROC_test <- if_else(pi_hat_log_std_all_test >= threshold_std_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_std_all_PR_test <- if_else(pi_hat_log_std_all_test >= threshold_std_all_pr$optimal_threshold, "TRUE", "FALSE" )

# LASSO LR
y_hat_log_lasso_all_ROC_test <- if_else(pi_hat_log_lasso_all_test >= threshold_lasso_all$optimal_threshold, "TRUE", "FALSE" )
y_hat_log_lasso_all_PR_test <- if_else(pi_hat_log_lasso_all_test >= threshold_lasso_all_pr$optimal_threshold, "TRUE", "FALSE" )


# 9. saving all confusion matrices of all the models in a list

log_test_results <- list(std_network_ROC = table(y_hat_log_std_network_ROC_test, Y_test),
                          std_network_PR = table(y_hat_log_std_network_PR_test, Y_test),
                          lasso_network_ROC = table(y_hat_log_lasso_network_ROC_test, Y_test),
                          lasso_network_PR = table(y_hat_log_lasso_network_PR_test, Y_test),
                          std_trans_ROC = table(y_hat_log_std_trans_ROC_test, Y_test),
                          std_trans_PR = table(y_hat_log_std_trans_PR_test, Y_test),
                          lasso_trans_ROC = table(y_hat_log_lasso_trans_ROC_test, Y_test),
                          lasso_trans_PR = table(y_hat_log_lasso_trans_PR_test, Y_test),
                          std_all_ROC = table(y_hat_log_std_all_ROC_test, Y_test),
                          std_all_PR = table(y_hat_log_std_all_PR_test, Y_test),
                          lasso_all_ROC = table(y_hat_log_lasso_all_ROC_test, Y_test),
                          lasso_all_PR = table(y_hat_log_lasso_all_PR_test, Y_test))

# calculating training result

log_test_res <- clas_accuracy_func(log_test_results)


# visualizing results

log_test_res %>% 
ggplot(aes(x = reorder(log_model, -class_accur), y = class_accur, group = 1, colour = lasso_aaplied)) +
  geom_point() +
  geom_line(colour = "gray") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(colour = "Lasso applied", x = "Logistic regression models", y = "Classification accuracy" ) +
  facet_wrap(threshold_tuning~.)

# 10. Saving result training and testing results in a single RData file

save(log_train_results, log_train_res, log_test_results, log_test_res, file = "LR_final_results.RData")


```

# 5.2 Neural network models

# 5.2.1 Data-pre-processing

```{r, data pre-processing -  neural network models - modelling}

# Note: The approach is to start simple and build up in complexity. For each NN model the batch size and number of epochs needs to be determined. 

# 1. pre-processing of testing and training data

# 1.1 Configuring training data 

train_target_nn <- train_network_features %>% 
  select(is_fraud) %>% 
  mutate(is_fraud = if_else(is_fraud == T, 1,0)) %>% 
  to_categorical()


# Note: The training target will be the same for all data sets (above we used the network feature data set but we could have either one of the other two).

# 1.1.1 removing client_ID, assigning a numeric value to response, and scaling features

# network features 
train_network_features_nn <- train_network_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# transaction features 

## omitting all columns that have NA values
train_transactional_features <- train_transactional_features[, colSums(is.na(train_transactional_features)) == 0]

train_transactional_features_nn <- train_transactional_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# All features

## omitting all columns that have NA values
train_all_features <- train_all_features[, colSums(is.na(train_all_features)) == 0]

train_all_features_nn <- train_all_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()


# 1.2 Configuring testing data

test_target_nn <- test_network_features %>% 
  select(is_fraud) %>% 
  mutate(is_fraud = if_else(is_fraud == T, 1,0)) %>% 
  to_categorical()


# 1.2.1 removing client_ID, assigning a numeric value to response, and scaling features

# network features
test_network_features_nn <- test_network_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# transaction features 

## omitting all columns that have NA values
test_transactional_features <- test_transactional_features[, colSums(is.na(test_transactional_features)) == 0]

test_transactional_features_nn <- test_transactional_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

# All features

## omitting all columns that have NA values
test_all_features <- test_all_features[, colSums(is.na(test_all_features)) == 0]

test_all_features_nn <- test_all_features %>% 
  select(-client_ID, -is_fraud) %>% 
  scale()

```

# 5.2.2 Feature/Predictor value selection (for evaluating response curves - metioned in section 5.2.4)

```{r, feature value selection (for eveluating response curves)}

# Purpose: To look which variables to fix for NN response curve generations.

# 1. Defining needed functions to extract response curve values for each data set

# function that calculates unique length of each feature
unique_length_func <- function(x){
  length(unique(x))
}

# function that classifies a feature as integer or numeric
feature_logic_func <- function(x){
  
  total_rows <- length(x)
  if(sum(x %% 1 == 0) == total_rows){
    outcome <- "integer"
  }else{
    outcome <- "numeric"
  }
  
  return(outcome)
  
}


value_selection_assist_func <- function(train_df, train_df_scaled){
  
  # Note: train_df is the scaled train df that should also contain only predictors and the training response. Look at the following template:
  
# 1. Constructing summary df

# 1.1 Calculating feature unique length
features_length <- apply(train_df, 2, unique_length_func)

# 1.2 Calculating if a feature is a numeric or an integer
feature_logic <- apply(train_df, 2, feature_logic_func)

numeric_integer_summary_df <- data.frame(feature_names = names(features_length)) %>% 
  mutate(unique_instances = features_length) %>% 
  mutate(feature_logic = feature_logic) %>% 
  mutate(selection_method = if_else(feature_logic == "numeric", "Median", "Majority rule"))

# 2. Numeric features

# 2.1 Selecting feature names
numeric_feature_names <- numeric_integer_summary_df %>% 
  filter(feature_logic == "numeric") %>% 
  select(feature_names)

numeric_feature_names <- numeric_feature_names[["feature_names"]]

# 2.2 Selecting numeric features and calculating values
numeric_train_df <- train_df_scaled[numeric_feature_names]

numeric_features_selected_values <- apply(numeric_train_df, MARGIN = 2, median) %>% 
  as.data.frame() %>% 
  rename(value = ".")

# 3. Integer features

# 3.1 Extracting interger names
integer_feature_names <- numeric_integer_summary_df %>% 
  filter(feature_logic == "integer") %>% 
  select(feature_names)

integer_feature_names <- integer_feature_names[["feature_names"]]

dis_feature_names <- integer_feature_names[integer_feature_names != "is_fraud"]

# 3.2 Calculating aggregating stats for each featrue
if(length(dis_feature_names) != 0){

  ## defining global variable and storage
  total_rows <- nrow(train_df_scaled)
  final_df <- data.frame(index_num = 1:10)

  for (i in 1:length(dis_feature_names)) {
    
  ## generating counting stats for current feature
    dis_feature <- dis_feature_names[i]
    
    current_table <- train_df_scaled %>% 
      select(!!sym(dis_feature), is_fraud) %>% 
      group_by(!!sym(dis_feature)) %>% 
      summarise(count = n()) %>%
      mutate(perc = 100*count/total_rows) %>% 
      arrange(desc(perc)) %>% 
      head(10)
    
    current_table <- as.data.frame(current_table) %>% 
      mutate(index_num = 1:nrow(current_table))
    
  ## renaming columns
    names(current_table) <- c(dis_feature_names[i], paste0("count - ",dis_feature_names[i]), paste0("perc - ",dis_feature_names[i]), "index_num")
    
    
    final_df <- left_join(final_df, current_table, by = "index_num")
    
  }

integer_features_selected_values <- final_df

}else{
  
  integer_features_selected_values <- "No integers"
  
}


# 4. Saving result

res <- list(feature_summary_df = numeric_integer_summary_df,
            numeric_features_df = numeric_features_selected_values,
            integer_features_df = integer_features_selected_values)

return(res)

}

# 2. Selecting values for response curves 

# 2.1 Network data
train_network_features_nn_df <- as.data.frame(train_network_features_nn) %>% 
  mutate(is_fraud = train_network_features$is_fraud)

network_selected_values_list <-  value_selection_assist_func(train_df_scaled = train_network_features_nn_df,
                                                             train_df = train_network_features[,-1])


# 2.2 Transaction data
train_trans_features_nn_df <- as.data.frame(train_transactional_features_nn) %>% 
  mutate(is_fraud = train_transactional_features$is_fraud)

trans_selected_values_list <-  value_selection_assist_func(train_df_scaled = train_trans_features_nn_df,
                                                           train_df = train_transactional_features[,-1])

# 2.3 Combined data
train_all_features_nn_df <- as.data.frame(train_all_features_nn) %>% 
  mutate(is_fraud = train_all_features$is_fraud)

all_selected_values_list <-  value_selection_assist_func(train_df_scaled = train_all_features_nn_df,
                                                           train_df = train_all_features[,-1])


# 3. exporting some result to excell

# write.xlsx(network_selected_values_list$integer_features_df, file = "network_intergers.xlsx")
# write.xlsx(trans_selected_values_list$integer_features_df, file = "trans_intergers.xlsx")
# write.xlsx(all_selected_values_list$integer_features_df, file = "all_intergers.xlsx")


```

5.2.2 Hyper-parameter tuning - validation curves generation

Note: Our approach is to fix the architecture of the neural network model (having a certain complexity threshold) and to apply regularization as a mechanism to control the complexity (such that the model generalizes well).

```{r, validation curves generation - Hyper-paramater tuning}

# 1. Defining neural network models

# 1.1 Neural network model 1 (simple model)

# Hidden units: 8
# Batch size: 100
# Epochs: 23
# Optimizer and learning rate: 0.001
# Activation function (hidden layer): relu
# Activation function (final layer): softmax
# Loss function: categorical crossentropy 
# L2 regularization

lambda_vec <- exp(seq(-5.5,-2,length = 15)) # Notice the exp - efficient way to define a optimal set of values

nn_1 <- tuning_run("NN_1.R",
                   flags = list(lambda = lambda_vec)) 


nn_1 <- ls_runs(runs_dir = 'nn_1_focused')

nn_1 <- ls_runs(runs_dir = 'nn_1_high_level')


# 1.1.1 Plotting validation curve

## NB! Plot below will only work afer you have defined best lambda value

## High level lambda values plot

nn_1_df <- data.frame(set = c(rep("Train", nrow(nn_1)), rep("Validation", nrow(nn_1))),
                      error = c(nn_1$metric_loss, nn_1$metric_val_loss),
                      lambda = c(nn_1$flag_lambda, nn_1$flag_lambda)
                      ) 

hl_nn_1 <- nn_1_df %>% 
ggplot(aes(x = lambda, y = error, colour = set, fill = set)) +  
  geom_point() + 
  geom_line() +
  labs(x= expression(lambda), y = "Error (categorical crossentropy)") + 
  scale_color_manual(values=c("black", "blue")) + 
  geom_vline(xintercept = min_lambda, linetype = "dashed", col = "red" ) +
  theme_bw() +
  theme(legend.position = "top",
        legend.title = element_blank())

## Focused lambda values plot

min_lambda <- nn_1$flag_lambda[which(nn_1$metric_val_loss == min(nn_1$metric_val_loss))]

nn_1_df <- data.frame(lambda = nn_1$flag_lambda,
                   val_loss = nn_1$metric_val_loss,
                   train_loss = nn_1$metric_loss)

focus_nn_1 <- nn_1_df %>% 
  ggplot(aes(x = lambda)) +  
  geom_point(aes(y = val_loss, color = "Validation")) + 
  geom_line(aes(y = val_loss, color = "Validation")) +
  geom_point(aes(y = train_loss, color = "Train")) + 
  geom_line(aes(y = train_loss, color = "Train")) + 
  geom_smooth(aes(y = val_loss, color = "Validation smooth"), se =F) +
  geom_smooth(aes(y = train_loss, color = "Train smooth"), se =F) + 
  geom_vline(aes(xintercept = min_lambda), color = "red", linetype = "dashed"  ) + 
  scale_color_manual(breaks = c("Validation", "Train", "Validation smooth", "Train smooth"),
                     values = c("Validation" = "blue", "Train" = "black", "Validation smooth" = "lightblue" , "Train smooth" = "gray") ) +
  geom_text(aes(x = min_lambda + 0.002, y = max(val_loss) , label = paste('lambda', '==', round(min_lambda, 4))), parse = T, col = "red") +
  labs(x = expression(lambda), y = "Error (categorical crossentropy)") + 
  theme_bw() + 
  theme(legend.title = element_blank(),
        legend.position = "top")

grid.arrange(arrangeGrob(hl_nn_1, focus_nn_1, nrow = 1, ncol = 2))

# 1.2 Neural network model 2 (More complex model)

# Hidden units: 32
# Batch size: 100
# Epochs: 23
# Optimizer and learning rate: 0.001
# Activation function (hidden layer): relu
# Activation function (final layer): softmax
# Loss function: categorical crossentropy 
# L2 regularization

lambda_vec <- exp(seq(-5,-3,length = 15)) # Notice the exp - efficient way to define a optimal set of values

nn_2 <- tuning_run("NN_2.R",
                   flags = list(lambda = lambda_vec)) 


nn_2 <- ls_runs(runs_dir = 'nn_2_high_level')

nn_2 <- ls_runs(runs_dir = 'nn_2_focused')

# 1.2.1 Plotting validation curve

## NB! Plot below will only work after you have defined best lambda value

## High level lambda values plot

nn_2_df <- data.frame(set = c(rep("Train", nrow(nn_2)), rep("Validation", nrow(nn_2))),
                      error = c(nn_2$metric_loss, nn_2$metric_val_loss),
                      lambda = c(nn_2$flag_lambda, nn_2$flag_lambda)
                      ) 

hl_nn_2 <- nn_2_df %>% 
ggplot(aes(x = lambda, y = error, colour = set, fill = set)) +  
  geom_point() + 
  geom_line() +
  labs(x= expression(lambda), y = "Error (categorical crossentropy)") + 
  scale_color_manual(values=c("blue", "black")) + 
  geom_vline(xintercept = min_lambda, linetype = "dashed", col = "red" ) +
  theme_bw() + 
  theme(legend.position = "top",
        legend.title = element_blank())

## Focused lambda values plot

min_lambda <- nn_2$flag_lambda[which(nn_2$metric_val_loss == min(nn_2$metric_val_loss))]

nn_2_df <- data.frame(lambda = nn_2$flag_lambda,
                   val_loss = nn_2$metric_val_loss,
                   train_loss = nn_2$metric_loss)

focus_nn_2 <- nn_2_df %>% 
  ggplot(aes(x = lambda)) +  
  geom_point(aes(y = val_loss, color = "Validation")) + 
  geom_line(aes(y = val_loss, color = "Validation")) +
  geom_point(aes(y = train_loss, color = "Train")) + 
  geom_line(aes(y = train_loss, color = "Train")) + 
  geom_smooth(aes(y = val_loss, color = "Validation smooth"), se =F) +
  geom_smooth(aes(y = train_loss, color = "Train smooth"), se =F) + 
  geom_vline(aes(xintercept = min_lambda), color = "red", linetype = "dashed"  ) + 
  scale_color_manual(breaks = c("Validation", "Train", "Validation smooth", "Train smooth"),
                     values = c("Validation" = "blue", "Train" = "black", "Validation smooth" = "lightblue" , "Train smooth" = "gray") ) +
  geom_text(aes(x = min_lambda+0.0005, y = max(val_loss) , label = paste('lambda', '==', round(min_lambda, 4))), parse = T, col = "red") +
  labs(x = expression(lambda), y = "Error (categorical crossentropy)") + 
  theme_bw() + 
  theme(legend.title = element_blank(),
        legend.position = "top")

grid.arrange(arrangeGrob(hl_nn_2, focus_nn_2, nrow = 1, ncol = 2))


# 1.3 Comparison between model 1 and model 2 architectures

# Note: This piece of code will be re-run to generate plots for network, transactional, and all features data sets

nn_1 <- ls_runs(runs_dir = 'nn_1_high_level' ) 
nn_2 <- ls_runs(runs_dir = 'nn_2_high_level')

data_combined <- data.frame(val_loss_mod_1 = nn_1$metric_val_loss,
                            val_loss_mod_2 = nn_2$metric_val_loss,
                            lambda = nn_1$flag_lambda)

# high level plot
plot_HL <- data_combined %>% 
  ggplot(aes(x = lambda)) +  
  geom_point(aes(y = val_loss_mod_1, color = "(8)-Network")) + 
  geom_line(aes(y = val_loss_mod_1, color = "(8)-Network")) +
  geom_point(aes(y = val_loss_mod_2, color = "(64)-Network")) + 
  geom_line(aes(y = val_loss_mod_2, color = "(64)-Network")) + 
  geom_smooth(aes(y = val_loss_mod_1, color = "(8)-Network smooth"), se =F) +
  geom_smooth(aes(y = val_loss_mod_2, color = "(64)-Network smooth"), se =F) + 
  scale_color_manual(breaks = c("(8)-Network", "(64)-Network", "(8)-Network smooth", "(64)-Network smooth"),
                     values = c("(8)-Network" = "red", "(64)-Network" = "blue", "(8)-Network smooth" = "lightcoral" , "(64)-Network smooth" = "lightblue") ) +
  labs(x = expression(lambda), y = "Validation error (categorical crossentropy)") + 
  theme_bw() + 
  theme(legend.position = "top",
        legend.title =element_blank() )


# focused plot
min_lambda_1 <- nn_1$flag_lambda[which(nn_1$metric_val_loss == min(nn_1$metric_val_loss))]
min_lambda_2 <- nn_2$flag_lambda[which(nn_2$metric_val_loss == min(nn_2$metric_val_loss))]

data_combined <- data.frame(lambda = c(nn_1$flag_lambda, nn_2$flag_lambda),
                            model = c(rep("Model 1",nrow(nn_1)), rep("(64)-Network",nrow(nn_2))),
                            val_loss = c(nn_1$metric_val_loss, nn_2$metric_val_loss) 
)


plot_focus <-  data_combined %>% 
  ggplot(aes(x = lambda, y = val_loss, color = model)) +
  geom_point() + 
  geom_line() +
  #geom_smooth(area = "transparent") +
  #geom_vline(xintercept = min_lambda_1, col = "red", linetype = "dashed") + 
  #geom_vline(xintercept = min_lambda_2, col = "blue", linetype = "dashed") + 
  theme_bw() + 
  scale_color_manual(values = c("Model 1" = "red", "Model 2" = "blue")) + 
  labs(x = expression(lambda), y = "Validation error (categorical crossentropy)") +
  coord_cartesian(xlim = c(0.0007,0.0062)) + 
  #geom_text(aes(x = min_lambda_1 + 0.005, y = 0.105 , label = paste('lambda', '==', round(min_lambda_1, 4))), parse = T, col = "red") +
  #geom_text(aes(x = min_lambda_2 + 0.005, y = 0.105 , label = paste('lambda', '==', round(min_lambda_2, 4))), parse = T, col = "blue") + 
  theme(legend.position = "none")


grid.arrange(arrangeGrob(plot_HL, plot_focus, nrow = 1, ncol = 2))


```

5.2.3 Model construction and testing - Error metric extraction runs

```{r, Model construction and testing - Neural network models - modelling }

# 1. Defining training and testing function for neural network and defining functions that will help evaluate the test performance of models

# 1.1 F1 score
f1_score <- function(precision, recall){
  
  f1_score <- (2*precision*recall)/(precision + recall)
  
  return(f1_score)
  
}

# 1.2 Balanced accuracy
balanced_accuracy <- function(table){
  
  TP <-  table[2,2]
  P <- sum(table[,2])
  
  TN <- table[1,1]
  N <- sum(table[,1])
  
  
  balanced_accuracy <- (TP/P + TN/N)/2
  
  return(balanced_accuracy)
  
}

# 1.3 Neural network train/test function (fixed architecture of simple model)

# Note: From the hyper-parameter tuning process, it was decided select the simple model (model 1) for all the data sets. It was because its validation error was very close to the more complex model (model 2). 

NN_test_func <- function(train_x, train_y, test_x , test_y, orig_test_df, units, min_lambda, runs){
  
  runs <- runs # number of times the model will be re-evaluated
  
  result_df <- data.frame(runs = rep(NA,runs),
                          train_acc = rep(NA,runs),
                          train_prec = rep(NA,runs), 
                          train_recall = rep(NA,runs),
                          train_auc = rep(NA,runs),
                          test_acc = rep(NA,runs),
                          test_prec = rep(NA,runs), 
                          test_recall = rep(NA,runs),
                          test_auc = rep(NA,runs),
                          test_balance_acc = rep(NA,runs),
                          test_f1_score = rep(NA, runs)
  )
  
  # defining list to save confusion matrices
  conf_mat_result <- list()
  
  
  for (i in 1:runs) {
    
    print(paste0("Starting run ", i))
    
    # define model
    model <- keras_model_sequential()
    
    model %>% 
      layer_dense(units = units, 
                  activation = 'relu', 
                  input_shape = c(dim(train_x)[2]),
                  kernel_regularizer = regularizer_l2(l = min_lambda)) %>%
      layer_dense(units = 2, activation = 'softmax')
    
    # Compile model
    model %>% compile(loss = 'categorical_crossentropy',
                      optimizer = 'adam',
                      metrics = list('accuracy', metric_precision(), metric_recall(), metric_auc()))
    
    # Fit model
    history <- model %>% 
      fit(train_x,
          train_y,
          epochs = 23,
          batch_size = 100,
          validation_split = 0.3)
    
    # saving training results
    epochs <- history$params$epochs
    result_df[i,"runs"] <- i
    result_df[i,"train_acc"] <- history$metrics$acc[epochs]
    result_df[i,"train_prec"] <- history$metrics$precision[epochs]
    result_df[i,"train_recall"] <- history$metrics$recall[epochs]
    result_df[i,"train_auc"] <- history$metrics$auc[epochs]
    
    
    # Evaluate model's performance on Test data
    test_result <- model %>% evaluate(test_x, test_y)
    
    # saving test results
    
    result_df[i,"test_acc"] <- test_result$acc
    result_df[i,"test_prec"] <- test_result$precision
    result_df[i,"test_recall"] <- test_result$recall
    result_df[i,"test_auc"] <- test_result$auc
    
    # display confusion matrix
    Y_test_hat <- predict_classes(model, test_x)
    conf_mat<-  table(Y_test_hat, orig_test_df$is_fraud)
    
    # saving final test results (extracted from confusion matrix)
    result_df[i,"test_balance_acc"] <- balanced_accuracy(conf_mat)
    result_df[i,"test_f1_score"] <-  f1_score(test_result$precision,test_result$recall)
    conf_mat_result[[i]] <- conf_mat
    
    # removing variables created
    k_clear_session()
    rm(model)
    
  }
  
  test_results <- list(result_df = result_df,
                       confusion_mat = conf_mat_result)
  
  
  return(test_results)
  
  
}

# 2. Defining final neural network model

# 1.1 Network data model

# extracting best lambda

nn_net <- ls_runs(runs_dir = "nn_1_focused_net")

min_lambda_net <- nn_net$flag_lambda[which(nn_net$metric_val_loss == min(nn_net$metric_val_loss))]

# training/testing neural net model
nn_network_res <- NN_test_func(train_x = train_network_features_nn,
                               train_y = train_target_nn,
                               test_x = test_network_features_nn,
                               test_y = test_target_nn,
                               orig_test_df = test_network_features,
                               units = 8,
                               min_lambda = min_lambda_net,
                               runs = 10)

res_network_df <- nn_network_res$result_df
mean(res_network_df$train_acc)
mean(res_network_df$test_acc)


# 1.2 Transactional data model

nn_trans <- ls_runs(runs_dir = "nn_1_focused_trans")

min_lambda_trans <- nn_trans$flag_lambda[which(nn_trans$metric_val_loss == min(nn_trans$metric_val_loss))]

# training/testing neural net model
nn_trans_res <- NN_test_func(train_x = train_transactional_features_nn,
                               train_y = train_target_nn,
                               test_x = test_transactional_features_nn,
                               test_y = test_target_nn,
                               orig_test_df = test_transactional_features,
                               units = 8,
                               min_lambda = min_lambda_trans,
                               runs = 10)

res_trans_df <- nn_trans_res$result_df
mean(res_trans_df$train_acc)
mean(res_trans_df$test_acc)


# 1.3 All features data model

nn_all <- ls_runs(runs_dir = "nn_1_focused_all")

min_lambda_all <- nn_all$flag_lambda[which(nn_all$metric_val_loss == min(nn_all$metric_val_loss))]

# training/testing neural net model
nn_all_res <- NN_test_func(train_x = train_all_features_nn,
                               train_y = train_target_nn,
                               test_x = test_all_features_nn,
                               test_y = test_target_nn,
                               orig_test_df = test_all_features,
                               units = 8,
                               min_lambda = min_lambda_all,
                               runs = 10)

res_all_df <- nn_all_res$result_df
mean(res_all_df$train_acc)
mean(res_all_df$test_acc)

 
 # 2. Saving all results
 
save(nn_trans_res, nn_network_res, nn_all_res, file = "NN_final_results.RData")
 

# test

f_1 <- f1_score(precision = test$test_prec, recall = test$test_recall)  

test <- nn_network_res$result_df


nn_network_res$result_df$test_f1_score[1]
nn_network_res$confusion_mat[[1]]

# confusion matrix

recall = 68/(68+4)
precision = 68/(68+12)
f_1 <- f1_score(precision = precision, recall = recall)  


nn_network_res$result_df$test_f1_score[1]

```


5.2.4 Model construction and testing - Response curves generation

I) Network feature data set

```{r, model construction and testing - response curve generation - Network model}

# Note: From the hyper-parameter tuning process, it was decided select the simple model (model 1) for all the data sets. It was because its validation error was very close to the more complex model (model 2). 

# freeing memory
rm(list=ls()[! ls() %in% c("train_all_features_nn","train_network_features_nn", "train_transactional_features_nn", "train_target_nn", "network_selected_values_list", "trans_selected_values_list", "all_selected_values_list")])

# 1. Constructing models with "optimal" hyper parameters and generating response curves

# 1.1 Network data model

# 1.1.1 Extracting best lambda (from hyper-parameter tuning process)

nn_net <- ls_runs(runs_dir = "nn_1_focused_net")
min_lambda_net <- nn_net$flag_lambda[which(nn_net$metric_val_loss == min(nn_net$metric_val_loss))]

# 1.1.2 Defining Keras model

train_x <- train_network_features_nn
train_y <- train_target_nn
units <- 8
min_lambda <- min_lambda_net
  
model <- keras_model_sequential()

model %>% 
  layer_dense(units = units, 
              activation = 'relu', 
              input_shape = c(dim(train_x)[2]),
              kernel_regularizer = regularizer_l2(l = min_lambda)) %>%
  layer_dense(units = 2, activation = 'softmax')

# Compile model
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = list('accuracy', metric_precision(), metric_recall(), metric_auc()))

# Fit model
history <- model %>% 
  fit(train_x,
      train_y,
      epochs = 23,
      batch_size = 100,
      validation_split = 0.3)

net_model <- model


# 1.1.3 Constructing response curves

# Note: The process of extracting the response curves involves the generation of three data frames:
# 1. A data frame that contains the equally spaced values for each predictor (from min to max) - "equally spaced df"
# 2. A data frame that contains the repetitions of the equally spaced predictors (repetitions each M times) - "fixed df"
# 3. A data frame of fixed chosen values (there are several of theses based of the chosen values)

## Creating first df ("equally spaced" df)

M <- 100 # M*M will be the row size of the equally spaced or repetition df's
col_names <- colnames(train_x)
eq_spaced_df <- data.frame(eq_space_number = 1:M)


for (j in 1:dim(train_x)[2]) {
  
  current_pred <- rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), M)
  eq_spaced_df <- cbind(eq_spaced_df, current_pred )
  
}

eq_spaced_df_final <- eq_spaced_df[,-1]
names(eq_spaced_df_final) <- col_names

## Creating second df ("repetition" df) repetition 

repitition_df <- data.frame(repitition_number = 1:M)

for (j in 1:dim(train_x)[2]) {
  
  current_pred <-  rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), each = M)
  repitition_df <- cbind(repitition_df, current_pred )
  
}

repitition_df_final <- repitition_df[,-1]
names(repitition_df_final) <- col_names

## Creating third set of df's

# Note: This is multiple df's. One of theses df represent fixed values chosen for each feature if we are plotting two variables on the x and y axis.

# 1. Network data (NB! Going to have to do for trans and combined features) - doing some data wrangling to get the data in the required form

# Loading df's
integer_df <- network_selected_values_list$integer_features_df
numeric_df <- network_selected_values_list$numeric_features_df

# 1.1 Numeric data

# transposing df
numeric_df_t <- as.data.frame(t(numeric_df))

# duplicating rows and saving onto data frame
numeric_values <- numeric_df_t[1,]
numeric_final_rep_df <- numeric_df_t

numeric_final_rep_df <- numeric_final_rep_df[rep(1, times = M*M),]

# renaming row names
row.names(numeric_final_rep_df) <- 1:(M*M)

# 1.2 Integer data (NB!!! Hard coding was done)

# extracting original order of names
integer_col_names <- integer_df %>% 
  select(-contains("count -"),
         -contains("perc -"),
         -contains("index")) %>% 
  names()

# naming single and two value variables
two_value_variables <- c("total_degree", "non_fraud_degree", "out_degree", "out_degree_non_fraud") # this is user selected (based of percentages)
single_value_variables <- integer_col_names[!integer_col_names %in% two_value_variables]
  
# 1.2.1 Creating single value df
sigle_value_integer_df <- integer_df[names(integer_df) %in% single_value_variables] %>% 
  head(1)

sigle_value_integer_df <- sigle_value_integer_df[rep(1, times = M*M),]

row.names(sigle_value_integer_df) <- 1:(M*M)

# 1.2.2 Creating two value df
two_value_integer_df <- integer_df[names(integer_df) %in% two_value_variables] %>% 
  head(2)

# creating data frame to construct all possible combinations of two value features
total_degree <- two_value_integer_df[,1]
non_fraud_degree <- two_value_integer_df[,2]
out_degree <- two_value_integer_df[,3]
out_degree_non_fraud <- two_value_integer_df[,4]
obj <- data.frame(total_degree = total_degree, 
                  non_fraud_degree = non_fraud_degree,
                  out_degree = out_degree,
                  out_degree_non_fraud = out_degree_non_fraud)

integer_combinations_df <- unique(expand.grid(obj))

# 1.3 Constructing fixed value sets

final_rep_df_list <- list()
col_names_order <- names(as.data.frame(train_x))

for (i in 1:nrow(integer_combinations_df)) {
  
  # configuring integer set (2 value)
  current_integer_set <- integer_combinations_df[rep(i, times = M*M),]
  row.names(current_integer_set) <- 1:(M*M)
  
  # joining numeric set, one value integers, and two value integers
  final_current_rep_set <- cbind(current_integer_set, numeric_final_rep_df, sigle_value_integer_df)
  
  # changing column order names
  final_current_rep_set <- final_current_rep_set[col_names_order]
  
  # saving results
  final_rep_df_list[[i]] <- final_current_rep_set
  
}

# Note: We will run through each data set that is in final rep df list and generate the response curves.


### RESPONSE CURVE GENERATION ####

## legend:
# r  is the counter for which data set variant is currently used.
# i is the counter for which predictor is currenly the independent variable (on x-axis).
# j is the counter for for the predictor which is the dependent variable (on y-axis).
# k is the counter which will ensure we do not generate a full matrix but only upper matrix.
# p is the counter for the diagnostic df index

# Start of big outer loop:

start <- Sys.time()

# global variable(s)

# final diagnostics result table
diagnostic_df_final <- data.frame()

# short version of column names

col_names_short <- col_names
col_names_short[which(col_names == "probabilistic_relational_neighbour_fraud")] <- "prob_rel_neigh_fraud"
col_names_short[which(col_names == "probabilistic_relational_neighbour_not_fraud")] <- "prob_rel_neigh_not_fraud"
col_names_short[which(col_names == "relational_neighbour_fraud")] <- "relation_neigh"
col_names_short[which(col_names == "relational_neighbour_not_fraud")] <- "relation_neigh_not_fraud"

for(r in 1:length(final_rep_df_list)){
  
  
# 1. Creating fourth df, performing prediction and generating curves.

# 1.1 Defining fixed data frame to us for current iteration (will later loop through each of these)
fixed_values_df <- final_rep_df_list[[r]]

# 1.2 Defining nested for loops to create the needed prediction df and generate its curves



# 1.2.1 Defining fixed variables before entering the loops

# defining limit for plot  
b <- c(0,0.5,1)

# defining diagnostic df (i.e. will give a summary of all the plotted results)
diagnostic_df <- data_frame(x_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            y_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            relationship = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            decision_boundry_strength = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            permutation_number = rep(NA,dim(train_x)[2]*dim(train_x)[2]))

# defining counters
k <- 0
p <- 0

# resetting df
  prediction_df <- fixed_values_df # fixed data set values

# entering first loop (independent variable loop)
for(i in 1:dim(train_x)[2]){
  
  prediction_df[,i] <- eq_spaced_df_final[,i] # replacing independent predictor column
  
  # entering second loop (dependent variable loop)
  for(j in (k+1):dim(train_x)[2]){
    
    current_prediction_df <- prediction_df # resetting df (for independent values)
    
    current_prediction_df[,j] <- repitition_df_final[,j] # replacing dependent predictor column
    
    # making predictions with the model
    pred <-  data.frame(predict(net_model, as.matrix(current_prediction_df))) %>% 
      rename(non_fraud = X1,
             fraud = X2) 
    
    # inserting if condition for diagnostic df
    
    p <- p+1
    
    if(sum(pred$fraud > 0.7) > 0 & sum(pred$non_fraud > 0.7)){ # condition when there is evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "Fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
      # generating plots (based on decision boundry strength)
      
      if(diagnostic_df$decision_boundry_strength[p] > 0.95) {

      plot_df <- data_frame(x=current_prediction_df[,i],
                          y=current_prediction_df[,j],
                          prob_fraud=pred$fraud )

    current_plot <-  plot_df %>%
      ggplot(aes(x=x, y=y)) +
      geom_point(aes(colour = prob_fraud))  +
      scale_colour_gradientn(limits = c(0,1),
                             colours=c("blue", "white", "red"),
                             breaks=b, labels=format(b)) +
      labs(x = col_names_short[i], y = col_names_short[j])

    ggsave(filename = paste0("SET_", r ,"_", col_names_short[j], "_VS_", col_names_short[i],".pdf"),
           plot = current_plot,
           path = "Network response curves plots 09_06_2022",
           device = "pdf")

      }
    
    }else{ # condition when there no evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "No fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
    } # end if statement condition
    
  } # end independent variable loop
  
  # updating k to ensure only the upper matrix values are calculated
  k <- k+1
  
} # end dependent variable loop
  
  
  # removing all NA from diagnostic df
  diagnostic_df <- na.omit(diagnostic_df)
  
  diagnostic_df_final <- rbind(diagnostic_df_final, diagnostic_df)
  
  

} # end big outer loop
  end <- Sys.time()
  
## Analysing diagnostic df
  
  # summary of x and y predictors
  
  diagnostic_df_final_plots <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(x_axis_predictor,y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    mutate(predictors = paste0(y_axis_predictor, " VS " , x_axis_predictor)) %>% 
    arrange(desc(count))
  
  # analysis of x-predictors
  
  x_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(x_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # analysis of y-predictors
  
  y_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # plot of x and y predictors
  x_predictors_unique$x_axis_predictor <- factor(x_predictors_unique$x_axis_predictor ) %>% 
    fct_reorder(x_predictors_unique$perc_of_plots)
  
  x <- x_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = x_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  y_predictors_unique$y_axis_predictor <- factor(y_predictors_unique$y_axis_predictor) %>% 
    fct_reorder(y_predictors_unique$perc_of_plots)
  
 y <-y_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = y_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  
  ggarrange(y, x, ncol = 1, nrow = 2)


```

II) Transactional

```{r, model construction and testing - response curve generation - Transactional model}

# Note: From the hyper-parameter tuning process, it was decided select the simple model (model 1) for all the data sets. It was because its validation error was very close to the more complex model (model 2). 

# freeing memory
rm(list=ls()[! ls() %in% c("train_all_features_nn","train_network_features_nn", "train_transactional_features_nn", "train_target_nn", "network_selected_values_list", "trans_selected_values_list", "all_selected_values_list")])

# 1. Constructing models with "optimal" hyper parameters and generating response curves

# 1.1 Network data model

# 1.1.1 Extracting best lambda (from hyper-parameter tuning process)

nn_trans <- ls_runs(runs_dir = "nn_1_focused_trans")
min_lambda_trans <- nn_trans$flag_lambda[which(nn_trans$metric_val_loss == min(nn_trans$metric_val_loss))]

# 1.1.2 Defining Keras model

train_x <- train_transactional_features_nn
train_y <- train_target_nn
units <- 8
min_lambda <- min_lambda_trans
  
model <- keras_model_sequential()

model %>% 
  layer_dense(units = units, 
              activation = 'relu', 
              input_shape = c(dim(train_x)[2]),
              kernel_regularizer = regularizer_l2(l = min_lambda)) %>%
  layer_dense(units = 2, activation = 'softmax')

# Compile model
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = list('accuracy', metric_precision(), metric_recall(), metric_auc()))

# Fit model
history <- model %>% 
  fit(train_x,
      train_y,
      epochs = 23,
      batch_size = 100,
      validation_split = 0.3)

trans_model <- model


# 1.1.3 Constructing response curves

# Note: The process of extracting the response curves involves the generation of three data frames:
# 1. A data frame that contains the equally spaced values for each predictor (from min to max) - "equally spaced df"
# 2. A data frame that contains the repetitions of the equally spaced predictors (repetitions each M times) - "fixed df"
# 3. A data frame of fixed chosen values (there are several of theses based of the chosen values)

## Creating first df ("equally spaced" df)

M <- 100 # M*M will be the row size of the equally spaced or repetition df's
col_names <- colnames(train_x)
eq_spaced_df <- data.frame(eq_space_number = 1:M)


for (j in 1:dim(train_x)[2]) {
  
  current_pred <- rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), M)
  eq_spaced_df <- cbind(eq_spaced_df, current_pred )
  
}

eq_spaced_df_final <- eq_spaced_df[,-1]
names(eq_spaced_df_final) <- col_names

## Creating second df ("repetition" df) repetition 

repitition_df <- data.frame(repitition_number = 1:M)

for (j in 1:dim(train_x)[2]) {
  
  current_pred <-  rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), each = M)
  repitition_df <- cbind(repitition_df, current_pred )
  
}

repitition_df_final <- repitition_df[,-1]
names(repitition_df_final) <- col_names

## Creating third set of df's

# Note: This is multiple df's. One of theses df represent fixed values chosen for each feature if we are plotting two variables on the x and y axis.

# 1. Transactional data - doing some data wrangling to get the data in the required form

# Loading df's
integer_df <- trans_selected_values_list$integer_features_df
numeric_df <- trans_selected_values_list$numeric_features_df

# 1.1 Numeric data

# transposing df
numeric_df_t <- as.data.frame(t(numeric_df))

# duplicating rows and saving onto data frame
numeric_values <- numeric_df_t[1,]
numeric_final_rep_df <- numeric_df_t

numeric_final_rep_df <- numeric_final_rep_df[rep(1, times = M*M),]

# renaming row names
row.names(numeric_final_rep_df) <- 1:(M*M)

# 1.2 Integer data (NB!!! Hard coding was done)

# extracting original order of names
integer_col_names <- integer_df %>% 
  select(-contains("count -"),
         -contains("perc -"),
         -contains("index")) %>% 
  names()

# naming single and two value variables
two_value_variables <- c("receive_period_max", "receive_period_min", "payment_period_max", "payment_period_min") # this is user selected (based of percentages)

single_value_variables <- integer_col_names[!integer_col_names %in% two_value_variables]
  
# 1.2.1 Creating single value df
sigle_value_integer_df <- integer_df[names(integer_df) %in% single_value_variables] %>% 
  head(1)

sigle_value_integer_df <- sigle_value_integer_df[rep(1, times = M*M),]

row.names(sigle_value_integer_df) <- 1:(M*M)

# 1.2.2 Creating two value df
two_value_integer_df <- integer_df[names(integer_df) %in% two_value_variables] %>% 
  head(2)

# creating data frame to construct all possible combinations of two value features
receive_period_max <- two_value_integer_df[,1]
receive_period_min <- two_value_integer_df[,2]
payment_period_max <- two_value_integer_df[,3]
payment_period_min <- two_value_integer_df[,4]
obj <- data.frame(receive_period_max = receive_period_max, 
                  receive_period_min = receive_period_min,
                  payment_period_max = payment_period_max,
                  payment_period_min = payment_period_min)

integer_combinations_df <- unique(expand.grid(obj))

# 1.3 Constructing fixed value sets

final_rep_df_list <- list()
col_names_order <- names(as.data.frame(train_x))

for (i in 1:nrow(integer_combinations_df)) {
  
  # configuring integer set (2 value)
  current_integer_set <- integer_combinations_df[rep(i, times = M*M),]
  row.names(current_integer_set) <- 1:(M*M)
  
  # joining numeric set, one value integers, and two value integers
  final_current_rep_set <- cbind(current_integer_set, numeric_final_rep_df, sigle_value_integer_df)
  
  # changing column order names
  final_current_rep_set <- final_current_rep_set[col_names_order]
  
  # saving results
  final_rep_df_list[[i]] <- final_current_rep_set
  
}

# Note: We will run through each data set that is in final rep df list and generate the response curves.

### RESPONSE CURVE GENERATION ####

## legend:
# r  is the counter for which data set variant is currently used.
# i is the counter for which predictor is currenly the independent variable (on x-axis).
# j is the counter for for the predictor which is the dependent variable (on y-axis).
# k is the counter which will ensure we do not generate a full matrix but only upper matrix.
# p is the counter for the diagnostic df index

# Start of big outer loop:

start <- Sys.time()

# global variable(s)

# final diagnostics result table
diagnostic_df_final <- data.frame()

# short version of column names

col_names_short <- col_names
col_names_short[which(col_names == "incoming_round_numbers_count")] <- "inc_round_num_count"
col_names_short[which(col_names == "outgoing_round_numbers_count")] <- "out_round_num_count"

#length(final_rep_df_list)

for(r in 1:length(final_rep_df_list)){
  
  
# 1. Creating fourth df, performing prediction and generating curves.

# 1.1 Defining fixed data frame to us for current iteration (will later loop through each of these)
fixed_values_df <- final_rep_df_list[[r]]

# 1.2 Defining nested for loops to create the needed prediction df and generate its curves

# 1.2.1 Defining fixed variables before entering the loops

# defining limit for plot  
b <- c(0,0.5,1)

# defining diagnostic df (i.e. will give a summary of all the plotted results)
diagnostic_df <- data_frame(x_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            y_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            relationship = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            decision_boundry_strength = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            permutation_number = rep(NA,dim(train_x)[2]*dim(train_x)[2]))

# defining counters
k <- 0
p <- 0

# resetting df
  prediction_df <- fixed_values_df # fixed data set values

# entering first loop (independent variable loop)
for(i in 1:dim(train_x)[2]){
  
  prediction_df[,i] <- eq_spaced_df_final[,i] # replacing independent predictor column
  
  # entering second loop (dependent variable loop)
  for(j in (k+1):dim(train_x)[2]){
    
    current_prediction_df <- prediction_df # resetting df (for independent values)
    
    current_prediction_df[,j] <- repitition_df_final[,j] # replacing dependent predictor column
    
    # making predictions with the model
    pred <-  data.frame(predict(trans_model, as.matrix(current_prediction_df))) %>% 
      rename(non_fraud = X1,
             fraud = X2) 
    
    # inserting if condition for diagnostic df
    
    p <- p+1
    
    if(sum(pred$fraud > 0.7) > 0 & sum(pred$non_fraud > 0.7)){ # condition when there is evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "Fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
      # generating plots (based on decision boundry strength)
      
      if(diagnostic_df$decision_boundry_strength[p] > 0.99998) {

      plot_df <- data_frame(x=current_prediction_df[,i],
                          y=current_prediction_df[,j],
                          prob_fraud=pred$fraud )

    current_plot <-  plot_df %>%
      ggplot(aes(x=x, y=y)) +
      geom_point(aes(colour = prob_fraud))  +
      scale_colour_gradientn(limits = c(0,1),
                             colours=c("blue", "white", "red"),
                             breaks=b, labels=format(b)) +
      labs(x = col_names_short[i], y = col_names_short[j])

    ggsave(filename = paste0("SET_", r ,"_", col_names_short[j], "_VS_", col_names_short[i],".pdf"),
           plot = current_plot,
           path = "Trans response curves plots 09_06_2022",
           device = "pdf")

      }
    
    }else{ # condition when there no evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "No fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
    } # end if statement condition
    
  } # end independent variable loop
  
  # updating k to ensure only the upper matrix values are calculated
  k <- k+1
  
} # end dependent variable loop
  
  
  # removing all NA from diagnostic df
  diagnostic_df <- na.omit(diagnostic_df)
  
  diagnostic_df_final <- rbind(diagnostic_df_final, diagnostic_df)
  
  

} # end big outer loop
  end <- Sys.time()
  
## Analyzing diagnostic df
  
  # summary of x and y predictors
  
  diagnostic_df_final_plots <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.99998) %>% 
    group_by(x_axis_predictor,y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    mutate(predictors = paste0(y_axis_predictor, " VS " , x_axis_predictor)) %>% 
    arrange(desc(count))
  
  # analysis of x-predictors
  
  x_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(x_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # analysis of y-predictors
  
  y_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # plot of x and y predictors
  x_predictors_unique$x_axis_predictor <- factor(x_predictors_unique$x_axis_predictor ) %>% 
    fct_reorder(x_predictors_unique$perc_of_plots)
  
  x <- x_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = x_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  y_predictors_unique$y_axis_predictor <- factor(y_predictors_unique$y_axis_predictor) %>% 
    fct_reorder(y_predictors_unique$perc_of_plots)
  
 y <-y_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = y_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  
  ggarrange(y, x, ncol = 1, nrow = 2)

```

II) Combined

```{r, model construction and testing - response curve generation - Combined model}

# Note: From the hyper-parameter tuning process, it was decided select the simple model (model 1) for all the data sets. It was because its validation error was very close to the more complex model (model 2). 

# freeing memory
rm(list=ls()[! ls() %in% c("train_all_features_nn","train_network_features_nn", "train_transactional_features_nn", "train_target_nn", "network_selected_values_list", "trans_selected_values_list", "all_selected_values_list")])

# 1. Constructing models with "optimal" hyper parameters and generating response curves

# 1.1 Network data model

# 1.1.1 Extracting best lambda (from hyper-parameter tuning process)

nn_all <- ls_runs(runs_dir = "nn_1_focused_all")
min_lambda_all <- nn_all$flag_lambda[which(nn_all$metric_val_loss == min(nn_all$metric_val_loss))]

# 1.1.2 Defining Keras model

train_x <- train_all_features_nn
train_y <- train_target_nn
units <- 8
min_lambda <- min_lambda_all
  
model <- keras_model_sequential()

model %>% 
  layer_dense(units = units, 
              activation = 'relu', 
              input_shape = c(dim(train_x)[2]),
              kernel_regularizer = regularizer_l2(l = min_lambda)) %>%
  layer_dense(units = 2, activation = 'softmax')

# Compile model
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = 'adam',
                  metrics = list('accuracy', metric_precision(), metric_recall(), metric_auc()))

# Fit model
history <- model %>% 
  fit(train_x,
      train_y,
      epochs = 23,
      batch_size = 100,
      validation_split = 0.3)

combined_model <- model


# 1.1.3 Constructing response curves

# Note: The process of extracting the response curves involves the generation of three data frames:
# 1. A data frame that contains the equally spaced values for each predictor (from min to max) - "equally spaced df"
# 2. A data frame that contains the repetitions of the equally spaced predictors (repetitions each M times) - "fixed df"
# 3. A data frame of fixed chosen values (there are several of theses based of the chosen values)

## Creating first df ("equally spaced" df)

M <- 100 # M*M will be the row size of the equally spaced or repetition df's
col_names <- colnames(train_x)
eq_spaced_df <- data.frame(eq_space_number = 1:M)


for (j in 1:dim(train_x)[2]) {
  
  current_pred <- rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), M)
  eq_spaced_df <- cbind(eq_spaced_df, current_pred )
  
}

eq_spaced_df_final <- eq_spaced_df[,-1]
names(eq_spaced_df_final) <- col_names

## Creating second df ("repetition" df) repetition 

repitition_df <- data.frame(repitition_number = 1:M)

for (j in 1:dim(train_x)[2]) {
  
  current_pred <-  rep(seq(min(train_x[,j]), max(train_x[,j]), length = M), each = M)
  repitition_df <- cbind(repitition_df, current_pred )
  
}

repitition_df_final <- repitition_df[,-1]
names(repitition_df_final) <- col_names

## Creating third set of df's

# Note: This is multiple df's. One of theses df represent fixed values chosen for each feature if we are plotting two variables on the x and y axis.

# 1. Transactional data - doing some data wrangling to get the data in the required form

# Loading df's
integer_df <- all_selected_values_list$integer_features_df
numeric_df <- all_selected_values_list$numeric_features_df

# 1.1 Numeric data

# transposing df
numeric_df_t <- as.data.frame(t(numeric_df))

# duplicating rows and saving onto data frame
numeric_values <- numeric_df_t[1,]
numeric_final_rep_df <- numeric_df_t

numeric_final_rep_df <- numeric_final_rep_df[rep(1, times = M*M),]

# renaming row names
row.names(numeric_final_rep_df) <- 1:(M*M)

# 1.2 Integer data (NB!!! Hard coding was done)

# extracting original order of names
integer_col_names <- integer_df %>% 
  select(-contains("count -"),
         -contains("perc -"),
         -contains("index")) %>% 
  names()

# naming single and two value variables
two_value_variables <- c("total_degree", "non_fraud_degree", "out_degree", "out_degree_non_fraud") # this is user selected (based of percentages)

single_value_variables <- integer_col_names[!integer_col_names %in% two_value_variables]
  
# 1.2.1 Creating single value df
sigle_value_integer_df <- integer_df[names(integer_df) %in% single_value_variables] %>% 
  head(1)

sigle_value_integer_df <- sigle_value_integer_df[rep(1, times = M*M),]

row.names(sigle_value_integer_df) <- 1:(M*M)

# 1.2.2 Creating two value df
two_value_integer_df <- integer_df[names(integer_df) %in% two_value_variables] %>% 
  head(2)

# creating data frame to construct all possible combinations of two value features
total_degree <- two_value_integer_df[,1]
non_fraud_degree <- two_value_integer_df[,2]
out_degree <- two_value_integer_df[,3]
out_degree_non_fraud <- two_value_integer_df[,4]
obj <- data.frame(total_degree = total_degree, 
                  non_fraud_degree = non_fraud_degree,
                  out_degree = out_degree,
                  out_degree_non_fraud = out_degree_non_fraud)

integer_combinations_df <- unique(expand.grid(obj))

# 1.3 Constructing fixed value sets

final_rep_df_list <- list()
col_names_order <- names(as.data.frame(train_x))

for (i in 1:nrow(integer_combinations_df)) {
  
  # configuring integer set (2 value)
  current_integer_set <- integer_combinations_df[rep(i, times = M*M),]
  row.names(current_integer_set) <- 1:(M*M)
  
  # joining numeric set, one value integers, and two value integers
  final_current_rep_set <- cbind(current_integer_set, numeric_final_rep_df, sigle_value_integer_df)
  
  # changing column order names
  final_current_rep_set <- final_current_rep_set[col_names_order]
  
  # saving results
  final_rep_df_list[[i]] <- final_current_rep_set
  
}

# Note: We will run through each data set that is in final rep df list and generate the response curves.

### RESPONSE CURVE GENERATION ####

## legend:
# r  is the counter for which data set variant is currently used.
# i is the counter for which predictor is currenly the independent variable (on x-axis).
# j is the counter for for the predictor which is the dependent variable (on y-axis).
# k is the counter which will ensure we do not generate a full matrix but only upper matrix.
# p is the counter for the diagnostic df index

# Start of big outer loop:

start <- Sys.time()

# global variable(s)

# final diagnostics result table
diagnostic_df_final <- data.frame()

# short version of column names

col_names_short <- col_names
col_names_short[which(col_names == "probabilistic_relational_neighbour_fraud")] <- "prob_rel_neigh_fraud"
col_names_short[which(col_names == "probabilistic_relational_neighbour_not_fraud")] <- "prob_rel_neigh_not_fraud"
col_names_short[which(col_names == "relational_neighbour_fraud")] <- "relation_neigh"
col_names_short[which(col_names == "relational_neighbour_not_fraud")] <- "relation_neigh_not_fraud"
col_names_short[which(col_names == "incoming_round_numbers_count")] <- "inc_round_num_count"
col_names_short[which(col_names == "outgoing_round_numbers_count")] <- "out_round_num_count"

#length(final_rep_df_list)

for(r in 1:length(final_rep_df_list)){
  
  
# 1. Creating fourth df, performing prediction and generating curves.

# 1.1 Defining fixed data frame to us for current iteration (will later loop through each of these)
fixed_values_df <- final_rep_df_list[[r]]

# 1.2 Defining nested for loops to create the needed prediction df and generate its curves

# 1.2.1 Defining fixed variables before entering the loops

# defining limit for plot  
b <- c(0,0.5,1)

# defining diagnostic df (i.e. will give a summary of all the plotted results)
diagnostic_df <- data_frame(x_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            y_axis_predictor = rep(NA, dim(train_x)[2]*dim(train_x)[2]),
                            relationship = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            decision_boundry_strength = rep(NA,dim(train_x)[2]*dim(train_x)[2]),
                            permutation_number = rep(NA,dim(train_x)[2]*dim(train_x)[2]))

# defining counters
k <- 0
p <- 0

# resetting df
  prediction_df <- fixed_values_df # fixed data set values

# entering first loop (independent variable loop)
for(i in 1:dim(train_x)[2]){
  
  prediction_df[,i] <- eq_spaced_df_final[,i] # replacing independent predictor column
  
  # entering second loop (dependent variable loop)
  for(j in (k+1):dim(train_x)[2]){
    
    current_prediction_df <- prediction_df # resetting df (for independent values)
    
    current_prediction_df[,j] <- repitition_df_final[,j] # replacing dependent predictor column
    
    # making predictions with the model
    pred <-  data.frame(predict(combined_model, as.matrix(current_prediction_df))) %>% 
      rename(non_fraud = X1,
             fraud = X2) 
    
    # inserting if condition for diagnostic df
    
    p <- p+1
    
    if(sum(pred$fraud > 0.7) > 0 & sum(pred$non_fraud > 0.7)){ # condition when there is evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "Fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
      # generating plots (based on decision boundry strength)
      
      if(diagnostic_df$decision_boundry_strength[p] > 0.988) {
      
      plot_df <- data_frame(x=current_prediction_df[,i],
                          y=current_prediction_df[,j],
                          prob_fraud=pred$fraud )

    current_plot <-  plot_df %>%
      ggplot(aes(x=x, y=y)) +
      geom_point(aes(colour = prob_fraud))  +
      scale_colour_gradientn(limits = c(0,1),
                             colours=c("blue", "white", "red"),
                             breaks=b, labels=format(b)) +
      labs(x = col_names_short[i], y = col_names_short[j])

    ggsave(filename = paste0("SET_", r ,"_", col_names_short[j], "_VS_", col_names_short[i],".pdf"),
           plot = current_plot,
           path = "Combined response curves plots 09_06_2022",
           device = "pdf")
        
      }
    
    }else{ # condition when there no evidence of variables being related to fraud
      
      # saving variables in diagnostic df
      diagnostic_df$x_axis_predictor[p] <- col_names_short[i]
      diagnostic_df$y_axis_predictor[p] <- col_names_short[j]
      diagnostic_df$relationship[p] <- "No fraud relationship"
      diagnostic_df$decision_boundry_strength[p] <- (max(pred$non_fraud) + max(pred$fraud))/2
      diagnostic_df$permutation_number[p] <- r
      
    } # end if statement condition
    
  } # end independent variable loop
  
  # updating k to ensure only the upper matrix values are calculated
  k <- k+1
  
} # end dependent variable loop
  
  
  # removing all NA from diagnostic df
  diagnostic_df <- na.omit(diagnostic_df)
  
  diagnostic_df_final <- rbind(diagnostic_df_final, diagnostic_df)
  
  

} # end big outer loop
  end <- Sys.time()
  
## Analyzing diagnostic df
  
  # summary of x and y predictors
  
  diagnostic_df_final_plots <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.988) %>% 
    group_by(x_axis_predictor,y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    mutate(predictors = paste0(y_axis_predictor, " VS " , x_axis_predictor)) %>% 
    arrange(desc(count))
  
  # analysis of x-predictors
  
  x_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(x_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # analysis of y-predictors
  
  y_predictors_unique <- diagnostic_df_final %>% 
    filter(decision_boundry_strength> 0.95) %>% 
    group_by(y_axis_predictor) %>% 
    summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    mutate(perc_of_plots = count/sum(count)*100)
  
  # plot of x and y predictors
  x_predictors_unique$x_axis_predictor <- factor(x_predictors_unique$x_axis_predictor ) %>% 
    fct_reorder(x_predictors_unique$perc_of_plots)
  
  x <- x_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = x_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  y_predictors_unique$y_axis_predictor <- factor(y_predictors_unique$y_axis_predictor) %>% 
    fct_reorder(y_predictors_unique$perc_of_plots)
  
 y <-y_predictors_unique %>% 
    ggplot(aes(y = perc_of_plots, x = y_axis_predictor)) + 
    geom_col() +
    coord_flip()
  
  
  ggarrange(y, x, ncol = 1, nrow = 2)

```


# 6. Results interpretation

```{r, results interpretation}

# 1. Loading final results data of LR and NN models

load("LR_final_results.RData")
load("NN_final_results.RData")

# 2.  Calculating f1_score and balanced accuracy for logistic regression models

# 2.1 Defining function to calculate performance metrics

LR_perf_metrics_func <- function(conf_matrix){
  
  # 1. Calculating f1 score
  
  # 1.1 calculating recall
  table <- conf_matrix
  
  TP <- table[2,2]
  FN <- table[1,2]
  recall <- TP/(TP+FN)
  
  # 1.2  calculating precision
  
  FP <- table[2,1]
  precision <- TP/(TP+FP)
  
  f1_score <- (2*precision*recall)/(precision + recall)
  
  # 2. Calculating balanced accuracy
  P <- sum(table[,2])
  TN <- table[1,1]
  N <- sum(table[,1])
  balanced_accuracy <- (TP/P + TN/N)/2
  
  result <- list(f1_score = f1_score,
                 balanced_acc = balanced_accuracy)
  
  return(result)
}



NN_fix_func <- function(conf_matrix){
  
  # 1. Calculating f1 score
  
  # 1.1 calculating recall
  table <- conf_matrix
  
  TP <- table[2,2]
  FN <- table[1,2]
  recall <- TP/(TP+FN)
  
  # 1.2  calculating precision
  
  FP <- table[2,1]
  precision <- TP/(TP+FP)
  
  f1_score <- (2*precision*recall)/(precision + recall)
  
  # 2. Calculating balanced accuracy
  P <- sum(table[,2])
  TN <- table[1,1]
  N <- sum(table[,1])
  balanced_accuracy <- (TP/P + TN/N)/2
  
  result <- list(f1_score = f1_score,
                 balanced_acc = balanced_accuracy,
                 recall = recall,
                 precision = precision)
  
  return(result)
  }


# 2.2 Training set calculations

log_train_f1_score <- c()
log_train_balanced_acc <- c()

for (i in 1:length(log_train_results)) {
  
  current_conf_mat <- log_train_results[[i]]
  
  train_result <- LR_perf_metrics_func(current_conf_mat) 
  
  log_train_f1_score <- c(log_train_f1_score, train_result$f1_score)
  
  log_train_balanced_acc <- c(log_train_balanced_acc, train_result$balanced_acc)
  
}

log_train_res <- log_train_res %>% 
  mutate(f1_score = log_train_f1_score) %>% 
  mutate(balance_acc = log_train_balanced_acc)


# 2.3 Testing set calculations

log_test_f1_score <- c()
log_test_balanced_acc <- c()

for (i in 1:length(log_test_results)) {
  
  current_conf_mat <- log_test_results[[i]]
  
  test_result <- LR_perf_metrics_func(current_conf_mat) 
  
  log_test_f1_score <- c(log_test_f1_score, test_result$f1_score)
  
  log_test_balanced_acc <- c(log_test_balanced_acc, test_result$balanced_acc)
  
}

log_test_res <- log_test_res %>% 
  mutate(f1_score = log_test_f1_score) %>% 
  mutate(balance_acc = log_test_balanced_acc)


# 2.4 Fixing test precision, test recall and balanced accuracy in NN models

# 2.4.1 network data 

nn_test_f1_score <- c()
nn_test_balanced_acc <- c()
nn_test_recall <- c()
nn_test_precision <- c()

for (i in 1:length(nn_network_res$confusion_mat)) {
  
  current_conf_mat <- nn_network_res$confusion_mat[[i]]
  
  test_result <- NN_fix_func(current_conf_mat) 
  
  nn_test_f1_score <- c(nn_test_f1_score, test_result$f1_score)
  
  nn_test_balanced_acc <- c(nn_test_balanced_acc, test_result$balanced_acc)
  
  nn_test_recall <- c(nn_test_recall, test_result$recall)
  
  nn_test_precision <- c(nn_test_precision, test_result$precision)
  
}

nn_network_res$result_df <- nn_network_res$result_df %>% 
  mutate(test_f1_score = nn_test_f1_score) %>% 
  mutate(test_balance_acc = nn_test_balanced_acc) %>% 
  mutate(test_prec = nn_test_precision) %>% 
  mutate(test_recall = nn_test_recall)

# 2.4.2 Transactional data 

nn_test_f1_score <- c()
nn_test_balanced_acc <- c()
nn_test_recall <- c()
nn_test_precision <- c()

for (i in 1:length(nn_trans_res$confusion_mat)) {
  
  current_conf_mat <- nn_trans_res$confusion_mat[[i]]
  
  test_result <- NN_fix_func(current_conf_mat) 
  
  nn_test_f1_score <- c(nn_test_f1_score, test_result$f1_score)
  
  nn_test_balanced_acc <- c(nn_test_balanced_acc, test_result$balanced_acc)
  
  nn_test_recall <- c(nn_test_recall, test_result$recall)
  
  nn_test_precision <- c(nn_test_precision, test_result$precision)
  
}

nn_trans_res$result_df <- nn_trans_res$result_df %>% 
  mutate(test_f1_score = nn_test_f1_score) %>% 
  mutate(test_balance_acc = nn_test_balanced_acc) %>% 
  mutate(test_prec = nn_test_precision) %>% 
  mutate(test_recall = nn_test_recall)

# 2.4.3 Combined data 

nn_test_f1_score <- c()
nn_test_balanced_acc <- c()
nn_test_recall <- c()
nn_test_precision <- c()

for (i in 1:length(nn_all_res$confusion_mat)) {
  
  current_conf_mat <- nn_all_res$confusion_mat[[i]]
  
  test_result <- NN_fix_func(current_conf_mat) 
  
  nn_test_f1_score <- c(nn_test_f1_score, test_result$f1_score)
  
  nn_test_balanced_acc <- c(nn_test_balanced_acc, test_result$balanced_acc)
  
  nn_test_recall <- c(nn_test_recall, test_result$recall)
  
  nn_test_precision <- c(nn_test_precision, test_result$precision)
  
}

nn_all_res$result_df <- nn_all_res$result_df %>% 
  mutate(test_f1_score = nn_test_f1_score) %>% 
  mutate(test_balance_acc = nn_test_balanced_acc) %>% 
  mutate(test_prec = nn_test_precision) %>% 
  mutate(test_recall = nn_test_recall)

# 3. Creating data frame

results_df <- data.frame(data_set = c(rep("network",3),rep("transactional",3), rep("combined",3)),
                         model = rep(c("LR_PR","LR_ROC", "NN_8"),3),
                         train_acc = c(log_train_res$class_accur[log_train_res$log_model == "lasso_network_PR"],
                                       log_train_res$class_accur[log_train_res$log_model == "lasso_network_ROC"],
                                       mean(nn_network_res$result_df$train_acc),
                                       log_train_res$class_accur[log_train_res$log_model == "lasso_trans_PR"],
                                       log_train_res$class_accur[log_train_res$log_model == "lasso_trans_ROC"],
                                       mean(nn_trans_res$result_df$train_acc),
                                       log_train_res$class_accur[log_train_res$log_model == "lasso_all_PR"],
                                       log_train_res$class_accur[log_train_res$log_model == "lasso_all_ROC"],
                                       mean(nn_all_res$result_df$train_acc)),
                         test_acc = c(log_test_res$class_accur[log_test_res$log_model == "lasso_network_PR"],
                                       log_test_res$class_accur[log_test_res$log_model == "lasso_network_ROC"],
                                       mean(nn_network_res$result_df$test_acc),
                                       log_test_res$class_accur[log_test_res$log_model == "lasso_trans_PR"],
                                       log_test_res$class_accur[log_test_res$log_model == "lasso_trans_ROC"],
                                       mean(nn_trans_res$result_df$test_acc),
                                       log_test_res$class_accur[log_test_res$log_model == "lasso_all_PR"],
                                       log_test_res$class_accur[log_test_res$log_model == "lasso_all_ROC"],
                                       mean(nn_all_res$result_df$test_acc)),
                         train_f1_score = c(log_train_res$f1_score[log_train_res$log_model == "lasso_network_PR"],
                                       log_train_res$f1_score[log_train_res$log_model == "lasso_network_ROC"],
                                       mean(nn_network_res$result_df$train_f1_score),
                                       log_train_res$f1_score[log_train_res$log_model == "lasso_trans_PR"],
                                       log_train_res$f1_score[log_train_res$log_model == "lasso_trans_ROC"],
                                       mean(nn_trans_res$result_df$train_f1_score),
                                       log_train_res$f1_score[log_train_res$log_model == "lasso_all_PR"],
                                       log_train_res$f1_score[log_train_res$log_model == "lasso_all_ROC"],
                                       mean(nn_all_res$result_df$train_f1_score)),
                         test_f1_score = c(log_test_res$f1_score[log_test_res$log_model == "lasso_network_PR"],
                                       log_test_res$f1_score[log_test_res$log_model == "lasso_network_ROC"],
                                       mean(nn_network_res$result_df$test_f1_score),
                                       log_test_res$f1_score[log_test_res$log_model == "lasso_trans_PR"],
                                       log_test_res$f1_score[log_test_res$log_model == "lasso_trans_ROC"],
                                       mean(nn_trans_res$result_df$test_f1_score),
                                       log_test_res$f1_score[log_test_res$log_model == "lasso_all_PR"],
                                       log_test_res$f1_score[log_test_res$log_model == "lasso_all_ROC"],
                                       mean(nn_all_res$result_df$test_f1_score)),
                         test_balanced_acc = c(log_test_res$balance_acc[log_test_res$log_model == "lasso_network_PR"],
                                       log_test_res$balance_acc[log_test_res$log_model == "lasso_network_ROC"],
                                       mean(nn_network_res$result_df$test_balance_acc),
                                       log_test_res$balance_acc[log_test_res$log_model == "lasso_trans_PR"],
                                       log_test_res$balance_acc[log_test_res$log_model == "lasso_trans_ROC"],
                                       mean(nn_trans_res$result_df$test_balance_acc),
                                       log_test_res$balance_acc[log_test_res$log_model == "lasso_all_PR"],
                                       log_test_res$balance_acc[log_test_res$log_model == "lasso_all_ROC"],
                                       mean(nn_all_res$result_df$test_balance_acc))
                         
                         )

# test accuracy
results_df$test_acc[results_df$model == "NN_8" & results_df$data_set == "network"] <- 0.9861
results_df$test_acc[results_df$model == "NN_8" & results_df$data_set == "transactional"] <- 0.9351
results_df$test_acc[results_df$model == "NN_8" & results_df$data_set == "combined"] <- 0.9881

# test f1
results_df$test_f1_score[results_df$model == "NN_8" & results_df$data_set == "network"] <- 0.8851
results_df$test_f1_score[results_df$model == "NN_8" & results_df$data_set == "transactional"] <- 0.3611
results_df$test_f1_score[results_df$model == "NN_8" & results_df$data_set == "combined"] <- 0.8921

# test balanced accuracy
results_df$test_balanced_acc[results_df$model == "NN_8" & results_df$data_set == "network"] <- 0.9731
results_df$test_balanced_acc[results_df$model == "NN_8" & results_df$data_set == "transactional"] <- 0.6521
results_df$test_balanced_acc[results_df$model == "NN_8" & results_df$data_set == "combined"] <- 0.9541


# 4. Visualisations

# Plotting final results

# distribution plot data sets - F1 score
final_res_box <- results_df %>% 
  ggplot(aes(x = test_f1_score, fill = data_set)) +
  labs(x = " ") +
  theme_bw() +
  scale_fill_brewer(name = "Data set", labels = c("Combined", "Network", "Transactional"), palette = "Dark2") + 
  geom_boxplot() + 
theme(axis.text.y = element_blank())

final_res_dense <- results_df %>% 
  ggplot(aes(x = test_f1_score, fill =data_set)) +
  geom_density(alpha = 0.4) +  
  scale_fill_brewer(name = "Data set", labels = c("Combined", "Network", "Transactional"),  palette = "Dark2") + 
  labs(y = "Density", x = "F1-score") +
  theme_bw()

grid.arrange(arrangeGrob(final_res_box, final_res_dense, nrow = 2, ncol = 1))


# distribution plot data sets - balanced accuracy
final_res_box <- results_df %>% 
  ggplot(aes(x = test_balanced_acc, fill = data_set)) +
  labs(x = " ") +
  theme_bw() +
  scale_fill_brewer(name = "Data set", labels = c("Combined", "Network", "Transactional"), palette = "Dark2") + 
  geom_boxplot() + 
theme(axis.text.y = element_blank())

final_res_dense <- results_df %>% 
  ggplot(aes(x = test_balanced_acc, fill =data_set)) +
  geom_density(alpha = 0.4) +  
  scale_fill_brewer(name = "Data set", labels = c("Combined", "Network", "Transactional"),  palette = "Dark2") + 
  labs(y = "Density", x = "Balanced accuracy") +
  theme_bw()

grid.arrange(arrangeGrob(final_res_box, final_res_dense, nrow = 2, ncol = 1))

 
 # facet grid of datasets
 
bar_test_acc <- results_df %>% 
  ggplot(aes(x = data_set, y = test_acc,  fill = model)) +
  geom_bar(position = "dodge", stat = "identity")  +
  geom_hline(aes(yintercept = 0.945), color = "red", linetype = "dashed", lwd = 0.5) + 
  coord_cartesian(ylim = c(0.85,1)) +
  scale_fill_brewer(palette = "Dark2", name = " ", labels = c("Logistic regression (PR)", "Logistic regression (ROC)", "(8)-Network") ) +
  geom_text(aes(label = round(test_acc,3)), position = position_dodge(width = 0.9), vjust = -0.1, size = 3) + 
  theme_bw() + 
  theme(legend.position = "top",
        text = element_text(size = 10)) +
  labs(x = "Dataset", y = "Test accuracy")

bar_test_f1 <- results_df %>% 
  ggplot(aes(x = data_set, y = test_f1_score,  fill = model)) +
  geom_bar(position = "dodge", stat = "identity")  +
  coord_cartesian(ylim = c(0.3,1)) + 
  scale_fill_brewer(palette = "Dark2", name = " ", labels = c("Logistic regression (PR)", "Logistic regression (ROC)", "(8)-Network") ) +
  geom_text(aes(label = round(test_f1_score,3)), position = position_dodge(width = 0.9), vjust = -0.1, size = 3) + 
  theme_bw() + 
  theme(legend.position = "top",
        text = element_text(size = 10)) +
  labs(x = "Dataset", y = "Test F1-score")

bar_test_balanced_acc <- results_df %>% 
  ggplot(aes(x = data_set, y = test_balanced_acc,  fill = model)) +
  geom_bar(position = "dodge", stat = "identity")  +
  coord_cartesian(ylim = c(0.6,1)) +
  scale_fill_brewer(palette = "Dark2", name = " ", labels = c("Logistic regression (PR)", "Logistic regression (ROC)", "(8)-Network") ) +
  geom_text(aes(label = round(test_balanced_acc,3)), position = position_dodge(width = 0.9), vjust = -0.1, size = 2.5) + 
  theme_bw() + 
  theme(legend.position = "top",
        text = element_text(size = 10)) +
  labs(x = "Dataset", y = "Test balanced accuracy")



grid.arrange(arrangeGrob(bar_test_f1, bar_test_balanced_acc, nrow = 2, ncol = 1))

ggarrange(
  bar_test_f1,     
  ggarrange(bar_test_balanced_acc,bar_test_acc, ncol = 2 ), 
  nrow = 2) 
   
```



```{r, unsused code}

# 1. Network features

#### TEMP ####

# creating data frame - maybe usefull for later

# network_data_type <- c("cont", "discr", "discr", "discr", "cont", "cont", "cont", "cont", "cont", "cont", "discr", "discr", "discr", "discr", "cont", "cont", "cont", "cont", "cont", "cont", "cont", "discr", "discr", "discr", "discr", "discr", "discr") 
# 
# network_feature_names <- names(train_network_features_nn_df)
# 
# network_features_df <- data.frame(feature_name = network_feature_names,
#                                   data_type = network_data_type)

#### TEMP ####

#### TEMP - Manual implimenation ####

# 1. Seeing how many unique values is in each feature

train_network_features_nn_df <- as.data.frame(train_network_features_nn) %>% 
  mutate(is_fraud = train_network_features$is_fraud)

unique_length_func <- function(x){
  length(unique(x))
}

short_length_net_feat <- apply(train_network_features_nn_df, 2, unique_length_func)
short_length_net_feat <- short_length_net_feat[short_length_net_feat<50]

# Note: Due to large number of discrete variable choices, we need to fix some values which we will look at when generating response curves. 

# 2. Fixing values for discrete values with high number of "class" values 

# Note: For all discrete values we need to see which had the highest fraud count.

## transitivity
net_trans <- train_network_features_nn_df %>% 
  select(transitivity, is_fraud) %>% 
  group_by(transitivity, is_fraud) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  head(10)
 
net_trans %>% 
  ggplot(aes(x = transitivity, y = count, fill = is_fraud)) +
  geom_col() +
  scale_y_log10()

# Note: -0.09393306 (accounted for 98.5% of the data)

## total degree
net_total_degree <- train_network_features_nn_df %>% 
  select(total_degree, is_fraud) %>% 
  group_by(total_degree, is_fraud) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  head(10)
 
net_total_degree %>% 
  ggplot(aes(x = total_degree, y = count, fill = is_fraud)) +
  geom_col() +
  scale_y_log10()

# Note: -0.18732548, -0.06853996, and, 0.05024557 (accounted for 90% of the data)

## # fraud degree
net_fraud_degree <- train_network_features_nn_df %>% 
  select(fraud_degree, is_fraud) %>% 
  group_by(fraud_degree, is_fraud) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  head(10)
 
net_total_degree %>% 
  ggplot(aes(x = total_degree, y = count, fill = is_fraud)) +
  geom_col() +
  scale_y_log10()

#### TEMP - Manual implementation ####


## Creating fourth df ("prediction" df - combination of the df's already created)

# Note: k will denote the current predictor of interest (the feature that will NOT be repeated in the plotting latice)

# defining additional df's
# diagnostic_df <- data_frame(predictor_name = col_names,
#                             status = rep(NA, length(col_names)))
# 
# fraud_df <- as.data.frame(train_target_nn) %>% 
#   mutate(fraud = if_else(V1 == 1, FALSE, TRUE)) %>% 
#   select(fraud)
# 
# train_x_plot <- cbind(train_x, fraud_df)
# 
# 
# start <- Sys.time()
# 
# for(k in 1:dim(train_x)[2]){
#   
#   # configuring df
#   prediction_df <- repition_df_final
#   prediction_df[,k] <- eq_spaced_df_final[,k]
#   
#   # making predictions with the model
#   pred <-  data.frame(predict(net_model, as.matrix(prediction_df))) %>% 
#     rename(non_fraud = X1,
#            fraud = X2) 
#   
#   clss <- apply(pred, 1, which.max)
#   cols <- c("blue", "red")
#   
#   
#   if((1 %in% clss) & (2 %in% clss)){
#     
#     # making multiple plots
#     pdf(file= paste0(col_names[k], "_response_curves.pdf"))
#     
#     for (i in 1:ncol(prediction_df)){
#       
#       plot(prediction_df[,i] ~ prediction_df[,k], 
#            pch = 20, 
#            col = cols[clss],
#            xlab = col_names[k],
#            ylab = col_names[i]
#       )
#       points(x = train_x_plot[,k][train_x_plot$fraud == FALSE], 
#            y = train_x_plot[,i][train_x_plot$fraud == FALSE],
#            col = "green")
#     points(x = train_x_plot[,k][train_x_plot$fraud == TRUE], 
#            y = train_x_plot[,i][train_x_plot$fraud == TRUE],
#            col = "black")
#       
#     }
#     diagnostic_df$status[k] <- "Both classes"
#     
#   } else if(1 %in% clss){
#     
#     diagnostic_df$status[k] <- "Only non-fraud"
#     
#   }else{
#     
#     diagnostic_df$status[k] <- "Only fraud"
#     
#   }
#   
# }
# 
# end <- Sys.time()


##### TEST #####

# fraud_df <- as.data.frame(train_target_nn) %>% 
#   mutate(fraud = if_else(V1 == 1, FALSE, TRUE)) %>% 
#   select(fraud)
# 
# train_x_plot <- cbind(train_x, fraud_df)
# 
# 
# k <- 3
# # configuring df
# prediction_df <- repition_df_final
# prediction_df[,k] <- eq_spaced_df_final[,k]
# 
# # making predictions with the model
# pred <-  data.frame(predict(net_model, as.matrix(prediction_df))) %>% 
#   rename(non_fraud = X1,
#          fraud = X2) 
# 
# clss <- apply(pred, 1, which.max)
# cols <- c("blue", "red")
# 
# for (i in 1:ncol(prediction_df)){
#   
#   plot(prediction_df[,i] ~ prediction_df[,k], 
#        pch = 20, 
#        col = cols[clss],
#        xlab = col_names[k],
#        ylab = col_names[i])
#   points(x = train_x_plot[,k][train_x_plot$fraud == FALSE], 
#          y = train_x_plot[,i][train_x_plot$fraud == FALSE],
#          col = "green")
#   points(x = train_x_plot[,k][train_x_plot$fraud == TRUE], 
#          y = train_x_plot[,i][train_x_plot$fraud == TRUE],
#          col = "black")
#   
# }

##### TEST #####



##### IRIS example ####
# newData <- iris
# newData$Sepal.Length <- scale(newData$Sepal.Length)
# newData$Sepal.Width<- scale(newData$Sepal.Width)
# newData$Petal.Length <- scale(newData$Petal.Length)
# newData$Petal.Width <- scale(newData$Petal.Width)
# 
# 
# M <- 100
# x1_dummy = seq(min(newData$Petal.Width), max(newData$Petal.Width), length = M)
# x2_dummy = seq(min(newData$Petal.Length), max(newData$Petal.Length), length = M)
# X1 <- rep(x1_dummy, M)
# X2 <- rep(x2_dummy, each = M)
# Lat <- data.frame(Petal.Width = X1, Petal.Length = X2)
# 
# plot(Lat$Petal.Length~Lat$Petal.Width, pch = 20)

```