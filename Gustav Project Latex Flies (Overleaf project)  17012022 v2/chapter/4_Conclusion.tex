%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% start writing
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion} \label{ch4_heading}

% reminded of projects premise
The project aimed to establish evidence suggesting
incorporating network metrics (derived using concepts of complex network analysis and graph theory) in the learning process (of a statistical model) can be advantageous for conventional anti-money laundering approaches. The rationale for the projects' hypothesis is that networks or graphs give us the ability to map \textit{relationships} in the real world \citep{baesens2015fraud}. Suppose the network analysis can capture these relationships components between entities. In that case, it can provide an additional dimension of information to the behaviour patterns on characteristics of money laundering banking clients. Therefore, possibly improving the identification and risk scoring sub-steps within the anti-money laundering process. The following chapter summarises the project's work, its findings, and makes suggestions for future work. 

\section{Project summary}
% chapter 2 - summary
To adequately address the projects research question, a comprehensive understanding of the three core elements of the project's methodology needed to be established. These elements are money laundering, machine learning, and network analytic's. The projects literature study, Chapter \ref{ch2_heading}, explained the elements mentioned above and investigated the research that has already been done. The chapter concluded by mentioning that there is evidence suggesting that network analytic's (extracting information from network or graphs) and machine learning (identifying complex non-linear patterns in data) can be used synergistically to produce stronger, more robust classification models. 

% chapter 3 - data generation and feature engneering
After conducting the literature study, a suitable framework or procedure for executing the project needed to be established. Figure \ref{fig:ch1_project_framework} gives an overview of the projects methodological procedure. This entire process is explained in detail in Chapter \ref{ch3_heading}. The first phase of the project was concerned with generating the project's raw data. The project used synthetically generated data using AMLSim, a multi-agent simulation platform specifically created to generate synthetic data for anti-money laundering research purposes \citep{AMLSim}. A detailed explanation of how the data simulator works can be found in section \ref{ch3_sub_heading_data}. Two financial data sets were generated using the data simulator - the raw training and testing data. The raw training data contained 12 043 accounts and 655 433 transactions, and the raw testing set contained 1446 accounts and 108 684 transactions. After generating the raw data, the project conducted an exploratory data analysis to establish an understanding of the data and validate the data simulators output. The project found that approximately 5\% of accounts were money laundering accounts, and  approximately 1\% of transactions were money laundering transactions in both data sets.
% chapter 3 - feature engineering
After the raw data was generated, the feature engineering process (Figure \ref{fig:ch1_project_framework}) could commence. The feature engineering process for the training and test raw data sets was identical. The feature engineering process took the raw data as input and produced three structured data tables. The first structures data table was the network feature data table. This data table was compiled by constructing an undirected weighted network and a directed network. The nodes in the networks would represent the bank accounts, and the network edges would represent the transactions between the accounts. After that, each network was decomposed into graph components of size $N>2$. Finally, the chosen network features or metrics could be extracted from each graph component. The complete network feature data table is illustrated in Table \ref{tab:ch3_network_features}. The second structured data table was the transactional feature data table. The project populated this table by creating useful transactional-based features from the raw data. Therefore, no network analysis was applied to construct the transactional feature data table. The complete transactional feature data table is illustrated in Table \ref{tab:ch3_trans_features}. The final structured data table was the combined feature data table. This table was constructed by combining the network feature data table with the transactional feature table. Therefore, joining the features shown in Table \ref{tab:ch3_network_features} and Table \ref{tab:ch3_trans_features}.\\
\\
The following sub-section described the modelling conducted in the project (sub-section \ref{ch3_sub_heading_modelling}). This section formed part of phase three of the project (Figure \ref{fig:ch1_project_framework}). The learning task of the project was a binary classification problem, where the model should predict, based on the set of input features given, if a bank account is involved in money laundering activity. The project defined two types of models. The first model was a logistic regression model and the second model was a neural network model. The modelling section provided a brief introduction to each model, explained the necessary data pre-processing that was conducted, and detailed the hyper-parameter tuning and training processes. The Chapter concluded with selecting the models it deemed fit to be tested on the testing data sets. The first model (``LR-ROC'') was a logistic regression model with L1 regularisation applied. The threshold value chosen for the model was based on the optimal threshold according to its ROC curve. The second model (``LR-PR'')  was identical to the first. However, the threshold value was chosen according to the optimal threshold value calculated from its precision-recall curve. The final model (``8-NN'') was the standard feed forward neural network model with 8 hidden neurons in its single hidden layer. A more detailed summary of each model and its chosen hyper-parameters is illustrated in Table \ref{tab:ch3_model_summary}.
% chapter 3 - Results
Chapter \ref{ch3_heading} concluded with the results obtained by each of the models when tested on each tests data set. The classification task of the project contained high-class imbalances in the response (mentioned as part of the EDA findings). Therefore, the performance metrics chosen needed to account for these severely skewed class distributions. The performance metrics chosen to evaluate each models performance on each of the data sets was the F1-score and balanced accuracy. The classification accuracy was also included, however, one should be cautious when jumping to conclusions using this metric (due to the accuracy paradox). Table \ref{tab:ch3_model_results} provides a summary of the test data set results for each model. On average, the network feature models (the models that used the network feature data set as their input) scored 0.371 higher on the F1-score than the transactional feature models. A similar result was obtained for the balanced accuracy, where the transactional feature models scored, on average, 0.21 less than the network feature models. Overall, the combined feature models performed slightly better or were equivalent to the network feature models. A visual representation of the results obtained is illustrated in Figure \ref{fig:ch3_final_results_bar}, Figure \ref{fig:ch3_final_results_f1_density}, and Figure \ref{fig:ch3_final_results_balanced_acc_density}.

\section{Findings and discussion}

% Comparing the trans models to the net models
The project's methodological process or framework was conducted so that the results will answer two essential questions. The first question is, \textit{how does the  performance of the statistical model, that used as input network features (derived from financial data) to predict money laundering accounts, compare to the same model but which uses transactional-based features as its input?} The project's results show that the network feature models significantly outperformed the transnational feature models. However, although the test results indicated the network feature models as the superior models, one should be critical about the results obtained. For example, how can one be sure that the project generated suitable transactional features? Or similarly, does there exist other transactional features that the project did not consider, which could have made a difference in the transactional model's performance? 

The second essential question is \textit{whether there is evidence that suggests that network features combined with transactional features deliver greater classification performance in predicting money laundering accounts?} From the test results, slight indications suggest combining the features sets could be beneficial. For example, models show marginally higher F1-score (Figure \ref{fig:ch3_final_results_f1_density}), and their balanced accuracy distributions are much narrower than the  network feature models (Figure \ref{fig:ch3_final_results_balanced_acc_density}). However, looking at the results holistically suggests that there is not much improvement in combining the feature sets. Again looking at the results from a critical point of view, there are some things the project could have implemented to possibly change this result. For example, suppose some formal feature selection (best-subset, forward/backward selection, etc.) or feature extraction (principal component analysis or Isometric mapping) was introduced. In that case, it could have selected only the more valuable'' features, reducing the noise within the data, and making the learning task easier. A final remark to consider is that even though the performance increase of the combined feature models was marginal to that of the network feature models, in reality, this would still be an improvement to an anti-money laundering process. This incremental improvement can significantly reduce the colossal loss commercial banks experience through money laundering activity.   

\section{Future work}

Throughout the project, particular research ``opportunities'' presented themselves. Conducting further research on any of these topics will be intriguing to see how they change the outcomes provided in the project. The main idea of each research opportunity is as follows:

\begin{itemize}
    \item \textit{Time-weighted network}: \citet{van2017gotcha} defined a time-weighted network where the weight assigned to each edge in the graph decays based on some decaying constant, $\gamma$. The assumption was made that nodes with more frequent contact with each other exhibit stronger relationships. The idea of incorporating a decaying constant into the project undirected weighted graph might provide a more realistic view of the accounts current relationship with each other. 
    
    \item \textit{Increasing project's scale}: The project was conducted on a relatively small scale i.t.o the number of accounts and transactions used. In a commercial bank, there can be anything from 1M to 10M banking clients and up to 1B transactions (depending on the historical time frame chosen). Therefore, it would be interesting to see how the project's implementation framework scales to such sizes and how the results are affected.
    
    \item \textit{Investigating specific money typologies:} The project took a very broad approach in analysing all the money laundering typologies the AMLSim has to offer. However, it would be valuable to establish what network features (or combined with transactional features) help predict different types of money laundering typologies.
    
    \item \textit{Apply the project's framework to actual financial data:} The project used synthetically generated financial data. Although the simulator used to generate the data is very intricate, it would still be interesting to see if similar results are obtained by using actual financial data of a commercial bank.    
    
\end{itemize}




